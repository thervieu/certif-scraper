[
  {
    "question_text": "You have a table in an Azure Synapse Analytics dedicated SQL pool. The table was created by using the following Transact-SQL statement.\n\nCREATE TABLE [dbo] . [DimEmployee] (\n[EmployeeKey] [int] IDENTITY (1,1) NOT NULL,\n[EmployeeID] [int] NOT NULL,\n[FirstName] [varchar] (100) NOT NULL,\n[LastName] [varchar] (100) NOT NULL,\n[JobTitle] [varchar] (100) NULL,\n[LastHireDate] [date] NULL,\n[StreetAddress] [varchar] (500) NOT NULL,\n[City] [varchar] (200) NOT NULL,\n[StateProvince] [varchar] (50) NOT NULL,\n[Portalcode] [varchar] (10) NOT NULL\n)\n\nYou need to alter the table to meet the following requirements:\u2711 Ensure that users can identify the current manager of employees.\u2711 Support creating an employee reporting hierarchy for your entire company.\u2711 Provide fast lookup of the managers' attributes such as name and job title.\n\nWhich column should you add to the table?",
    "question_type": "single",
    "choices": [
      "[ManagerEmployeeID] [smallint] NULL",
      "[ManagerEmployeeKey] [smallint] NULL",
      "[ManagerEmployeeKey] [int] NULL",
      "[ManagerName] [varchar](200) NULL"
    ],
    "site_answers": [
      "[ManagerEmployeeKey] [int] NULL"
    ]
  },
  {
    "question_text": "You have an Azure Synapse workspace named MyWorkspace that contains an Apache Spark database named mytestdb.\n\nYou run the following command in an Azure Synapse Analytics Spark pool in MyWorkspace.\n\nCREATE TABLE mytestdb.myParquetTable(EmployeeID int,EmployeeName string,EmployeeStartDate date)USING Parquet -You then use Spark to insert a row into mytestdb.myParquetTable. The row contains the following data.\n\n\nEmployeeName EmployeeID EmployeeStartDate\nAlice        24         2020-01-25\n\nOne minute later, you execute the following query from a serverless SQL pool in MyWorkspace.\n\nSELECT EmployeeID -FROM mytestdb.dbo.myParquetTableWHERE EmployeeName = 'Alice';What will be returned by the query?",
    "question_type": "single",
    "choices": [
      "24",
      "an error",
      "a null value"
    ],
    "site_answers": [
      "24"
    ]
  },
  {
    "question_text": "DRAG DROP -You have a table named SalesFact in an enterprise data warehouse in Azure Synapse Analytics. SalesFact contains sales data from the past 36 months and has the following characteristics:\u2711 Is partitioned by month\u2711 Contains one billion rows\u2711 Has clustered columnstore indexAt the beginning of each month, you need to remove data from SalesFact that is older than 36 months as quickly as possible.\n\nWhich three actions should you perform in sequence in a stored procedure? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\n\nSelect and Place:",
    "question_type": "multiple",
    "choices": [
      "Switch the partition containing the stale data from SalesFact to SalesFact_Work.",
      "Truncate the partition containing the stale data.",
      "Drop the SalesFact_Work table.",
      "Create an empty table named SalesFact_Work that has the same schema as SalesFact.",
      "Execute a DELETE statement where the value in the Date column is more than 36 months ago.",
      "Copy the data to a new table by using CREATE TABLE AS SELECT (CTAS)."],
    "site_answers": [
      "Create an empty table named SalesFact_Work that has the same schema as SalesFact.",
      "Switch the partition containing the stale data from SalesFact to SalesFact_Work.",
      "Drop the SalesFact_Work table."]
  },
  {
    "question_text": "You have files and folders in Azure Data Lake Storage Gen2 for an Azure Synapse workspace as shown in the following exhibit.\n\n topfolder -> [ file1, folder1, folder2, file4] ; folder1 -> file2 ; folder2 -> file3 \n\nYou create an external table named ExtTable that has LOCATION='/topfolder/'.\n\nWhen you query ExtTable by using an Azure Synapse Analytics serverless SQL pool, which files are returned?",
    "question_type": "single",
    "choices": [
      "File2.csv and File3.csv only",
      "File1.csv and File4.csv only",
      "File1.csv, File2.csv, File3.csv, and File4.csv",
      "File1.csv only"
    ],
    "site_answers": [
      "File1.csv and File4.csv only"
    ]
  },
  {
    "question_text": "HOTSPOT -You are planning the deployment of Azure Data Lake Storage Gen2.\n\nYou have the following two reports that will access the data lake:\u2711 Report1: Reads three columns from a file that contains 50 columns.\u2711 Report2: Queries a single record based on a timestamp.\n\nYou need to recommend in which format to store the data in the data lake to support the reports. The solution must minimize read times.\n\nWhat should you recommend for each report? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:",
    "question_type": "multiple",
    "choices": ["R1 Avro", "R1 Parquet", "R1 CSV", "R1 TSV", "R2 Avro", "R2 Parquet", "R2 CSV", "R2 TSV"],
    "site_answers": [
      "R1 Parquet", "R2 Avro"
    ]
  },
  {
    "question_text": "You are designing the folder structure for an Azure Data Lake Storage Gen2 container.\n\nUsers will query data by using a variety of services including Azure Databricks and Azure Synapse Analytics serverless SQL pools. The data will be secured by subject area. Most queries will include data from the current year or current month.\n\nWhich folder structure should you recommend to support fast queries and simplified folder security?",
    "question_type": "single",
    "choices": [
      "/{SubjectArea}/{DataSource}/{DD}/{MM}/{YYYY}/{FileData}_{YYYY}_{MM}_{DD}.csv",
      "/{DD}/{MM}/{YYYY}/{SubjectArea}/{DataSource}/{FileData}_{YYYY}_{MM}_{DD}.csv",
      "/{YYYY}/{MM}/{DD}/{SubjectArea}/{DataSource}/{FileData}_{YYYY}_{MM}_{DD}.csv",
      "/{SubjectArea}/{DataSource}/{YYYY}/{MM}/{DD}/{FileData}_{YYYY}_{MM}_{DD}.csv"
    ],
    "site_answers": [
      "/{SubjectArea}/{DataSource}/{YYYY}/{MM}/{DD}/{FileData}_{YYYY}_{MM}_{DD}.csv"
    ]
  },
  {
    "question_text": "HOTSPOT -You need to output files from Azure Data Factory.\n\nWhich file format should you use for each type of output? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:",
    "question_type": "multiple",
    "choices": ["Col. format Avro", "Col. format Parquet", "Col. format GZip", "Col. format TXT", "JSON w/ timestamp Avro", "JSON w/ timestamp Parquet", "JSON w/ timestamp GZip", "JSON w/ timestamp TXT"],
    "site_answers": ["Col. format Parquet", "JSON w/ timestamp Avro"]
  },
  {
    "question_text": "HOTSPOT -You use Azure Data Factory to prepare data to be queried by Azure Synapse Analytics serverless SQL pools.\n\nFiles are initially ingested into an Azure Data Lake Storage Gen2 account as 10 small JSON files. Each file contains the same data attributes and data from a subsidiary of your company.\n\nYou need to move the files to a different folder and transform the data to meet the following requirements:\u2711 Provide the fastest possible query times.\u2711 Automatically infer the schema from the underlying files.\n\nHow should you configure the Data Factory copy activity? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:",
    "question_type": "multiple",
    "choices": ["Copy behavior: Flatten hier.", "Copy behavior: Merge files", "Copy behavior: Preserver hier.", "Sink file type: CSV", "Sink file type: JSON", "Sink file type: Parquet", "Sink file type: TXT"],
    "site_answers": ["Copy behavior: Merge files", "Sink file type: Parquet"]
  },
  {
    "question_text": "HOTSPOT -You have a data model that you plan to implement in a data warehouse in Azure Synapse Analytics as shown in the following exhibit.\n\nEmployee <=> Fact_DailyBooking <=>Customer <-> (lié à Fact) Time\n\nAll the dimension tables will be less than 2 GB after compression, and the fact table will be approximately 6 TB. The dimension tables will be relatively static with very few data inserts and updates.\n\nWhich type of table should you use for each table? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:",
    "question_type": "multiple",
    "choices": ["Empl/Custo/Time: Hash", "Empl/Custo/Time: Round robin", "Empl/Custo/Time: Replicated", "DailyBooking: Hash", "DailyBooking: Round robin", "DailyBooking: Replicated"],
    "site_answers": ["Empl/Custo/Time: Replicated", "DailyBooking: Hash"]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Data Lake Storage Gen2 container.\n\nData is ingested into the container, and then transformed by a data integration application. The data is NOT modified after that. Users can read files in the container but cannot modify the files.\n\nYou need to design a data archiving solution that meets the following requirements:\u2711 New data is accessed frequently and must be available as quickly as possible.\u2711 Data that is older than five years is accessed infrequently but must be available within one second when requested.\u2711 Data that is older than seven years is NOT accessed. After seven years, the data must be persisted at the lowest cost possible.\u2711 Costs must be minimized while maintaining the required availability.\n\nHow should you manage the data? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one pointHot Area:",
    "question_type": "multiple",
    "choices": ["5yo data : Delete the blob.","5yo data : Move to archive storage.","5yo data : Move to cool storage.","5yo data : Move to hot storage.","7yo data : Delete the blob.","7yo data : Move to archive storage.","7yo data : Move to cool storage.","7yo data : Move to hot storage."],
    "site_answers": ["5yo data : Move to cool storage.", "7yo data : Move to archive storage."]
  },
  {
    "question_text": "DRAG DROP -You need to create a partitioned table in an Azure Synapse Analytics dedicated SQL pool.\n\nHow should you complete the Transact-SQL statement? \n\nCREATE TABLE table1\n(\nID INTEGER,\ncoll VARCHAR(10),\nco12 VARCHAR (10)\n) WITH (\nXXX = HASH (ID),\nYYY (ID RANGE LEFT FOR VALUES (1, 1000000, 2000000) ));\n\nTo answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\nSelect and Place:",
    "question_type": "multiple",
    "choices": ["CLUSTERED INDEX","COLLATE","DISTRIBUTION","PARTITION","PARTITION FUNCTION","PARTITION SCHEME"],
    "site_answers": ["DISTRIBUTION","PARTITION"]
  },
  {
    "question_text": "You need to design an Azure Synapse Analytics dedicated SQL pool that meets the following requirements:\u2711 Can return an employee record from a given point in time.\u2711 Maintains the latest employee information.\u2711 Minimizes query complexity.\n\nHow should you model the employee data?",
    "question_type": "single",
    "choices": [
      "as a temporal table",
      "as a SQL graph table",
      "as a degenerate dimension table",
      "as a Type 2 slowly changing dimension (SCD) table"
    ],
    "site_answers": [
      "as a Type 2 slowly changing dimension (SCD) table"
    ]
  },
  {
    "question_text": "You have an enterprise-wide Azure Data Lake Storage Gen2 account. The data lake is accessible only through an Azure virtual network named VNET1.\n\nYou are building a SQL pool in Azure Synapse that will use data from the data lake.\n\nYour company has a sales team. All the members of the sales team are in an Azure Active Directory group named Sales. POSIX controls are used to assign theSales group access to the files in the data lake.\n\nYou plan to load data to the SQL pool every hour.\n\nYou need to ensure that the SQL pool can load the sales data from the data lake.\n\nWhich three actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each area selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Add the managed identity to the Sales group.",
      "Use the managed identity as the credentials for the data load process.",
      "Create a shared access signature (SAS).",
      "Add your Azure Active Directory (Azure AD) account to the Sales group.",
      "Use the shared access signature (SAS) as the credentials for the data load process.",
      "Create a managed identity."
    ],
    "site_answers": [
      "Add the managed identity to the Sales group.",
      "Use the managed identity as the credentials for the data load process.",
      "Create a managed identity."
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse Analytics dedicated SQL pool that contains the users shown in the following table.\n\nUser    Role\nUser1    Server Admin\nUser2    db_datereader\n\nUser1 executes a query on the database, and the query returns the results shown in the following exhibit.\n\n\n\nSELECT c.name,\n  tbl.name as table_name,\n  typ.name as datatype,\n  c.is_masked,\n  c.masking_runction\nFROM sys.masked_columns AS c\nINNER JOIN sys. tables AS tbl ON c. [object_id] = tbl. [object_id]\nINNER JOIN sys.types typ ON c.user_type_id = typ.user_type_id\nWHERE is_masked = 1;\n\nResults\n\nname         table_name   datatype  is_masked  masking_function\nBirthDate     DimCustomer  date      1          default ()\nGender        DimCustomer  nvarchar  1          default ()\nEmailAddress  DimCustomer  nvarchar  1          email ()\nYearlyIncome  DimCustomer  money     1          default ()\n\nUser1 is the only user who has access to the unmasked data.\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\n\nNOTE: Each correct selection is worth one point.\n\n\n\nWhen User2 queries the YearlyIncome column, the values returned will be X.\n\nWhen User1 queries the BirthDate column, the values returned will be Y.\n\n",
    "question_type": "multiple",
    "choices": [
      "X a random number", "X the values stored in the database", "X XXXX", "X 0",
      "Y a random date", "Y the values stored in the database", "Y XXXX","Y 1900-01-01"],
    "site_answers": ["X 0", "Y the values stored in the database"]
  },
  {
    "question_text": "You have an enterprise data warehouse in Azure Synapse Analytics.\n\nUsing PolyBase, you create an external table named [Ext].[Items] to query Parquet files stored in Azure Data Lake Storage Gen2 without importing the data to the data warehouse.\n\nThe external table has three columns.\n\nYou discover that the Parquet files have a fourth column named ItemID.\n\nWhich command should you run to add the ItemID column to the external table?A.\n\nB.\n\nC.\n\nD.",
    "question_type": "multiple",
    "choices": [
      "ALTER EXTERNAL TABLE [Ext] . [Items]\nADD [ItemID] int;",
      "DROP EXTERNAL FILE FORMAT parquetfile1;\nCREATE EXTERNAL FILE FORMAT parquetfile1\nWITH (\nFORMAT_TYPE = PARQUET,\nnDATA_COMPRESSION = 'org. apache. hadoop. io. compress. SnappyCodec'",
      "DROP EXTERNAL TABLE [Ext] . [Items]\nCREATE EXTERNAL TABLE [Ext] . [Items]\n( [ItemID] [int] NULL,\n[ItemName] nvarchar (50) NULL,\n[ItemType] nvarchar (20) NULL,\n[ItemDescription] nvarchar (250) )\nWITH (\nLOCATION= '/Items/' ,\nDATA_SOURCE = AzureDataLakeStore,\nFILE_FORMAT = PARQUET,\nREJECT_TYPE = VALUE,\nREJECT VALUE = 0 ) ;",
      "ALTER TABLE [Ext] . [Items]\nADD [ItemID] int;"],
    "site_answers": ["DROP EXTERNAL TABLE [Ext] . [Items]\nCREATE EXTERNAL TABLE [Ext] . [Items]\n( [ItemID] [int] NULL,\n[ItemName] nvarchar (50) NULL,\n[ItemType] nvarchar (20) NULL,\n[ItemDescription] nvarchar (250) )\nWITH (\nLOCATION= '/Items/' ,\nDATA_SOURCE = AzureDataLakeStore,\nFILE_FORMAT = PARQUET,\nREJECT_TYPE = VALUE,\nREJECT VALUE = 0 ) ;"]
  },
  {
    "question_text": "HOTSPOT -You have two Azure Storage accounts named Storage1 and Storage2. Each account holds one container and has the hierarchical namespace enabled. The system has files that contain data stored in the Apache Parquet format.\n\nYou need to copy folders and files from Storage1 to Storage2 by using a Data Factory copy activity. The solution must meet the following requirements:\u2711 No transformations must be performed.\u2711 The original folder structure must be retained.\u2711 Minimize time required to perform the copy activity.\n\nHow should you configure the copy activity? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nSource dataset type: X\nCopy activity copy behavior: Y\n",
    "question_type": "multiple",
    "choices": [
      "X Binary",
      "X Parquet",
      "X Delimited text",
      "Y FlattenHierarchy","Y MergeFiles", "Y PreserveHierarchy"],
    "site_answers": ["X Parquet", "Y PreserveHierarchy"]
  },
  {
    "question_text": "You have an Azure Data Lake Storage Gen2 container that contains 100 TB of data.\n\nYou need to ensure that the data in the container is available for read workloads in a secondary region if an outage occurs in the primary region. The solution must minimize costs.\n\nWhich type of data redundancy should you use?",
    "question_type": "single",
    "choices": [
      "geo-redundant storage (GRS)",
      "read-access geo-redundant storage (RA-GRS)",
      "zone-redundant storage (ZRS)",
      "locally-redundant storage (LRS)"
    ],
    "site_answers": [
      "geo-redundant storage (GRS)"
    ]
  },
  {
    "question_text": "You plan to implement an Azure Data Lake Gen 2 storage account.\n\nYou need to ensure that the data lake will remain available if a data center fails in the primary Azure region. The solution must minimize costs.\n\nWhich type of replication should you use for the storage account?",
    "question_type": "single",
    "choices": [
      "geo-redundant storage (GRS)",
      "geo-zone-redundant storage (GZRS)",
      "locally-redundant storage (LRS)",
      "zone-redundant storage (ZRS)"
    ],
    "site_answers": [
      "zone-redundant storage (ZRS)"
    ]
  },
  {
    "question_text": "HOTSPOT -You have a SQL pool in Azure Synapse.\n\nYou plan to load data from Azure Blob storage to a staging table. Approximately 1 million rows of data will be loaded daily. The table will be truncated before each daily load.\n\nYou need to create the staging table. The solution must minimize how long it takes to load the data to the staging table.\n\nHow should you configure the table? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nDistribution X\nIndexing Y\nPartitioning Z",
    "question_type": "multiple",
    "choices": ["X Hash","X Replicated","X Round-robin","Y Clustered","Y Clustered columnstore","Y Heap","Z Date","Z None"],
    "site_answers": ["X Round-robin","Y Heap","Z None"]
  },
  {
    "question_text": "You are designing a fact table named FactPurchase in an Azure Synapse Analytics dedicated SQL pool. The table contains purchases from suppliers for a retail store. FactPurchase will contain the following columns.\n\nName              Data type     Nullable\nPurchaseKey       Bigint        No\nDateKey           Int           No\nSupplierKey       Int           No\nStockItemKey      Int           No\nPurchaseOrderID   Int           Yes\nOrderedQuantity   Int           No\nOrderedOuters     Int           No\nReceivedOuters    Int           No\nPackage           Nvarchar(50)  No\nIsOrderFinalized  Bit           No\nLineageKey        Int           No\n\nFactPurchase will have 1 million rows of data added daily and will contain three years of data.\n\nTransact-SQL queries similar to the following query will be executed daily.\n\nSELECT -SupplierKey, StockItemKey, IsOrderFinalized, COUNT(*)FROM FactPurchase -WHERE DateKey >= 20210101 -AND DateKey <= 20210131 -GROUP By SupplierKey, StockItemKey, IsOrderFinalizedWhich table distribution will minimize query times?",
    "question_type": "single",
    "choices": [
      "replicated",
      "hash-distributed on PurchaseKey",
      "round-robin",
      "hash-distributed on IsOrderFinalized"
    ],
    "site_answers": [
      "hash-distributed on PurchaseKey"
    ]
  },
  {
    "question_text": "HOTSPOT \nFrom a website analytics system, you receive data extracts about user interactions such as downloads, link clicks, form submissions, and video plays.\n\nThe data contains the following columns.\n\nName               Sample value\nDate               15 Jan 2021\nEventCategory      Videos\nEventAction        Play\nEventLabel         Contoso Promotional\nChannelGrouping    Social\nTotalEvents        150\nUniqueEvents       120\nSessionWithEvents  99\n\nYou need to design a star schema to support analytical queries of the data. The star schema will contain four tables including a date dimension.\n\nTo which table should you add each column? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nEventCategory X\nChannelGrouping Y\nTotalEvents Z",
    "question_type": "multiple",
    "choices": ["X DimChannel","X DimDate","X DimEvent","X FactEvents","Y DimChannel","Y DimDate","Y DimEvent","Y FactEvents","Z DimChannel","Z DimDate","Z DimEvent","Z FactEvents"],
    "site_answers": ["X DimEvent","Y DimChannel","Z FactEvents"]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.\n\nYou plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.\n\nYou need to prepare the files to ensure that the data copies quickly.\n\nSolution: You convert the files to compressed delimited text files.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "Yes"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.\n\nYou plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.\n\nYou need to prepare the files to ensure that the data copies quickly.\n\nSolution: You copy the files to a table that has a columnstore index.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "No"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.\n\nYou plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.\n\nYou need to prepare the files to ensure that the data copies quickly.\n\nSolution: You modify the files to ensure that each row is more than 1 MB.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "No"
    ]
  },
  {
    "question_text": "You build a data warehouse in an Azure Synapse Analytics dedicated SQL pool.\n\nAnalysts write a complex SELECT query that contains multiple JOIN and CASE statements to transform data for use in inventory reports. The inventory reports will use the data and additional WHERE parameters depending on the report. The reports will be produced once daily.\n\nYou need to implement a solution to make the dataset available for the reports. The solution must minimize query times.\n\nWhat should you implement?",
    "question_type": "single",
    "choices": [
      "an ordered clustered columnstore index",
      "a materialized view",
      "result set caching",
      "a replicated table"
    ],
    "site_answers": [
      "a materialized view"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics workspace named WS1 that contains an Apache Spark pool named Pool1.\n\nYou plan to create a database named DB1 in Pool1.\n\nYou need to ensure that when tables are created in DB1, the tables are available automatically as external tables to the built-in serverless SQL pool.\n\nWhich format should you use for the tables in DB1?",
    "question_type": "single",
    "choices": [
      "CSV",
      "ORC",
      "JSON",
      "Parquet"
    ],
    "site_answers": [
      "Parquet"
    ]
  },
  {
    "question_text": "You are planning a solution to aggregate streaming data that originates in Apache Kafka and is output to Azure Data Lake Storage Gen2. The developers who will implement the stream processing solution use Java.\n\nWhich service should you recommend using to process the streaming data?",
    "question_type": "single",
    "choices": [
      "Azure Event Hubs",
      "Azure Data Factory",
      "Azure Stream Analytics",
      "Azure Databricks"
    ],
    "site_answers": [
      "Azure Databricks"
    ]
  },
  {
    "question_text": "You plan to implement an Azure Data Lake Storage Gen2 container that will contain CSV files. The size of the files will vary based on the number of events that occur per hour.\n\nFile sizes range from 4 KB to 5 GB.\n\nYou need to ensure that the files stored in the container are optimized for batch processing.\n\nWhat should you do?",
    "question_type": "single",
    "choices": [
      "Convert the files to JSON",
      "Convert the files to Avro",
      "Compress the files",
      "Merge the files"
    ],
    "site_answers": [
      "Merge the files"
    ]
  },
  {
    "question_text": "HOTSPOT -You store files in an Azure Data Lake Storage Gen2 container. The container has the storage policy shown in the following exhibit.\n\n\"rules\": [\n  \"enabled\": true,\n  \"name\": \"contosorule\",\n  \"type\": \"Lifecycle\",\n  \"definition\": {\n    \"actions\":\n      \"version\": \"delete\": \"daysAfterCreationGreaterThan\": 60\n      \"baseBlob\": \"tierToCool\": \"daysAfterModificationGreaterThan\": 30\n    \"filters\": {\n      \"blobTypes\": \"blockBlob\"\n      \"prefixMatch: \"container1/contoso\"\n]\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nThe files are XXX afeter 30 days.\nThe storage policy applies to YYY.",
    "question_type": "multiple",
    "choices": [
      "X deleted from the container","X moved to archive storage","X moved to cool storage","X moved to hot storage",
      "Y container1/contoso.csv","Y container1/docs/contoso.json","Y container1/mycontoso/contoso.csv"],
    "site_answers": [
      "X moved to cool storage", "Y container1/contoso.csv"
    ]
  },
  {
    "question_text": "You are designing a financial transactions table in an Azure Synapse Analytics dedicated SQL pool. The table will have a clustered columnstore index and will include the following columns:\u2711 TransactionType: 40 million rows per transaction type\u2711 CustomerSegment: 4 million per customer segment\u2711 TransactionMonth: 65 million rows per monthAccountType: 500 million per account typeYou have the following query requirements:\u2711 Analysts will most commonly analyze transactions for a given month.\u2711 Transactions analysis will typically summarize transactions by transaction type, customer segment, and/or account typeYou need to recommend a partition strategy for the table to minimize query times.\n\nOn which column should you recommend partitioning the table?",
    "question_type": "single",
    "choices": [
      "CustomerSegment",
      "AccountType",
      "TransactionType",
      "TransactionMonth"
    ],
    "site_answers": [
      "TransactionMonth"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Data Lake Storage Gen2 account named account1 that stores logs as shown in the following table.\n\nType             Designated retention period\nApplication      360 days\nInfrastructure  60 days\n\nYou do not expect that the logs will be accessed during the retention periods.\n\nYou need to recommend a solution for account1 that meets the following requirements:\u2711 Automatically deletes the logs at the end of each retention period\u2711 Minimizes storage costsWhat should you include in the recommendation? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nTo minimize storage costs: X\nTo delete logs automatically: Y",
    "question_type": "multiple",
    "choices": [
      "X Store the infrastructure logs and the application logs in the Archive access tier","X Store the infrastructure logs and the application logs in the Cool access tier",
      "X Store the infrastructure logs in the Cool access tier and the application logs in the Archive access tier",
      "Y Azure Data Factory pipelines","Y Azure Blob storage lifecycle management rules","Y Immutable Azure Blob storage time-based retention policies"
    ],
    "site_answers": ["X Store the infrastructure logs in the Cool access tier and the application logs in the Archive access tier","Y Azure Blob storage lifecycle management rules"]
  },
  {
    "question_text": "You plan to ingest streaming social media data by using Azure Stream Analytics. The data will be stored in files in Azure Data Lake Storage, and then consumed by using Azure Databricks and PolyBase in Azure Synapse Analytics.\n\nYou need to recommend a Stream Analytics data output format to ensure that the queries from Databricks and PolyBase against the files encounter the fewest possible errors. The solution must ensure that the files can be queried quickly and that the data type information is retained.\n\nWhat should you recommend?",
    "question_type": "single",
    "choices": [
      "JSON",
      "Parquet",
      "CSV",
      "Avro"
    ],
    "site_answers": [
      "Parquet"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 contains a partitioned fact table named dbo.\n\nSales and a staging table named stg.\n\nSales that has the matching table and partition definitions.\n\nYou need to overwrite the content of the first partition in dbo.\n\nSales with the content of the same partition in stg.\n\nSales. The solution must minimize load times.\n\nWhat should you do?",
    "question_type": "single",
    "choices": [
      "Insert the data from stg.Sales into dbo.Sales.",
      "Switch the first partition from dbo.Sales to stg.Sales.",
      "Switch the first partition from stg.Sales to dbo.Sales.",
      "Update dbo.Sales from stg.Sales."
    ],
    "site_answers": [
      "Switch the first partition from stg.Sales to dbo.Sales."
    ]
  },
  {
    "question_text": "You are designing a slowly changing dimension (SCD) for supplier data in an Azure Synapse Analytics dedicated SQL pool.\n\nYou plan to keep a record of changes to the available fields.\n\nThe supplier data contains the following columns.\n\nName                   Description\nSupplierSystemID       Unique supplier ID in an enterprise resource planning (ERP) system\nSupplierName           Name of the supplier company\nSupplierAddress1       Address of the supplier company\nSupplierAddress2       Second address of the supplier company\nSupplierCity           City of the supplier company\nSupplierStateProvince  State or province of the supplier company\nSupplierCountry        Country of the supplier company\nSupplierPostalCode     Postal code of the supplier company\nSupplierDescription    Free-test description of the supplier company\nSupplierCategory       Category of goods provided by the supplier company\n\nWhich three additional columns should you add to the data to create a Type 2 SCD? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "surrogate primary key",
      "effective start date",
      "business key",
      "last modified date",
      "effective end date",
      "foreign key"
    ],
    "site_answers": [
      "surrogate primary key",
      "effective start date",
      "effective end date"
    ]
  },
  {
    "question_text": "HOTSPOT -You have a Microsoft SQL Server database that uses a third normal form schema.\n\nYou plan to migrate the data in the database to a star schema in an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to design the dimension tables. The solution must optimize read operations.\n\nWhat should you include in the solution? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nTransform data for the dimension tables by: X\nFor the primary key columns in the dimension tables, use: Y",
    "question_type": "multiple",
    "choices": [
      "X Maintaining to a third normal form","X Normalizing to a fourth normal form","X Denormalizing to a second normal form",
      "Y New IDENTITY columns","Y A new computed column","Y The business key column from the source sys"
    ],
    "site_answers": ["X Denormalizing to a second normal form","Y New IDENTITY columns"]
  },
  {
    "question_text": "HOTSPOT -You plan to develop a dataset named Purchases by using Azure Databricks. Purchases will contain the following columns:\u2711 ProductID\u2711 ItemPrice\u2711 LineTotal\u2711 Quantity\u2711 StoreID\u2711 Minute\u2711 Month\u2711 HourYear -\u2711 DayYou need to store the data to support hourly incremental load pipelines that will vary for each Store ID. The solution must minimize storage costs.\n\nHow should you complete the code? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n df.write.A(B).mode(\"append\").C",
    "question_type": "multiple",
    "choices": [
      "A bucketBy","A partitionBy","A range","sortBy",
      "B (\"*\")", "B (\"StoreID\",\"Hour\")", "B (\"StoreID\",\"Year\",\"Month\",\"Day\"\"Hour\")",
      "csv(\"/Purchases\")","json(\"/Purchases\")","parquet(\"/Purchases\")","saveAsTable(\"/Purchases\")", 
    ],
    "site_answers": ["A partitionBy", "B (\"StoreID\",\"Year\",\"Month\",\"Day\"\"Hour\")", "parquet(\"/Purchases\")"]
  },
  {
    "question_text": "You are designing a partition strategy for a fact table in an Azure Synapse Analytics dedicated SQL pool. The table has the following specifications:\u2711 Contain sales data for 20,000 products.\n\nUse hash distribution on a column named ProductID.\u2711 Contain 2.4 billion records for the years 2019 and 2020.\n\nWhich number of partition ranges provides optimal compression and performance for the clustered columnstore index?",
    "question_type": "single",
    "choices": [
      "40",
      "240",
      "400",
      "2,400"
    ],
    "site_answers": [
      "40"
    ]
  },
  {
    "question_text": "HOTSPOT -You are creating dimensions for a data warehouse in an Azure Synapse Analytics dedicated SQL pool.\n\nYou create a table by using the Transact-SQL statement shown in the following exhibit.\n\nCREATE TABLE [DBO] . [DimProduct] (\n  [ProductKey] [int] IDENTITY (1,1) NOT NULL,\n  [ProductSourceID] [int] NOT NULL,\n  [ProductName] [nvarchar] (100) NOT NULL,\n  [ProductNumber] [nvarchar] (25) NOT NULL,\n  [Color] [nvarchar] (15) NULL,\n  [Size] [nvarchar] (5) NULL,\n  [Weight] [decimal] (8, 2) NULL,\n  [ProductCategory] [nvarchar] (100) NULL,\n  [SellStartDate] [date] NOT NULL,\n  [SellEndDate] [date] NULL,\n  [RowInsertedDateTime] [datetime] NOT NULL,\n  [RowUpdatedDateTime] [datetime] NOT NULL,\n  [ETLAuditID] [int] NOT NULL\n)\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nDimproduct is a XXX SCD.\nTheProductKey column is YYY.",
    "question_type": "multiple",
    "choices": [
      "X Type 0","X Type 1","X Type 2",
      "Y a surrogate key","Y a business key","Y an audit column"],
    "site_answers": ["X type 2", "Y a surrogate key"]
  },
  {
    "question_text": "You are designing a fact table named FactPurchase in an Azure Synapse Analytics dedicated SQL pool. The table contains purchases from suppliers for a retail store. FactPurchase will contain the following columns.\n\nName              Data type     Nullable\nPurchaseKey       Bigint        No\nDateKey           Int           No\nSupplierKey       Int           No\nStockItemKey      Int           No\nPurchaseOrderID   Int           Yes\nOrderedQuantity   Int           No\nOrderedOuters     Int           No\nReceivedOuters    Int           No\nPackage           Nvarchar(50)  No\nIsOrderFinalized  Bit           No\nLineageKey        Int           No\n\nFactPurchase will have 1 million rows of data added daily and will contain three years of data.\n\nTransact-SQL queries similar to the following query will be executed daily.\n\nSELECT -SupplierKey, StockItemKey, COUNT(*)FROM FactPurchase -WHERE DateKey >= 20210101 -AND DateKey <= 20210131 -GROUP By SupplierKey, StockItemKeyWhich table distribution will minimize query times?",
    "question_type": "single",
    "choices": [
      "replicated",
      "hash-distributed on PurchaseKey",
      "round-robin",
      "hash-distributed on DateKey"
    ],
    "site_answers": [
      "hash-distributed on PurchaseKey"
    ]
  },
  {
    "question_text": "You are implementing a batch dataset in the Parquet format.\n\nData files will be produced be using Azure Data Factory and stored in Azure Data Lake Storage Gen2. The files will be consumed by an Azure Synapse Analytics serverless SQL pool.\n\nYou need to minimize storage costs for the solution.\n\nWhat should you do?",
    "question_type": "single",
    "choices": [
      "Use Snappy compression for the files.",
      "Use OPENROWSET to query the Parquet files.",
      "Create an external table that contains a subset of columns from the Parquet files.",
      "Store all data as string in the Parquet files."
    ],
    "site_answers": [
      "Use Snappy compression for the files."
    ]
  },
  {
    "question_text": "DRAG DROP -You need to build a solution to ensure that users can query specific files in an Azure Data Lake Storage Gen2 account from an Azure Synapse Analytics serverless SQL pool.\n\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\n\nNOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.\n\nSelect and Place:",
    "question_type": "multiple",
    "choices": [
      "Create an external file format object",
      "Create an external data source",
      "Create a query that uses Create Table as Select",
      "Create a table",
      "Create an external table"
    ],
    "site_answers": [
      "Create an external data source",
      "Create an external file format object",
      "Create an external table"
    ]
  },
  {
    "question_text": "You are designing a data mart for the human resources (HR) department at your company. The data mart will contain employee information and employee transactions.\n\nFrom a source system, you have a flat extract that has the following fields:\u2711 EmployeeIDFirstName -\u2711 LastName\u2711 Recipient\u2711 GrossAmount\u2711 TransactionID\u2711 GovernmentID\u2711 NetAmountPaid\u2711 TransactionDateYou need to design a star schema data model in an Azure Synapse Analytics dedicated SQL pool for the data mart.\n\nWhich two tables should you create? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "a dimension table for Transaction",
      "a dimension table for EmployeeTransaction",
      "a dimension table for Employee",
      "a fact table for Employee",
      "a fact table for Transaction"
    ],
    "site_answers": [
      "a dimension table for Employee",
      "a fact table for Transaction"
    ]
  },
  {
    "question_text": "You are designing a dimension table for a data warehouse. The table will track the value of the dimension attributes over time and preserve the history of the data by adding new rows as the data changes.\n\nWhich type of slowly changing dimension (SCD) should you use?",
    "question_type": "single",
    "choices": [
      "Type 0",
      "Type 1",
      "Type 2",
      "Type 3"
    ],
    "site_answers": [
      "Type 2"
    ]
  },
  {
    "question_text": "DRAG DROP -You have data stored in thousands of CSV files in Azure Data Lake Storage Gen2. Each file has a header row followed by a properly formatted carriage return (/ r) and line feed (/n).\n\nYou are implementing a pattern that batch loads the files daily into a dedicated SQL pool in Azure Synapse Analytics by using PolyBase.\n\nYou need to skip the header row when you import the files into the data warehouse. Before building the loading pattern, you need to prepare the required database objects in Azure Synapse Analytics.\n\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\n\nNOTE: Each correct selection is worth one pointSelect and Place:",
    "question_type": "multiple",
    "choices": [
      "Create a database scoped credential that uses Azure Active Directory Application and a Service Principal Key",
      "Create an external data source that uses the abfs location",
      "Use CREATE EXTERNAL TABLE AS SELECT (CETAS) and configure the reject options to specify reject values or percentages",
      "Create an external file format and set the First Row option"
    ],
    "site_answers": [
      "Create an external data source that uses the abfs location",
      "Create an external file format and set the First Row option",
      "Use CREATE EXTERNAL TABLE AS SELECT (CETAS) and configure the reject options to specify reject values or percentages"
    ]
  },
  {
    "question_text": "HOTSPOT -You are building an Azure Synapse Analytics dedicated SQL pool that will contain a fact table for transactions from the first half of the year 2020.\n\nYou need to ensure that the table meets the following requirements:\u2711 Minimizes the processing time to delete data that is older than 10 years\u2711 Minimizes the I/O for queries that use year-to-date valuesHow should you complete the Transact-SQL statement? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nCREATE TABLE [dbo] . [FactTransaction]\n(\n[TransactionTypeID] int   NOT NULL,\n[TransactionDateID] int   NOT NULL,\n[CustomerID]        int   NOT NULL,\n[RecipientID]       int   NOT NULL,\n[Amount]            money NOT NULL\n)\nWITH (X (Y  RANGE RIGHT FOR VALUES (20200101, 20200201, 20200301, 20200401, 20200501, 20200601)))",
    "question_type": "multiple",
    "choices": [
      "X CLUSTERED COLUMNSTORE INDE",
      "X DISTRIBUTION",
      "X PARTITION",
      "X TRUNCATE_TARGET",
      "Y [TransactionDateID]",
      "Y [TransactionDateID], [TransactionTypeID]",
      "Y HASH ( [TransactionTypeID])",
      "Y ROUND ROBIN"
    ],
    "site_answers": [
      "X PARTITION", "Y [TransactionDataID]"
    ]
  },
  {
    "question_text": "You are performing exploratory analysis of the bus fare data in an Azure Data Lake Storage Gen2 account by using an Azure Synapse Analytics serverless SQL pool.\n\nYou execute the Transact-SQL query shown in the following exhibit.\n\nSELECT\n  payment_type,\n  SUM(fare amount) AS fare total\nFROM OPENROWSET (\n    BULK 'csv/busfare/tripdata 2020 *. csv',\n    DATA SOURCE = 'BusData',\n    FORMAT = 'CSV', PARSER VERSION = '2.0',\n    FIRSTROW = 2\n  )\n  WITH (\n    payment_type INT 10,\n    fare amount FLOAT 11\n  ) AS nyc\nGROUP BY payment_type\nORDER BY payment_type;\n\nWhat do the query results include?",
    "question_type": "single",
    "choices": [
      "Only CSV files in the tripdata_2020 subfolder.",
      "All files that have file names that beginning with \"tripdata_2020\".",
      "All CSV files that have file names that contain \"tripdata_2020\".",
      "Only CSV that have file names that beginning with \"tripdata_2020\"."
    ],
    "site_answers": [
      "Only CSV that have file names that beginning with \"tripdata_2020\"."
    ]
  },
  {
    "question_text": "DRAG DROP -You use PySpark in Azure Databricks to parse the following JSON input.\n\"persons\": [\n  {\"name\":\"Keith\",\n  \"age\":30,\n  \"dogs\": [\"Fido\", \"Fluffy\"]\n  },{\n  \"name\":\"Donna\",\n  \"age\":46,\n  \"dogs\": [\"Spot\"]}\n]\n\nYou need to output the data in the following tabular format.\n\nowner   age  dog\nKeith   30   Fido\nKeith   30   Fluffy\nDonna   46   Spot\n\nHow should you complete the PySpark code? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the spit bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\nSelect and Place:\n\ndbutils.fs.put(\"/tmp/source.json\", source_json, True)\nsource_df = spark.read.option(\"multiline\", \"true\") .json(\"/tmp/source.json\")\npersons = source_df.XXX.XXX (\"persons\") .alias(\"persons\"))\npersons_dogs = persons.select(col(\"persons.name\").alias(\"owner\"), col(\"persons.age\").alias(\"age\"),\nexplode XXX (\"dog\"))\n(\"persons-dogs\").\ndisplay (persons_dogs)",
    "question_type": "multiple",
    "choices": [ "alias","array_union","createDataFrame","explode","select","translate"
    ],
    "site_answers": ["select","explode","alias"
    ]
  },
  {
    "question_text": "HOTSPOT -You are designing an application that will store petabytes of medical imaging data.\n\nWhen the data is first created, the data will be accessed frequently during the first week. After one month, the data must be accessible within 30 seconds, but files will be accessed infrequently. After one year, the data will be accessed infrequently but must be accessible within five minutes.\n\nYou need to select a storage strategy for the data. The solution must minimize costs.\n\nWhich storage tier should you use for each time frame? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nFirst week: X\nAfter one month: Y\nAfter one year: Z",
    "question_type": "multiple",
    "choices": ["X Archive","X Cool","X Hot","Y Archive","Y Cool","Y Hot","Z Archive","Z Cool","Z Hot"],
    "site_answers": ["X Hot","Y Cool","Z Cool"]
  },
  {
    "question_text": "You have an Azure Synapse Analytics Apache Spark pool named Pool1.\n\nYou plan to load JSON files from an Azure Data Lake Storage Gen2 container into the tables in Pool1. The structure and data types vary by file.\n\nYou need to load the files into the tables. The solution must maintain the source data types.\n\nWhat should you do?",
    "question_type": "single",
    "choices": [
      "Use a Conditional Split transformation in an Azure Synapse data flow.",
      "Use a Get Metadata activity in Azure Data Factory.",
      "Load the data by using the OPENROWSET Transact-SQL command in an Azure Synapse Analytics serverless SQL pool.",
      "Load the data by using PySpark."
    ],
    "site_answers": [
      "Load the data by using PySpark."
    ]
  },
  {
    "question_text": "You have an Azure Databricks workspace named workspace1 in the Standard pricing tier. Workspace1 contains an all-purpose cluster named cluster1.\n\nYou need to reduce the time it takes for cluster1 to start and scale up. The solution must minimize costs.\n\nWhat should you do first?",
    "question_type": "single",
    "choices": [
      "Configure a global init script for workspace1.",
      "Create a cluster policy in workspace1.",
      "Upgrade workspace1 to the Premium pricing tier.",
      "Create a pool in workspace1."
    ],
    "site_answers": [
      "Create a pool in workspace1."
    ]
  },
  {
    "question_text": "HOTSPOT -You are building an Azure Stream Analytics job that queries reference data from a product catalog file. The file is updated daily.\n\nThe reference data input details for the file are shown in the Input exhibit. (Click the Input tab.)The storage account container view is shown in the Refdata exhibit. (Click the Refdata tab.)\n\nproducts\ncontainer: refdata\npath pattern: product.csv\nDate format YYYY/MM/DD\nTime format: HH\nCSV, Comma, UTF-8\n\ncontainer refdata\nrefdata/2020-03-20/product.csv\n\nYou need to configure the Stream Analytics job to pick up the new reference data.\n\nWhat should you configure? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nPath pattern: X\nDate format: Y",
    "question_type": "multiple",
    "choices": [
      "X {date}/product.csv", "X {date}/{time}/product.csv", "X product.csv", "X */product.csv",
      "Y MM/DD/YYY", "Y YYYY/MM/DD", "Y YYYY-DD-MM","Y YYYY-MM-DD"
    ],
    "site_answers": ["X {date}/product.csv","Y YYYY-MM-DD"]
  },
  {
    "question_text": "HOTSPOT -You are building a database in an Azure Synapse Analytics serverless SQL pool.\n\nYou have data stored in Parquet files in an Azure Data Lake Storege Gen2 container.\n\nRecords are structured as shown in the following sample.{\"id\": 123,\"address_housenumber\": \"19c\",\"address_line\": \"Memory Lane\",\"applicant1_name\": \"Jane\",\"applicant2_name\": \"Dev\"}The records contain two applicants at most.\n\nYou need to build a table that includes only the address fields.\n\nHow should you complete the Transact-SQL statement? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:",
    "question_type": "multiple",
    "choices": ["CREATE EXTERNAL TABLE","CREATE TABLE","CREATE VIEW", "CROSS APPLY", "OPENJSON","OPENROWSET (BULK http link)"],
    "site_answers": ["CREATE EXTERNAL TABLE","OPENROWSET (BULK http link)"]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse Analytics dedicated SQL pool named Pool1 and an Azure Data Lake Storage Gen2 account named Account1.\n\nYou plan to access the files in Account1 by using an external table.\n\nYou need to create a data source in Pool1 that you can reference when you create the external table.\n\nHow should you complete the Transact-SQL statement? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nCREATE EXTERNAL DATA SOURCE source1\nWITH\n( LOCATION = https://account1.XXX.cor.windows.net, YYY)",
    "question_type": "multiple",
    "choices": ["blob","dfs","table","PUSHDOWN = ON", "TYPE = BLOB_STORAGE","TYPE = HADOOP"],
    "site_answers": ["blob","TYPE = HADOOP"]
    "link":"https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables"
  },
  {
    "question_text": "You have an Azure subscription that contains an Azure Blob Storage account named storage1 and an Azure Synapse Analytics dedicated SQL pool namedPool1.\n\nYou need to store data in storage1. The data will be read by Pool1. The solution must meet the following requirements:Enable Pool1 to skip columns and rows that are unnecessary in a query.\u2711 Automatically create column statistics.\u2711 Minimize the size of files.\n\nWhich type of file should you use?",
    "question_type": "single",
    "choices": [
      "JSON",
      "Parquet",
      "Avro",
      "CSV"
    ],
    "site_answers": [
      "Parquet"
    ]
  },
  {
    "question_text": "DRAG DROP -You plan to create a table in an Azure Synapse Analytics dedicated SQL pool.\n\nData in the table will be retained for five years. Once a year, data that is older than five years will be deleted.\n\nYou need to ensure that the data is distributed evenly across partitions. The solution must minimize the amount of time required to delete old data.\n\nHow should you complete the Transact-SQL statement? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\nSelect and Place:\nDistriubtion: XXX\nPartition: YYY",
    "question_type": "multiple",
    "choices": ["CustomerKey","HASH","ROUND_ROBIN","REPLICATE","OrderDateKey","SalesOrderNumber"],
    "site_answers": ["HASH","OrderDateKey"]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Data Lake Storage Gen2 service.\n\nYou need to design a data archiving solution that meets the following requirements:\u2711 Data that is older than five years is accessed infrequently but must be available within one second when requested.\u2711 Data that is older than seven years is NOT accessed.\u2711 Costs must be minimized while maintaining the required availability.\n\nHow should you manage the data? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nData > 5yo: X\nData > 7yo: Y",
    "question_type": "multiple",
    "choices": ["X Delete the blob.","X Move to archive storage.","X Move to cool storage.","X Move to hot storage.","Y Delete the blob.","Y Move to archive storage.","Y Move to cool storage.","Y Move to hot storage."],],
    "site_answers": ["X Move to cool storage.","Y Move to archive storage."]
  },
  {
    "question_text": "HOTSPOT -You plan to create an Azure Data Lake Storage Gen2 account.\n\nYou need to recommend a storage solution that meets the following requirements:\u2711 Provides the highest degree of data resiliency\u2711 Ensures that content remains available for writes if a primary data center failsWhat should you include in the recommendation? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nReplication mechanism: X\nFailover process: Y",
    "question_type": "multiple",
    "choices": [
      "X Change feed","X Zone-redundant storage (ZRS)","X Read-access geo-redundant storage (RA-GRS)","X Read-access geo-zone-redundant storage (RA-GRS)",
      "Y Failover initiated by Microsoft","Y Failover manually initiated by the customer","Y Failover automatically initiated by an Azure Automation job"
    ],
    "site_answers": ["X Read-access geo-zone-redundant storage (RA-GRS)",,"Y Failover manually initiated by the customer"]
  },
  {
    "question_text": "You need to implement a Type 3 slowly changing dimension (SCD) for product category data in an Azure Synapse Analytics dedicated SQL pool.\n\nYou have a table that was created by using the following Transact-SQL statement.\n\nWhich two columns should you add to the table? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.\n\nA.\n\nB.\n\nC.\n\nD.\n\nE.",
    "question_type": "multiple",
    "choices": ["CurrentProductCategory","OriginalProductCategory"],
    "site_answers": ["CurrentProductCategory","OriginalProductCategory"]
  },
  {
    "question_text": "DRAG DROP -You have an Azure subscription.\n\nYou plan to build a data warehouse in an Azure Synapse Analytics dedicated SQL pool named pool1 that will contain staging tables and a dimensional model.\n\nPool1 will contain the following tables.\n\nYou need to design the table storage for pool1. The solution must meet the following requirements:\u2711 Maximize the performance of data loading operations to Staging.\n\nWebSessions.\u2711 Minimize query times for reporting queries against the dimensional model.\n\nWhich type of table distribution should you use for each table? To answer, drag the appropriate table distribution types to the correct tables. Each table distribution type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\nSelect and Place:\nCommon.Data:\nMarketing.Web.Sessions:\nStaging.Web.Sessions:\n",
    "question_type": "multiple",
    "choices": ["X Hash","X Replicated","X round-Robin","Y Hash","Y Replicated","Y Round-robin","Z Hash","Z Replicated","Z Round-robin"],
    "site_answers": ["X Replicated","Y Hash","Z Round-robin"]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to create a table named FactInternetSales that will be a large fact table in a dimensional model. FactInternetSales will contain 100 million rows and two columns named SalesAmount and OrderQuantity. Queries executed on FactInternetSales will aggregate the values in SalesAmount and OrderQuantity from the last year for a specific product. The solution must minimize the data size and query execution time.\n\nHow should you complete the code? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nWITH: X\n, DISTRIBUTION: Y",
    "question_type": "multiple",
    "choices": ["(X CLUSTERED COLUMNSTORE INDEX","X (CLUSTERED INDEX ([OrderDateKey])","(X HEAP","(X INDEX on [ProductKey]",
      "Y Hash([OrderDateKey])","Y Hash([ProductKey])","Y REPLICATE","ROUND_ROBIN"],
    "site_answers": ["(X CLUSTERED COLUMNSTORE INDEX","Y Hash([ProductKey])"]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1. Table1 contains the following:\u2711 One billion rows\u2711 A clustered columnstore index\u2711 A hash-distributed column named Product Key\u2711 A column named Sales Date that is of the date data type and cannot be nullThirty million rows will be added to Table1 each month.\n\nYou need to partition Table1 based on the Sales Date column. The solution must optimize query performance and data loading.\n\nHow often should you create a partition?",
    "question_type": "single",
    "choices": [
      "once per month",
      "once per year",
      "once per day",
      "once per week"
    ],
    "site_answers": [
      "once per month"
    ]
  },
  {
    "question_text": "You have an Azure Databricks workspace that contains a Delta Lake dimension table named Table1.\n\nTable1 is a Type 2 slowly changing dimension (SCD) table.\n\nYou need to apply updates from a source table to Table1.\n\nWhich Apache Spark SQL operation should you use?",
    "question_type": "single",
    "choices": [
      "CREATE",
      "UPDATE",
      "ALTER",
      "MERGE"
    ],
    "site_answers": [
      "MERGE"
    ]
  },
  {
    "question_text": "You are designing an Azure Data Lake Storage solution that will transform raw JSON files for use in an analytical workload.\n\nYou need to recommend a format for the transformed files. The solution must meet the following requirements:\u2711 Contain information about the data types of each column in the files.\u2711 Support querying a subset of columns in the files.\u2711 Support read-heavy analytical workloads.\u2711 Minimize the file size.\n\nWhat should you recommend?",
    "question_type": "single",
    "choices": [
      "JSON",
      "CSV",
      "Apache Avro",
      "Apache Parquet"
    ],
    "site_answers": [
      "Apache Parquet"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.\n\nYou plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.\n\nYou need to prepare the files to ensure that the data copies quickly.\n\nSolution: You modify the files to ensure that each row is less than 1 MB.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "Yes"
    ]
  },
  {
    "question_text": "You plan to create a dimension table in Azure Synapse Analytics that will be less than 1 GB.\n\nYou need to create the table to meet the following requirements:\u2711 Provide the fastest query time.\u2711 Minimize data movement during queries.\n\nWhich type of table should you use?",
    "question_type": "single",
    "choices": [
      "replicated",
      "hash distributed",
      "heap",
      "round-robin"
    ],
    "site_answers": [
      "replicated"
    ]
  },
  {
    "question_text": "You are designing a dimension table in an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to create a surrogate key for the table. The solution must provide the fastest query performance.\n\nWhat should you use for the surrogate key?",
    "question_type": "single",
    "choices": [
      "a GUID column",
      "a sequence object",
      "an IDENTITY column"
    ],
    "site_answers": [
      "an IDENTITY column"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to create a fact table named Table1 that will store sales data from the last three years. The solution must be optimized for the following query operations:\u2022\tShow order counts by week.\u2022\tCalculate sales totals by region.\u2022\tCalculate sales totals by product.\u2022\tFind all the orders from a given month.\n\nWhich data should you use to partition Table1?",
    "question_type": "single",
    "choices": [
      "product",
      "month",
      "week",
      "region"
    ],
    "site_answers": [
      "month"
    ]
  },
  {
    "question_text": "You are designing the folder structure for an Azure Data Lake Storage Gen2 account.\n\nYou identify the following usage patterns:\u2022\tUsers will query data by using Azure Synapse Analytics serverless SQL pools and Azure Synapse Analytics serverless Apache Spark pools.\u2022\tMost queries will include a filter on the current year or week.\u2022\tData will be secured by data source.\n\nYou need to recommend a folder structure that meets the following requirements:\u2022\tSupports the usage patterns\u2022\tSimplifies folder security\u2022\tMinimizes query timesWhich folder structure should you recommend?",
    "question_type": "single",
    "choices": [
      "\\DataSource\\SubjectArea\\YYYY\\WW\\FileData_YYYY_MM_DD.parquet",
      "\\DataSource\\SubjectArea\\YYYY-WW\\FileData_YYYY_MM_DD.parquet",
      "DataSource\\SubjectArea\\WW\\YYYY\\FileData_YYYY_MM_DD.parquet",
      "\\YYYY\\WW\\DataSource\\SubjectArea\\FileData_YYYY_MM_DD.parquet",
      "WW\\YYYY\\SubjectArea\\DataSource\\FileData_YYYY_MM_DD.parquet"
    ],
    "site_answers": [
      "\\DataSource\\SubjectArea\\YYYY\\WW\\FileData_YYYY_MM_DD.parquet"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 contains a table named table1.\n\nYou load 5 TB of data into table1.\n\nYou need to ensure that columnstore compression is maximized for table1.\n\nWhich statement should you execute?",
    "question_type": "single",
    "choices": [
      "DBCC INDEXDEFRAG (pool1, table1)",
      "DBCC DBREINDEX (table1)",
      "ALTER INDEX ALL on table1 REORGANIZE",
      "ALTER INDEX ALL on table1 REBUILD"
    ],
    "site_answers": [
      "ALTER INDEX ALL on table1 REBUILD"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool named pool1.\n\nYou plan to implement a star schema in pool and create a new table named DimCustomer by using the following code.\n\nYou need to ensure that DimCustomer has the necessary columns to support a Type 2 slowly changing dimension (SCD).\n\nWhich two columns should you add? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "[HistoricalSalesPerson] [nvarchar] (256) NOT NULL",
      "[EffectiveEndDate] [datetime] NOT NULL",
      "[PreviousModifiedDate] [datetime] NOT NULL",
      "[RowID] [bigint] NOT NULL",
      "[EffectiveStartDate] [datetime] NOT NULL"
    ],
    "site_answers": [
      "[EffectiveEndDate] [datetime] NOT NULL",
      "[EffectiveStartDate] [datetime] NOT NULL"
    ]
  },
  {
    "question_text": "You have an Azure subscription that contains an Azure Data Lake Storage Gen2 account named account1 and an Azure Synapse Analytics workspace named workspace1.\n\nYou need to create an external table in a serverless SQL pool in workspace1. The external table will reference CSV files stored in account1. The solution must maximize performance.\n\nHow should you configure the external table?",
    "question_type": "single",
    "choices": [
      "Use a native external table and authenticate by using a shared access signature (SAS).",
      "Use a native external table and authenticate by using a storage account key.",
      "Use an Apache Hadoop external table and authenticate by using a shared access signature (SAS).",
      "Use an Apache Hadoop external table and authenticate by using a service principal in Microsoft Azure Active Directory (Azure AD), part of Microsoft Entra."
    ],
    "site_answers": [
      "Use a native external table and authenticate by using a shared access signature (SAS)."
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse Analytics serverless SQL pool that contains a database named db1. The data model for db1 is shown in the following exhibit.\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the exhibit.\n7 tables all star connect expect 2: DimGeography and DimCustomer\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": ["To create star schema: Join DimGeography and DimCustomer","After star schema converted, will be 6 tables"],
    "site_answers": ["To create star schema: Join DimGeography and DimCustomer","After star schema converted, will be 6 tables"]
  },
  {
    "question_text": "You have an Azure Databricks workspace and an Azure Data Lake Storage Gen2 account named storage1.\n\nNew files are uploaded daily to storage1.\n\nYou need to recommend a solution that configures storage1 as a structured streaming source. The solution must meet the following requirements:\u2022\tIncrementally process new files as they are uploaded to storage1.\u2022\tMinimize implementation and maintenance effort.\u2022\tMinimize the cost of processing millions of files.\u2022\tSupport schema inference and schema drift.\n\nWhich should you include in the recommendation?",
    "question_type": "single",
    "choices": [
      "COPY INTO",
      "Azure Data Factory",
      "Auto Loader",
      "Apache Spark FileStreamSource"
    ],
    "site_answers": [
      "Auto Loader"
    ]
  },
  {
    "question_text": "You have an Azure subscription that contains the resources shown in the following table.\n\nName      Type                               Description\nstorage1  Azure Blob storage account         Contains publicly accessible TSV files that\n                                             do NOT have a header row\nWS1       Azure Synapse Analytics workspace  Contains a serverless SQL pool\n\nYou need to read the TSV files by using ad-hoc queries and the OPENROWSET function. The solution must assign a name and override the inferred data type of each column.\n\nWhat should you include in the OPENROWSET function?",
    "question_type": "single",
    "choices": [
      "the WITH clause",
      "the ROWSET_OPTIONS bulk option",
      "the DATAFILETYPE bulk option",
      "the DATA_SOURCE parameter"
    ],
    "site_answers": [
      "the WITH clause"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool.\n\nYou plan to create a fact table named Table1 that will contain a clustered columnstore index.\n\nYou need to optimize data compression and query performance for Table1.\n\nWhat is the minimum number of rows that Table1 should contain before you create partitions?",
    "question_type": "single",
    "choices": [
      "100,000",
      "600,000",
      "1 million",
      "60 million"
    ],
    "site_answers": [
      "60 million"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool that contains a table named DimSalesPerson. DimSalesPerson contains the following columns:\u2022\tRepSourceID\u2022\tSalesRepID\u2022\tFirstName\u2022\tLastName\u2022\tStartDate\u2022\tEndDate\u2022\tRegionYou are developing an Azure Synapse Analytics pipeline that includes a mapping data flow named Dataflow1. Dataflow1 will read sales team data from an external source and use a Type 2 slowly changing dimension (SCD) when loading the data into DimSalesPerson.\n\nYou need to update the last name of a salesperson in DimSalesPerson.\n\nWhich two actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Update three columns of an existing row.",
      "Update two columns of an existing row.",
      "Insert an extra row.",
      "Update one column of an existing row."
    ],
    "site_answers": [
      "Insert an extra row.",
      "Update one column of an existing row."
    ]
  },
  {
    "question_text": "HOTSPOT -You plan to use an Azure Data Lake Storage Gen2 account to implement a Data Lake development environment that meets the following requirements:\u2022\tRead and write access to data must be maintained if an availability zone becomes unavailable.\u2022\tData that was last modified more than two years ago must be deleted automatically.\u2022\tCosts must be minimized.\n\nWhat should you configure? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n Storage redundancy: X\nData selection: Y",
    "question_type": "multiple",
    "choices": [
      "X Geo-zone-redundant storage (GZRS)","X Locally-redundant storage (LRS)","X Zone-redundant storage (ZRS)",
      "Y A lifecycle management policy","Y Soft delete","Y Versioning"
    ],
    "site_answers": ["X Zone-redundant storage (ZRS)","Y A lifecycle management policy"]
  },
  {
    "question_text": "HOTSPOT -You are developing an Azure Synapse Analytics pipeline that will include a mapping data flow named Dataflow1. Dataflow1 will read customer data from an external source and use a Type 1 slowly changing dimension (SCD) when loading the data into a table named DimCustomer in an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to ensure that Dataflow1 can perform the following tasks:\u2022 Detect whether the data of a given customer has changed in the DimCustomer table.\u2022 Perform an upsert to the DimCustomer table.\n\nWhich type of transformation should you use for each task? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nDetect whether the data of a given customer has changed in the DimCustomer table: X\nPerform an upsert to the DimCustomer table: Y",
    "question_type": "multiple",
    "choices": [
      "X: Aggregate","X: Derived column","X: Surrogate key",
      "Y: Alter row","Y: Assert","Y: Cast"
    ],
    "site_answers": [
      "X: Aggregate","Y: Alter"
    ]
  },
  {
    "question_text": "DRAG DROP -You have an Azure Synapse Analytics serverless SQL pool.\n\nYou have an Azure Data Lake Storage account named adls1 that contains a public container named container1. The container1 container contains a folder named folder1.\n\nYou need to query the top 100 rows of all the CSV files in folder1.\n\nHow should you complete the query? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point\n\nSELECT TOP 100 *\n  FROm XXX (\n    YYY 'https://adls1.dfs.core.windows.net/container1/folder1/*.csv',\n    FORMAT = 'CSV' AS rows\n  )",
    "question_type": "multiple",
    "choices": [
      "BULK","DATA_SOURCE","LOCATION","OPENROWSET"
    ],
    "site_answers": [
      "OPENROWSET","BULK"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics workspace named WS1 that contains an Apache Spark pool named Pool1.\n\nYou plan to create a database named DB1 in Pool1.\n\nYou need to ensure that when tables are created in DB1, the tables are available automatically as external tables to the built-in serverless SQL pool.\n\nWhich format should you use for the tables in DB1?",
    "question_type": "single",
    "choices": [
      "Parquet",
      "ORC",
      "JSON",
      "HIVE"
    ],
    "site_answers": [
      "Parquet"
    ]
  },
  {
    "question_text": "You have an Azure Data Lake Storage Gen2 account named storage1.\n\nYou plan to implement query acceleration for storage1.\n\nWhich two file types support query acceleration? Each correct answer presents a complete solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "JSON",
      "Apache Parquet",
      "XML",
      "CSV",
      "Avro"
    ],
    "site_answers": [
      "JSON",
      "CSV"
    ]
  },
  {
    "question_text": "You have an Azure subscription that contains the resources shown in the following table.\n\nYou need to read the files in storage1 by using ad-hoc queries and the OPENROWSET function. The solution must ensure that each rowset contains a single JSON record.\n\nTo what should you set the FORMAT option of the OPENROWSET function?",
    "question_type": "single",
    "choices": [
      "JSON",
      "DELTA",
      "PARQUET",
      "CSV"
    ],
    "site_answers": [
      "CSV"
    ]
  },
  {
    "question_text": "DRAG DROP -You have a data warehouse.\n\nYou need to implement a slowly changing dimension (SCD) named Product that will include three columns named ProductName, ProductColor, and ProductSize. The solution must meet the following requirements:\u2022\tPrevent changes to the values stored in ProductName.\u2022\tRetain only the current and the last values in ProductSize.\u2022\tRetain all the current and previous values in ProductColor.\n\nWhich type of SCD should you implement for each column? To answer, drag the appropriate types to the correct columns. Each type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": ["ProductName","Color","Size","Type 0","Type 1","Type 2","Type 3"],
    "site_answers": ["ProductName","Color","Size","Type 0","Type 2","Type 3"],
  },
  {
    "question_text": "HOTSPOT -You are incrementally loading data into fact tables in an Azure Synapse Analytics dedicated SQL pool.\n\nEach batch of incoming data is staged before being loaded into the fact tables.\n\nYou need to ensure that the incoming data is staged as quickly as possible.\n\nHow should you configure the staging tables? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nTable distribution: X\nTable structure: Y",
    "question_type": "multiple",
    "choices": ["X HASH","X REPLICATE","X ROUND_ROBIN","Y Clustered index","Y Columnstore index","Y Heap"],
    "site_answers": ["X ROUND_ROBIN","Y Heap"]
  },
  {
    "question_text": "You have an Azure subscription that contains an Azure Synapse Analytics workspace named ws1 and an Azure Cosmos DB database account named Cosmos1. Cosmos1 contains a container named container1 and ws1 contains a serverless SQL pool.\n\nYou need to ensure that you can query the data in container1 by using the serverless SQL pool.\n\nWhich three actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Enable Azure Synapse Link for Cosmos1.",
      "Disable the analytical store for container1.",
      "In ws1, create a linked service that references Cosmos1.",
      "Enable the analytical store for container1.",
      "Disable indexing for container1."
    ],
    "site_answers": [
      "Enable Azure Synapse Link for Cosmos1.",
      "In ws1, create a linked service that references Cosmos1.",
      "Enable the analytical store for container1."
    ]
  },
  {
    "question_text": "DRAG DROP -You have an Azure subscription that contains an Azure Data Lake Storage Gen2 account named account1 and a user named User1.\n\nIn account1, you create a container named container1. In container1, you create a folder named folder1.\n\nYou need to ensure that User1 can list and read all the files in folder1. The solution must use the principle of least privilege.\n\nHow should you configure the permissions for each folder? To answer, drag the appropriate permissions to the correct folders. Each permission may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\ncontainer1/: XXX\ncontainer1/folder1: YYY",
    "question_type": "multiple",
    "choices": ["Execute","None","Read","Read an Write","Read and Execute","Write"],
    "site_answers": ["Execute","Read and Execute"]
  },
  {
    "question_text": "You have an Azure Data Factory pipeline named pipeline1.\n\nYou need to execute pipeline1 at 2 AM every day. The solution must ensure that if the trigger for pipeline1 stops, the next pipeline execution will occur at 2 AM, following a restart of the trigger.\n\nWhich type of trigger should you create?",
    "question_type": "single",
    "choices": [
      "schedule",
      "tumbling",
      "storage event",
      "custom event"
    ],
    "site_answers": [
      "schedule"
    ]
  },
  {
    "question_text": "You manage an enterprise data warehouse in Azure Synapse Analytics.\n\nUsers report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.\n\nYou need to monitor resource utilization to determine the source of the performance issues.\n\nWhich metric should you monitor?",
    "question_type": "single",
    "choices": [
      "DWU limit",
      "Cache hit percentage",
      "Local tempdb percentage",
      "Data IO percentage"
    ],
    "site_answers": [
      "Cache hit percentage"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse Analytics serverless SQL pool.\n\nYou have an Apache Parquet file that contains 10 columns.\n\nYou need to query data from the file. The solution must return only two columns.\n\nHow should you complete the query? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": ["Bulk because Parquet + Serverless SQL","columns as rows https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-files"],
    "site_answers": ["Bulk because Parquet + Serverless SQL","columns as rows https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-files"],
  },
  {
    "question_text": "You have an Azure Synapse Analytics workspace that contains an Apache Spark pool named SparkPool1. SparkPool1 contains a Delta Lake table named SparkTable1.\n\nYou need to recommend a solution that supports Transact-SQL queries against the data referenced by SparkTable1. The solution must ensure that the queries can use partition elimination.\n\nWhat should you include in the recommendation?",
    "question_type": "single",
    "choices": [
      "a partitioned table in a dedicated SQL pool",
      "a partitioned view in a dedicated SQL pool",
      "a partitioned index in a dedicated SQL pool",
      "a partitioned view in a serverless SQL pool"
    ],
    "site_answers": [
      "a partitioned view in a serverless SQL pool"
    ]
  },
  {
    "question_text": "You are designing a sales transactions table in an Azure Synapse Analytics dedicated SQL pool. The table will contain approximately 60 million rows per month and will be partitioned by month. The table will use a clustered column store index and round-robin distribution.\n\nApproximately how many rows will there be for each combination of distribution and partition?",
    "question_type": "single",
    "choices": [
      "1 million",
      "5 million",
      "20 million",
      "60 million"
    ],
    "site_answers": [
      "1 million"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics workspace.\n\nYou plan to deploy a lake database by using a database template in Azure Synapse.\n\nWhich two elements are included in the template? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "relationships",
      "data formats",
      "linked services",
      "table permissions",
      "table definitions"
    ],
    "site_answers": [
      "relationships",
      "table definitions"
    ]
  },
  {
    "question_text": "You are implementing a star schema in an Azure Synapse Analytics dedicated SQL pool.\n\nYou plan to create a table named DimProduct.\n\nDimProduct must be a Type 3 slowly changing dimension (SCD) table that meets the following requirements:\u2022\tThe values in two columns named ProductKey and ProductSourceID will remain the same.\u2022\tThe values in three columns named ProductName, ProductDescription, and Color can change.\n\nYou need to add additional columns to complete the following table definition.\n\nWhich three columns should you add? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "[EffectiveStartDate] [datetime] NOT NULL",
      "[EffectiveEndDate] [datetime] NOT NULL",
      "[OriginalProductDescription] NVARCHAR(2000) NOT NULL",
      "[IsCurrentRow] [bit] NOT NULL",
      "[OriginalColor] NVARCHAR(50) NOT NULL",
      "[OriginalProductName] NVARCHAR(100) NULL"
    ],
    "site_answers": [
      "[OriginalProductDescription] NVARCHAR(2000) NOT NULL",
      "[OriginalColor] NVARCHAR(50) NOT NULL",
      "[OriginalProductName] NVARCHAR(100) NULL"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 contains a fact table named Table1. Table1 contains sales data. Sixty-five million rows of data are added to Table1 monthly.\n\nAt the end of each month, you need to remove data that is older than 36 months. The solution must minimize how long it takes to remove the data.\n\nHow should you partition Table1, and how should you remove the old data? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": ["Partition per month because monthly","Switch the olfest partition to other table and drop other table"],
    "site_answers": ["Partition per month because monthly","Switch the olfest partition to other table and drop other table"],
  },
  {
    "question_text": "You have an Azure subscription that contains an Azure Synapse Analytics serverless SQL pool.\n\nYou execute the following query.\n\nCREATE EXTERNAL TABLE Order [...]\nWhere will the rows returned by the query be stored?",
    "question_type": "single",
    "choices": [
      "in a file in a data lake",
      "in a relational database",
      "in a global temporary table",
      "in a session temporary table",
      "https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&tabs=dedicated#overview-azure-synapse-analytics"
    ],
    "site_answers": [
      "in a file in a data lake"
    ]
  },
  {
    "question_text": "You are deploying a lake database by using an Azure Synapse database template.\n\nYou need to add additional tables to the database. The solution must use the same grouping method as the template tables.\n\nWhich grouping method should you use?",
    "question_type": "single",
    "choices": [
      "partition style",
      "business area",
      "size",
      "facts and dimensions"
    ],
    "site_answers": [
      "business area"
    ]
  },
  {
    "question_text": "You have an Azure data factory connected to a Git repository that contains the following branches:\u2022\tmain: Collaboration branch\u2022\tabc: Feature branch\u2022\txyz: Feature branchYou save changes to a pipeline in the xyz branch.\n\nYou need to publish the changes to the live service.\n\nWhat should you do first?",
    "question_type": "single",
    "choices": [
      "Publish the data factory.",
      "Create a pull request to merge the changes into the main branch.",
      "Create a pull request to merge the changes into the abc branch.",
      "Push the code to a remote origin."
    ],
    "site_answers": [
      "Create a pull request to merge the changes into the main branch."
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure subscription that contains an Azure data factory named ADF1.\n\nFrom Azure Data Factory Studio, you build a complex data pipeline in ADF1.\n\nYou discover that the Save button is unavailable, and there are validation errors that prevent the pipeline from being published.\n\nYou need to ensure that you can save the logic of the pipeline.\n\nSolution: You enable Git integration for ADF1.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "Yes"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure subscription that contains an Azure data factory named ADF1.\n\nFrom Azure Data Factory Studio, you build a complex data pipeline in ADF1.\n\nYou discover that the Save button is unavailable, and there are validation errors that prevent the pipeline from being published.\n\nYou need to ensure that you can save the logic of the pipeline.\n\nSolution: You view the JSON code representation of the resource and copy the JSON to a file.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "Yes"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure subscription that contains an Azure data factory named ADF1.\n\nFrom Azure Data Factory Studio, you build a complex data pipeline in ADF1.\n\nYou discover that the Save button is unavailable, and there are validation errors that prevent the pipeline from being published.\n\nYou need to ensure that you can save the logic of the pipeline.\n\nSolution: You export ADF1 as an Azure Resource Manager (ARM) template.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "No"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Databricks workspace.\n\nYou read data from a CSV file by using a notebook, and then load the data to a DataFrame.\n\nYou need to add rows from the DataFrame to an existing Delta table by using Python code.\n\nHow should you complete the code? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nnew_rows_df.write.format(\"X\").mode\"Y\").save(delta_table_path)",
    "question_type": "multiple",
    "choices": ["csv","delta","json","parquet","append","ignore","overwrite","error"],
    "site_answers": ["delta","append"]
  },
  {
    "question_text": "DRAG DROP -You have an Azure subscription that contains an Azure Cosmos DB for NoSQL account named account1. The account1 account contains a container named Container1 that has the following configurations:\u2022\tAnalytical store: On\u2022\tTTL: 3600You need to remove analytical store support from Container1. The solution must meet the following requirements:\u2022\tMinimize the impact on the apps that reference Container1.\u2022\tMinimize storage usage.\n\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.",
    "question_type": "multiple",
    "choices": [],
    "site_answers": []
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure subscription that contains an Azure data factory named ADF1.\n\nFrom Azure Data Factory Studio, you build a complex data pipeline in ADF1.\n\nYou discover that the Save button is unavailable, and there are validation errors that prevent the pipeline from being published.\n\nYou need to ensure that you can save the logic of the pipeline.\n\nSolution: You disable all the triggers for ADF1.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "No"
    ]
  },
  {
    "question_text": "You have an Azure subscription that contains the resources shown in the following table.\n\nYou need to implement Azure Synapse Link for Azure SQL Database.\n\nWhich two actions should you perform on sql1? Each correct answer presents a part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Update the firewall rules to allow Azure services to access sql1.",
      "Enable the system-assigned managed identity.",
      "From the Access control (IAM) settings, assign the Contributor role to the system-assigned managed identity of workspace1.",
      "Disable Transparent Data Encryption (TDE).",
      "https://learn.microsoft.com/en-us/azure/synapse-analytics/synapse-link/connect-synapse-link-sql-database"
    ],
    "site_answers": [
      "Update the firewall rules to allow Azure services to access sql1.",
      "Enable the system-assigned managed identity."
    ]
  },
  {
    "question_text": "You have an Azure subscription that contains an Azure Cosmos DB database. Azure Synapse Link is implemented on the database.\n\nYou configure a full fidelity schema for the analytical store.\n\nYou perform the following actions:\u2022\tInsert {\"customerID\": 12, \"customer\": \u201cTailspin Toys\"} as the first document in the container.\u2022\tInsert {\"customerID\": \"14\", \"customer\": \"Contoso\"} as the second document in the container.\n\nHow many columns will the analytical store contain?",
    "question_type": "single",
    "choices": [
      "1",
      "2",
      "3",
      "4"
    ],
    "site_answers": [
      "3"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Data Lake Storage account that contains CSV files. The CSV files contain sales order data and are partitioned by using the following format./data/salesorders/year=xxxx/month=yYou need to retrieve only the sales orders from January 2023 and February 2023.\n\nHow should you complete the query? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": ["so.filepath(0) = 2023 AND so.filepath (1) IN (1,2)", "so.year = 2023 AND (so.month = 1 or so.month = 2)", "so.filepath(1) = '2023' AND so.filepath (2) IN ('1','2')"],
    "site_answers": ["so.filepath(1) = '2023' AND so.filepath (2) IN ('1','2')"]
  },
  {
    "question_text": "HOTSPOT -You plan to create a real-time monitoring app that alerts users when a device travels more than 200 meters away from a designated location.\n\nYou need to design an Azure Stream Analytics job to process the data for the planned app. The solution must minimize the amount of code developed and the number of technologies used.\n\nWhat should you include in the Stream Analytics job? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:",
    "question_type": "multiple",
    "choices": ["Input: Stream","Input: Reference", "Function: Aggregate","Function: Geospatial","Function: Windowing"],
    "site_answers": ["Input: Stream","Function: Geospatial"]
  },
  {
    "question_text": "A company has a real-time data analysis solution that is hosted on Microsoft Azure. The solution uses Azure Event Hub to ingest data and an Azure StreamAnalytics cloud job to analyze the data. The cloud job is configured to use 120 Streaming Units (SU).\n\nYou need to optimize performance for the Azure Stream Analytics job.\n\nWhich two actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Implement event ordering.",
      "Implement Azure Stream Analytics user-defined functions (UDF).",
      "Implement query parallelization by partitioning the data output.",
      "Scale the SU count for the job up.",
      "Scale the SU count for the job down.",
      "Implement query parallelization by partitioning the data input."
    ],
    "site_answers": [
      "Implement query parallelization by partitioning the data output.",
      "Implement query parallelization by partitioning the data input."
    ]
  },
  {
    "question_text": "You need to trigger an Azure Data Factory pipeline when a file arrives in an Azure Data Lake Storage Gen2 container.\n\nWhich resource provider should you enable?",
    "question_type": "single",
    "choices": [
      "Microsoft.Sql",
      "Microsoft.Automation",
      "Microsoft.EventGrid",
      "Microsoft.EventHub"
    ],
    "site_answers": [
      "Microsoft.EventGrid"
    ]
  },
  {
    "question_text": "You plan to perform batch processing in Azure Databricks once daily.\n\nWhich type of Databricks cluster should you use?",
    "question_type": "single",
    "choices": [
      "High Concurrency",
      "automated",
      "interactive"
    ],
    "site_answers": [
      "automated"
    ]
  },
  {
    "question_text": "HOTSPOT -You are processing streaming data from vehicles that pass through a toll booth.\n\nYou need to use Azure Stream Analytics to return the license plate, vehicle make, and hour the last vehicle passed during each 10-minute window.\n\nHow should you complete the query? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:",
    "question_type": "multiple",
    "choices": ["MAX (Time), TumbinlWindows minute (10), DATEDIFF(minute, Input, LastInWindow)"]
    "site_answers": ["MAX (Time), TumbinlWindows minute (10), DATEDIFF(minute, Input, LastInWindow)"]
  },
  {
    "question_text": "You have an Azure Data Factory instance that contains two pipelines named Pipeline1 and Pipeline2.\n\nPipeline1 has the activities shown in the following exhibit.\n\nPipeline2 has the activities shown in the following exhibit.\n\nYou execute Pipeline2, and Stored procedure1 in Pipeline1 fails.\n\nWhat is the status of the pipeline runs?",
    "question_type": "single",
    "choices": [
      "Pipeline1 and Pipeline2 succeeded.",
      "Pipeline1 and Pipeline2 failed.",
      "Pipeline1 succeeded and Pipeline2 failed.",
      "Pipeline1 failed and Pipeline2 succeeded."
    ],
    "site_answers": [
      "Pipeline1 and Pipeline2 succeeded."
    ]
  },
  {
    "question_text": "You have an Azure Data Factory that contains 10 pipelines.\n\nYou need to label each pipeline with its main purpose of either ingest, transform, or load. The labels must be available for grouping and filtering when using the monitoring experience in Data Factory.\n\nWhat should you add to each pipeline?",
    "question_type": "single",
    "choices": [
      "a resource tag",
      "a correlation ID",
      "a run group ID",
      "an annotation"
    ],
    "site_answers": [
      "an annotation"
    ]
  },
  {
    "question_text": "You are designing a statistical analysis solution that will use custom proprietary Python functions on near real-time data from Azure Event Hubs.\n\nYou need to recommend which Azure service to use to perform the statistical analysis. The solution must minimize latency.\n\nWhat should you recommend?",
    "question_type": "single",
    "choices": [
      "Azure Synapse Analytics",
      "Azure Databricks",
      "Azure Stream Analytics",
      "Azure SQL Database"
    ],
    "site_answers": [
      "Azure Databricks"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an enterprise data warehouse in Azure Synapse Analytics that contains a table named FactOnlineSales. The table contains data from the start of 2009 to the end of 2012.\n\nYou need to improve the performance of queries against FactOnlineSales by using table partitions. The solution must meet the following requirements:\u2711 Create four partitions based on the order date.\u2711 Ensure that each partition contains all the orders placed during a given calendar year.\n\nHow should you complete the T-SQL command? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nCREATE TABLE [dbo] . FactOnlineSales ([...])\nWITH (CLUSTERED COLUMNSTORE INDEX)\nPARTITION ([OrderDateKey] RANGE XXX FOR VALUES\n(YYY))",
    "question_type": "multiple",
    "choices": [
      "X: LEFT","X: RIGHT",
      "Y: 20090101,20121231","X: 20100101,20110101,20120101","X: 20090101,20100101,20110101,20120101"
    ],
    "site_answers": [
      "X: RIGHT","Y: 20100101,20110101,20120101"
    ]
  },
  {
    "question_text": "You need to implement a Type 3 slowly changing dimension (SCD) for product category data in an Azure Synapse Analytics dedicated SQL pool.\n\nYou have a table that was created by using the following Transact-SQL statement.\n\nWhich two columns should you add to the table? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "[EffectiveStartDate] [datetime] NOT NULL,",
      "[CurrentProductCategory] [nvarchar] (100) NOT NULL,",
      "[EffectiveEndDate] [datetime] NULL,",
      "[ProductCategory] [nvarchar] (100) NOT NULL,",
      "[OriginalProductCategory] [nvarchar] (100) NOT NULL,"
    ],
    "site_answers": [
      "[CurrentProductCategory] [nvarchar] (100) NOT NULL,",
      "[OriginalProductCategory] [nvarchar] (100) NOT NULL,"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are designing an Azure Stream Analytics solution that will analyze Twitter data.\n\nYou need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once.\n\nSolution: You use a hopping window that uses a hop size of 10 seconds and a window size of 10 seconds.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "Yes"
    ]
  },
  {
    "question_text": "DRAG DROP -You need to create an Azure Data Factory pipeline to process data for the following three departments at your company: Ecommerce, retail, and wholesale. The solution must ensure that data can also be processed for the entire company.\n\nHow should you complete the Data Factory data flow script? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\nSelect and Place:\n\nCleanData\n  split(\n      XXX\n      YYY\n  ) ~> SplitByDept@(ZZZ)",
    "question_type": "multiple",
    "choices": [
      "all, ecommerce, retail, wholesale","dept == 'ecommerce', dept == 'retail', dept == 'wholesale'",
      "dept == 'ecommerce', dept == 'wholesale', dept == 'retail'",
      "disjoint: false","disjoint: true","ecommerce, retail, wholesale, all"],
    "site_answers": [
      "dept == 'ecommerce', dept == 'retail', dept == 'wholesale'","disjoint: true","ecommerce, retail, wholesale, all"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are designing an Azure Stream Analytics solution that will analyze Twitter data.\n\nYou need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once.\n\nSolution: You use a hopping window that uses a hop size of 5 seconds and a window size 10 seconds.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "No"
    ]
  },
  {
    "question_text": "HOTSPOT -You are designing a near real-time dashboard solution that will visualize streaming data from remote sensors that connect to the internet. The streaming data must be aggregated to show the average value of each 10-second interval. The data will be discarded after being displayed in the dashboard.\n\nThe solution will use Azure Stream Analytics and must meet the following requirements:\u2711 Minimize latency from an Azure Event hub to the dashboard.\u2711 Minimize the required storage.\u2711 Minimize development effort.\n\nWhat should you include in the solution? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one pointHot Area:\n\nAzure Stream Analytics input type:X\nAzure Stream Analytics output type: Y\nAggregation query location: Z",
    "question_type": "multiple",
    "choices": [
      "X: Azure Event Hub","X: Azure SQL Database","X: Azure Stream Analytics","X: Microsoft Power BI",
      "Y: Azure Event Hub","Y: Azure SQL Database","Y: Azure Stream Analytics","Y: Microsoft Power BI",
      "Z: Azure Event Hub","Z: Azure SQL Database","Z: Azure Stream Analytics","Z: Microsoft Power BI"
    ],
    "site_answers": [
      "X: Azure Event Hub","Y: Microsoft Power BI","Z: Azure Stream Analytics"
    ]
  },
  {
    "question_text": "You are creating an Azure Data Factory data flow that will ingest data from a CSV file, cast columns to specified types of data, and insert the data into a table in anAzure Synapse Analytic dedicated SQL pool. The CSV file contains three columns named username, comment, and date.\n\nThe data flow already contains the following:\u2711 A source transformation.\u2711 A Derived Column transformation to set the appropriate types of data.\u2711 A sink transformation to land the data in the pool.\n\nYou need to ensure that the data flow meets the following requirements:\u2711 All valid rows must be written to the destination table.\u2711 Truncation errors in the comment column must be avoided proactively.\u2711 Any rows containing comment values that will cause truncation errors upon insert must be written to a file in blob storage.\n\nWhich two actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "To the data flow, add a sink transformation to write the rows to a file in blob storage.",
      "To the data flow, add a Conditional Split transformation to separate the rows that will cause truncation errors.",
      "To the data flow, add a filter transformation to filter out rows that will cause truncation errors.",
      "Add a select transformation to select only the rows that will cause truncation errors."
    ],
    "site_answers": [
      "To the data flow, add a sink transformation to write the rows to a file in blob storage.",
      "To the data flow, add a Conditional Split transformation to separate the rows that will cause truncation errors."
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure SQL database named Database1 and two Azure event hubs named HubA and HubB. The data consumed from each source is shown in the following table.\n\nYou need to implement Azure Stream Analytics to calculate the average fare per mile by driver.\n\nHow should you configure the Stream Analytics input for each source? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nHubA: X\nHubB: Y\nDatabase1: Z\n",
    "question_type": "multiple",
    "choices": [
      "X: Stream","X: Reference",
      "Y: Stream","Y: Reference",
      "Z: Stream","Z: Reference"
    ],
    "site_answers": [
      "X: Stream","Y: Stream","Z: Reference"
    ]
  },
  {
    "question_text": "You have an Azure Storage account and a data warehouse in Azure Synapse Analytics in the UK South region.\n\nYou need to copy blob data from the storage account to the data warehouse by using Azure Data Factory. The solution must meet the following requirements:\u2711 Ensure that the data remains in the UK South region at all times.\u2711 Minimize administrative effort.\n\nWhich type of integration runtime should you use?",
    "question_type": "single",
    "choices": [
      "Azure integration runtime",
      "Azure-SSIS integration runtime",
      "Self-hosted integration runtime"
    ],
    "site_answers": [
      "Azure integration runtime"
    ]
  },
  {
    "question_text": "HOTSPOT -You are building an Azure Analytics query that will receive input data from Azure IoT Hub and write the results to Azure Blob storage.\n\nYou need to calculate the difference in the number of readings per sensor per hour.\n\nHow should you complete the query? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nSELECT sensorId,\n  growth = reading -\n    XXX (reading) OVER (PARTITION BY sensorId YYY (hour,1))\nFROM input",
    "question_type": "multiple",
    "choices": [
      "X: LAG","X: LAST","X: LEAD",
      "Y: LIMIT DURATION","Y: OFFSET","Y: WHEN"
    ],
    "site_answers": [
      "X: LAG","Y: LIMIT DURATION"
    ]
  },
  {
    "question_text": "You have an Azure Stream Analytics job that receives clickstream data from an Azure event hub.\n\nYou need to define a query in the Stream Analytics job. The query must meet the following requirements:\u2711 Count the number of clicks within each 10-second window based on the country of a visitor.\u2711 Ensure that each click is NOT counted more than once.\n\nHow should you define the Query?",
    "question_type": "single",
    "choices": [
      "SELECT Country, Avg(*) AS Average FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, SlidingWindow(second, 10)",
      "SELECT Country, Count(*) AS Count FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, TumblingWindow(second, 10)",
      "SELECT Country, Avg(*) AS Average FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, HoppingWindow(second, 10, 2)",
      "SELECT Country, Count(*) AS Count FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, SessionWindow(second, 5, 10)"
    ],
    "site_answers": [
      "SELECT Country, Count(*) AS Count FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, TumblingWindow(second, 10)"
    ]
  },
  {
    "question_text": "You need to schedule an Azure Data Factory pipeline to execute when a new file arrives in an Azure Data Lake Storage Gen2 container.\n\nWhich type of trigger should you use?",
    "question_type": "single",
    "choices": [
      "on-demand",
      "tumbling window",
      "schedule",
      "event"
    ],
    "site_answers": [
      "event"
    ]
  },
  {
    "question_text": "You have two Azure Data Factory instances named ADFdev and ADFprod. ADFdev connects to an Azure DevOps Git repository.\n\nYou publish changes from the main branch of the Git repository to ADFdev.\n\nYou need to deploy the artifacts from ADFdev to ADFprod.\n\nWhat should you do first?",
    "question_type": "single",
    "choices": [
      "From ADFdev, modify the Git configuration.",
      "From ADFdev, create a linked service.",
      "From Azure DevOps, create a release pipeline.",
      "From Azure DevOps, update the main branch."
    ],
    "site_answers": [
      "From Azure DevOps, create a release pipeline."
    ]
  },
  {
    "question_text": "You are developing a solution that will stream to Azure Stream Analytics. The solution will have both streaming data and reference data.\n\nWhich input type should you use for the reference data?",
    "question_type": "single",
    "choices": [
      "Azure Cosmos DB",
      "Azure Blob storage",
      "Azure IoT Hub",
      "Azure Event Hubs"
    ],
    "site_answers": [
      "Azure Blob storage"
    ]
  },
  {
    "question_text": "You are designing an Azure Stream Analytics job to process incoming events from sensors in retail environments.\n\nYou need to process the events to produce a running average of shopper counts during the previous 15 minutes, calculated at five-minute intervals.\n\nWhich type of window should you use?",
    "question_type": "single",
    "choices": [
      "snapshot",
      "tumbling",
      "hopping",
      "sliding"
    ],
    "site_answers": [
      "hopping"
    ]
  },
  {
    "question_text": "You are designing an Azure Databricks table. The table will ingest an average of 20 million streaming events per day.\n\nYou need to persist the events in the table for use in incremental load pipeline jobs in Azure Databricks. The solution must minimize storage costs and incremental load times.\n\nWhat should you include in the solution?",
    "question_type": "single",
    "choices": [
      "Partition by DateTime fields.",
      "Sink to Azure Queue storage.",
      "Include a watermark column.",
      "Use a JSON format for physical data storage."
    ],
    "site_answers": [
      "Partition by DateTime fields."
    ]
  },
  {
    "question_text": "HOTSPOT -You have a self-hosted integration runtime in Azure Data Factory.\n\nThe current status of the integration runtime has the following configurations:\u2711 Status: Running\u2711 Type: Self-Hosted\u2711 Version: 4.4.7292.1\u2711 Running / Registered Node(s): 1/1\u2711 High Availability Enabled: False\u2711 Linked Count: 0\u2711 Queue Length: 0\u2711 Average Queue Duration. 0.00sThe integration runtime has the following node details:\u2711 Name: X-M\u2711 Status: Running\u2711 Version: 4.4.7292.1\u2711 Available Memory: 7697MB\u2711 CPU Utilization: 6%\u2711 Network (In/Out): 1.21KBps/0.83KBps\u2711 Concurrent Jobs (Running/Limit): 2/14\u2711 Role: Dispatcher/Worker\u2711 Credential Status: In SyncUse the drop-down menus to select the answer choice that completes each statement based on the information presented.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nIf the X-M node becomes unavailable, all executed pipelines will: X\n\nThe number of concurrent jobs and the CPU usage indicate that the Concurrent Jobs (Running/Limit) value should be: Y",
    "question_type": "multiple",
    "choices": [
      "X: fail until the node comes back online","X: switch to another integration runtime","X: exceed the CPU limit",
      "Y: raised","Y: lowered","Y: left as is"
    ],
    "site_answers": [
      "X: fail until the node comes back online","Y: lowered"
    ]
  },
  {
    "question_text": "You have an Azure Databricks workspace named workspace1 in the Standard pricing tier.\n\nYou need to configure workspace1 to support autoscaling all-purpose clusters. The solution must meet the following requirements:\u2711 Automatically scale down workers when the cluster is underutilized for three minutes.\u2711 Minimize the time it takes to scale to the maximum number of workers.\u2711 Minimize costs.\n\nWhat should you do first?",
    "question_type": "single",
    "choices": [
      "Enable container services for workspace1.",
      "Upgrade workspace1 to the Premium pricing tier.",
      "Set Cluster Mode to High Concurrency.",
      "Create a cluster policy in workspace1."
    ],
    "site_answers": [
      "Upgrade workspace1 to the Premium pricing tier."
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are designing an Azure Stream Analytics solution that will analyze Twitter data.\n\nYou need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once.\n\nSolution: You use a tumbling window, and you set the window size to 10 seconds.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "Yes"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are designing an Azure Stream Analytics solution that will analyze Twitter data.\n\nYou need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once.\n\nSolution: You use a session window that uses a timeout size of 10 seconds.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "No"
    ]
  },
  {
    "question_text": "You use Azure Stream Analytics to receive data from Azure Event Hubs and to output the data to an Azure Blob Storage account.\n\nYou need to output the count of records received from the last five minutes every minute.\n\nWhich windowing function should you use?",
    "question_type": "single",
    "choices": [
      "Session",
      "Tumbling",
      "Sliding",
      "Hopping"
    ],
    "site_answers": [
      "Hopping"
    ]
  },
  {
    "question_text": "HOTSPOT -You configure version control for an Azure Data Factory instance as shown in the following exhibit.\n\ngit repo\nrepo_name: dwh_batchetl\ncollab branch: main\npublish_branch: adf_publish\nroo folder: /\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nARM templates for the pipeline assets are stored in X.\nA Data Factory ARM template named contososales can be found in Y.",
    "question_type": "multiple",
    "choices": [
      "X /","X adf_publish","X main","X Parameterization template",
      "Y /","Y /contososales","Y /dwh_batchetl/adf_publish/contososales","Y /main"
    ],
    "site_answers": [
      "X adf_publish","Y /dwh_batchetl/adf_publish/contososales"
    ]
  },
  {
    "question_text": "HOTSPOT -You are designing an Azure Stream Analytics solution that receives instant messaging data from an Azure Event Hub.\n\nYou need to ensure that the output from the Stream Analytics job counts the number of messages per time zone every 15 seconds.\n\nHow should you complete the Stream Analytics query? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nSelect TimeZone, count (*) AS MessageCount\nFROM MessageStream XXX CreatedAt\nGROUP BY TimeZone, YYY (second, 15)",
    "question_type": "multiple",
    "choices": [
      "X LAST","X OVER","X SYSTEM.TIMESTAMP()","X TIMESTAMP BY",
      "Y HOPPINGWINDOW","Y SESSIONWINDOW","Y SLIDINGWINDOW","Y TUMBLINGWINDOW"
    ],
    "site_answers": [
      "X TIMESTAMP BY","Y TUMBLINGWINDOW"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Data Factory instance named ADF1 and two Azure Synapse Analytics workspaces named WS1 and WS2.\n\nADF1 contains the following pipelines:\u2711 P1: Uses a copy activity to copy data from a nonpartitioned table in a dedicated SQL pool of WS1 to an Azure Data Lake Storage Gen2 account\u2711 P2: Uses a copy activity to copy data from text-delimited files in an Azure Data Lake Storage Gen2 account to a nonpartitioned table in a dedicated SQL pool of WS2You need to configure P1 and P2 to maximize parallelism and performance.\n\nWhich dataset settings should you configure for the copy activity if each pipeline? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:",
    "question_type": "multiple",
    "choices": [
      "P1: Set the Copy method to Bulk insert","P1: Set the Copy method to PolyBase","P1: Set the Isolation level to Repeatable read","P1: Set the Partition option to Dynamic range",
      "P2: Set the Copy method to Bulk insert","P2: Set the Copy method to PolyBase","P2: Set the Isolation level to Repeatable read","P2: Set the Partition option to Dynamic range"
    ],
    "site_answers": [
      "P1: Set the Partition option to Dynamic range","P1: Set the Copy method to PolyBase"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Storage account that generates 200,000 new files daily. The file names have a format of {YYYY}/{MM}/{DD}/{HH}/{CustomerID}.csv.\n\nYou need to design an Azure Data Factory solution that will load new data from the storage account to an Azure Data Lake once hourly. The solution must minimize load times and costs.\n\nHow should you configure the solution? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:",
    "question_type": "multiple",
    "choices": [
      "X: Full Load","X: Incremental Load","X: Load individual files as they arrive",
      "Y: Fixed schedule","Y New file","Y Tumbling window"
    ],
    "site_answers": [
      "X: Incremental Load","Y Tumbling window"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:\u2711 A workload for data engineers who will use Python and SQL.\u2711 A workload for jobs that will run notebooks that use Python, Scala, and SQL.\u2711 A workload that data scientists will use to perform ad hoc analysis in Scala and R.\n\nThe enterprise architecture team at your company identifies the following standards for Databricks environments:\u2711 The data engineers must share a cluster.\u2711 The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.\u2711 All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.\n\nYou need to create the Databricks clusters for the workloads.\n\nSolution: You create a Standard cluster for each data scientist, a Standard cluster for the data engineers, and a High Concurrency cluster for the jobs.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "No"
    ]
  },
  {
    "question_text": "You have the following Azure Data Factory pipelines:\u2711 Ingest Data from System1\u2711 Ingest Data from System2\u2711 Populate Dimensions\u2711 Populate FactsIngest Data from System1 and Ingest Data from System2 have no dependencies. Populate Dimensions must execute after Ingest Data from System1 and IngestData from System2. Populate Facts must execute after Populate Dimensions pipeline. All the pipelines must execute every eight hours.\n\nWhat should you do to schedule the pipelines for execution?",
    "question_type": "single",
    "choices": [
      "Add an event trigger to all four pipelines.",
      "Add a schedule trigger to all four pipelines.",
      "Create a patient pipeline that contains the four pipelines and use a schedule trigger.",
      "Create a patient pipeline that contains the four pipelines and use an event trigger."
    ],
    "site_answers": [
      "Create a patient pipeline that contains the four pipelines and use a schedule trigger."
    ]
  },
  {
    "question_text": "DRAG DROP -You are responsible for providing access to an Azure Data Lake Storage Gen2 account.\n\nYour user account has contributor access to the storage account, and you have the application ID and access key.\n\nYou plan to use PolyBase to load data into an enterprise data warehouse in Azure Synapse Analytics.\n\nYou need to configure PolyBase to connect the data warehouse to storage account.\n\nWhich three components should you create in sequence? To answer, move the appropriate components from the list of components to the answer area and arrange them in the correct order.\n\nSelect and Place:",
    "question_type": "multiple",
    "choices": ["a database scoped credential","an asymmetric key","an external data source","a database encryption key","an external file format"],
    "site_answers": [
      "an asymmetric key","a database scoped credential","an external data source"
    ]
  },
  {
    "question_text": "You are monitoring an Azure Stream Analytics job by using metrics in Azure.\n\nYou discover that during the last 12 hours, the average watermark delay is consistently greater than the configured late arrival tolerance.\n\nWhat is a possible cause of this behavior?",
    "question_type": "single",
    "choices": [
      "Events whose application timestamp is earlier than their arrival time by more than five minutes arrive as inputs.",
      "There are errors in the input data.",
      "The late arrival policy causes events to be dropped.",
      "The job lacks the resources to process the volume of incoming data."
    ],
    "site_answers": [
      "The job lacks the resources to process the volume of incoming data."
    ]
  },
  {
    "question_text": "HOTSPOT -You are building an Azure Stream Analytics job to retrieve game data.\n\nYou need to ensure that the job returns the highest scoring record for each five-minute time interval of each game.\n\nHow should you complete the Stream Analytics query? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nSELECT XXXas HighestScore\nFROM input TIMESTAMP BY CreatedAt\nGROUP BY YYY",
    "question_type": "multiple",
    "choices": [
      "X: Collect(Score)","X: CollectTop(1) OVER(ORDER BY Score Desc)","X: Game, MAX(Score)","X: TopOne() OVER(PARTITION BY Game ORDER BY Score Desc)",
      "Y: Game","Y: Hopping(minute,5)","Y: Tumbling(minute,5)","Y: Windows(TumblingWindow(minute,5),Hopping(minute,5))"
    ],
    "site_answers": [
      "X: TopOne() OVER(PARTITION BY Game ORDER BY Score Desc)","Y: Tumbling(minute,5)"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure Data Lake Storage account that contains a staging zone.\n\nYou need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "No"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:\u2711 A workload for data engineers who will use Python and SQL.\u2711 A workload for jobs that will run notebooks that use Python, Scala, and SQL.\u2711 A workload that data scientists will use to perform ad hoc analysis in Scala and R.\n\nThe enterprise architecture team at your company identifies the following standards for Databricks environments:\u2711 The data engineers must share a cluster.\u2711 The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.\u2711 All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.\n\nYou need to create the Databricks clusters for the workloads.\n\nSolution: You create a High Concurrency cluster for each data scientist, a High Concurrency cluster for the data engineers, and a Standard cluster for the jobs.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "No"
    ]
  },
  {
    "question_text": "You are designing an Azure Databricks cluster that runs user-defined local processes.\n\nYou need to recommend a cluster configuration that meets the following requirements:\u2711 Minimize query latency.\u2711 Maximize the number of users that can run queries on the cluster at the same time.\u2711 Reduce overall costs without compromising other requirements.\n\nWhich cluster type should you recommend?",
    "question_type": "single",
    "choices": [
      "Standard with Auto Termination",
      "High Concurrency with Autoscaling",
      "High Concurrency with Auto Termination",
      "Standard with Autoscaling"
    ],
    "site_answers": [
      "High Concurrency with Autoscaling"
    ]
  },
  {
    "question_text": "HOTSPOT -You are building an Azure Data Factory solution to process data received from Azure Event Hubs, and then ingested into an Azure Data Lake Storage Gen2 container.\n\nThe data will be ingested every five minutes from devices into JSON files. The files have the following naming pattern./{deviceType}/in/{YYYY}/{MM}/{DD}/{HH}/{deviceID}_{YYYY}{MM}{DD}HH}{mm}.jsonYou need to prepare the data for batch data processing so that there is one dataset per hour per deviceType. The solution must minimize read times.\n\nHow should you configure the sink for the copy activity? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nParameter: X\nNaming pattern: Y\nCopy behaviour: Z\n",
    "question_type": "multiple",
    "choices": [
      "X: @pipeline(),TriggerTime","X: @pipeline(),TriggerType","X: @trigger().outputs.windowStartTime","X: @trigger().startTime",
      "Y: /{deviceID}/out/{YYYY}/{MM}/{DD}/{HH}.json","Y: /{YYYY}/{MM}/{DD}/{deviceType}.json","Y: /{YYYY}/{MM}/{DD}/{HH}.json","Y: /{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json",
      "Z: Add dynamic content","Z: Flatten hierarchy","Z: Merge files"
    ],
    "site_answers": [
      "X: @trigger().startTime","Y: /{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json","Z: Merge files"
    ]
  },
  {
    "question_text": "DRAG DROP -You are designing an Azure Data Lake Storage Gen2 structure for telemetry data from 25 million devices distributed across seven key geographical regions. Each minute, the devices will send a JSON payload of metrics to Azure Event Hubs.\n\nYou need to recommend a folder structure for the data. The solution must meet the following requirements:\u2711 Data engineers from each region must be able to build their own pipelines for the data of their respective region only.\u2711 The data must be processed at least once every 15 minutes for inclusion in Azure Synapse Analytics serverless SQL pools.\n\nHow should you recommend completing the structure? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\nSelect and Place:\n/A/B/C.json",
    "question_type": "multiple",
    "choices": [
      "{deviceID}","{mm}/{HH}/{DD}/{MM}/{YYYY]","{regionID}/{deviceID]",
      "{regionID}/raw","{YYYY] /{MM}/{DD}/{HH}","{YYYY]/{MM}/{DD}/{HH}/{mm}",
      "raw/{deviceID}","raw/{regionID}"
    ],
    "site_answers": [
      "raw/{regionID}","{YYYY]/{MM}/{DD}/{HH}/{mm}","{deviceID}"
    ]
  },
  {
    "question_text": "You are creating a new notebook in Azure Databricks that will support R as the primary language but will also support Scala and SQL.\n\nWhich switch should you use to switch between languages?",
    "question_type": "single",
    "choices": [
      "%<language>",
      "@<Language >",
      "\\\\[<language >]",
      "\\\\(<language >)"
    ],
    "site_answers": [
      "%<language>"
    ]
  },
  {
    "question_text": "You have an Azure Data Factory pipeline that performs an incremental load of source data to an Azure Data Lake Storage Gen2 account.\n\nData to be loaded is identified by a column named LastUpdatedDate in the source table.\n\nYou plan to execute the pipeline every four hours.\n\nYou need to ensure that the pipeline execution meets the following requirements:\u2711 Automatically retries the execution when the pipeline run fails due to concurrency or throttling limits.\u2711 Supports backfilling existing data in the table.\n\nWhich type of trigger should you use?",
    "question_type": "single",
    "choices": [
      "event",
      "on-demand",
      "schedule",
      "tumbling window"
    ],
    "site_answers": [
      "tumbling window"
    ]
  },
  {
    "question_text": "You are designing a solution that will copy Parquet files stored in an Azure Blob storage account to an Azure Data Lake Storage Gen2 account.\n\nThe data will be loaded daily to the data lake and will use a folder structure of {Year}/{Month}/{Day}/.\n\nYou need to design a daily Azure Data Factory data load to minimize the data transfer between the two accounts.\n\nWhich two configurations should you include in the design? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point",
    "question_type": "multiple",
    "choices": [
      "Specify a file naming pattern for the destination.",
      "Delete the files in the destination before loading the data.",
      "Filter by the last modified date of the source files.",
      "Delete the source files after they are copied."
    ],
    "site_answers": [
      "Specify a file naming pattern for the destination.",
      "Filter by the last modified date of the source files."
    ]
  },
  {
    "question_text": "You plan to build a structured streaming solution in Azure Databricks. The solution will count new events in five-minute intervals and report only events that arrive during the interval. The output will be sent to a Delta Lake table.\n\nWhich output mode should you use?",
    "question_type": "single",
    "choices": [
      "update",
      "complete",
      "append"
    ],
    "site_answers": [
      "append"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1.\n\nYou have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1.\n\nYou plan to insert data from the files in container1 into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer ofTable1.\n\nYou need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.\n\nSolution: In an Azure Synapse Analytics pipeline, you use a data flow that contains a Derived Column transformation.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "Yes"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1.\n\nYou have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1.\n\nYou plan to insert data from the files in container1 into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer ofTable1.\n\nYou need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.\n\nSolution: You use a dedicated SQL pool to create an external table that has an additional DateTime column.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "No"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1.\n\nYou have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1.\n\nYou plan to insert data from the files in container1 into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer ofTable1.\n\nYou need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.\n\nSolution: You use an Azure Synapse Analytics serverless SQL pool to create an external table that has an additional DateTime column.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "No"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1.\n\nYou have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1.\n\nYou plan to insert data from the files in container1 into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer ofTable1.\n\nYou need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.\n\nSolution: In an Azure Synapse Analytics pipeline, you use a Get Metadata activity that retrieves the DateTime of the files.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "No"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure Data Lake Storage account that contains a staging zone.\n\nYou need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "Yes"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure Data Lake Storage account that contains a staging zone.\n\nYou need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes mapping data flow, and then inserts the data into the data warehouse.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "No"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou have an Azure Data Lake Storage account that contains a staging zone.\n\nYou need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.\n\nSolution: You schedule an Azure Databricks job that executes an R notebook, and then inserts the data into the data warehouse.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "Yes"
    ]
  },
  {
    "question_text": "You plan to create an Azure Data Factory pipeline that will include a mapping data flow.\n\nYou have JSON data containing objects that have nested arrays.\n\nYou need to transform the JSON-formatted data into a tabular dataset. The dataset must have one row for each item in the arrays.\n\nWhich transformation method should you use in the mapping data flow?",
    "question_type": "single",
    "choices": [
      "new branch",
      "unpivot",
      "alter row",
      "flatten"
    ],
    "site_answers": [
      "flatten"
    ]
  },
  {
    "question_text": "You use Azure Stream Analytics to receive Twitter data from Azure Event Hubs and to output the data to an Azure Blob storage account.\n\nYou need to output the count of tweets during the last five minutes every five minutes. Each tweet must only be counted once.\n\nWhich windowing function should you use?",
    "question_type": "single",
    "choices": [
      "a five-minute Sliding window",
      "a five-minute Session window",
      "a five-minute Hopping window that has a one-minute hop",
      "a five-minute Tumbling window"
    ],
    "site_answers": [
      "a five-minute Tumbling window"
    ]
  },
  {
    "question_text": "You are planning a streaming data solution that will use Azure Databricks. The solution will stream sales transaction data from an online store. The solution has the following specifications:The output data will contain items purchased, quantity, line total sales amount, and line total tax amount.\u2711 Line total sales amount and line total tax amount will be aggregated in Databricks.\u2711 Sales transactions will never be updated. Instead, new rows will be added to adjust a sale.\n\nYou need to recommend an output mode for the dataset that will be processed by using Structured Streaming. The solution must minimize duplicate data.\n\nWhat should you recommend?",
    "question_type": "single",
    "choices": [
      "Update",
      "Complete",
      "Append"
    ],
    "site_answers": [
      "Append"
    ]
  },
  {
    "question_text": "You have an enterprise data warehouse in Azure Synapse Analytics named DW1 on a server named Server1.\n\nYou need to determine the size of the transaction log file for each distribution of DW1.\n\nWhat should you do?",
    "question_type": "single",
    "choices": [
      "On DW1, execute a query against the sys.database_files dynamic management view.",
      "From Azure Monitor in the Azure portal, execute a query against the logs of DW1.",
      "Execute a query against the logs of DW1 by using the Get-AzOperationalInsightsSearchResult PowerShell cmdlet.",
      "On the master database, execute a query against the sys.dm_pdw_nodes_os_performance_counters dynamic management view."
    ],
    "site_answers": [
      "On the master database, execute a query against the sys.dm_pdw_nodes_os_performance_counters dynamic management view."
    ],
    "link": "https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor"
  },
  {
    "question_text": "You are designing an anomaly detection solution for streaming data from an Azure IoT hub. The solution must meet the following requirements:\u2711 Send the output to Azure Synapse.\u2711 Identify spikes and dips in time series data.\u2711 Minimize development and configuration effort.\n\nWhich should you include in the solution?",
    "question_type": "single",
    "choices": [
      "Azure Databricks",
      "Azure Stream Analytics",
      "Azure SQL Database"
    ],
    "site_answers": [
      "Azure Stream Analytics"
    ]
  },
  {
    "question_text": "A company uses Azure Stream Analytics to monitor devices.\n\nThe company plans to double the number of devices that are monitored.\n\nYou need to monitor a Stream Analytics job to ensure that there are enough processing resources to handle the additional load.\n\nWhich metric should you monitor?",
    "question_type": "single",
    "choices": [
      "Early Input Events",
      "Late Input Events",
      "Watermark delay",
      "Input Deserialization Errors"
    ],
    "site_answers": [
      "Watermark delay"
    ]
  },
  {
    "question_text": "HOTSPOT -You are designing an enterprise data warehouse in Azure Synapse Analytics that will store website traffic analytics in a star schema.\n\nYou plan to have a fact table for website visits. The table will be approximately 5 GB.\n\nYou need to recommend which distribution type and index type to use for the table. The solution must provide the fastest query performance.\n\nWhat should you recommend? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nDistribution: X\nIndex: Y",
    "question_type": "multiple",
    "choices": [
      "X: Hash","X: Round robin","X: Replicated",
      "Y: Clustered columnstore","Y: Clustered","Y: Nonclustered"
    ],
    "site_answers": ["X: Hash","Y: Clustered columnstore"]
  },
  {
    "question_text": "You have an Azure Stream Analytics job.\n\nYou need to ensure that the job has enough streaming units provisioned.\n\nYou configure monitoring of the SU % Utilization metric.\n\nWhich two additional metrics should you monitor? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Backlogged Input Events",
      "Watermark Delay",
      "Function Events",
      "Out of order Events",
      "Late Input Events"
    ],
    "site_answers": [
      "Backlogged Input Events",
      "Watermark Delay"
    ]
  },
  {
    "question_text": "You have an activity in an Azure Data Factory pipeline. The activity calls a stored procedure in a data warehouse in Azure Synapse Analytics and runs daily.\n\nYou need to verify the duration of the activity when it ran last.\n\nWhat should you use?",
    "question_type": "single",
    "choices": [
      "activity runs in Azure Monitor",
      "Activity log in Azure Synapse Analytics",
      "the sys.dm_pdw_wait_stats data management view in Azure Synapse Analytics",
      "an Azure Resource Manager template"
    ],
    "site_answers": [
      "activity runs in Azure Monitor"
    ]
  },
  {
    "question_text": "You have an Azure Data Factory pipeline that is triggered hourly.\n\nThe pipeline has had 100% success for the past seven days.\n\nThe pipeline execution fails, and two retries that occur 15 minutes apart also fail. The third failure returns the following error.\n\nErrorCode=UserErrorFileNotFound,'Type=Microsoft.\n\nDataTransfer.\n\nCommon.\n\nShared.\n\nHybridDeliveryException,Message=ADLS Gen2 operation failed for: Operation returned an invalid status code 'NotFound'. Account: 'contosoproduksouth'. Filesystem: wwi. Path: 'BIKES/CARBON/year=2021/month=01/day=10/hour=06'. ErrorCode: 'PathNotFound'. Message: 'The specified path does not exist.'. RequestId: '6d269b78-901f-001b-4924-e7a7bc000000'. TimeStamp: 'Sun, 10 Jan 2021 07:45:05What is a possible cause of the error?",
    "question_type": "single",
    "choices": [
      "The parameter used to generate year=2021/month=01/day=10/hour=06 was incorrect.",
      "From 06:00 to 07:00 on January 10, 2021, there was no data in wwi/BIKES/CARBON.",
      "From 06:00 to 07:00 on January 10, 2021, the file format of data in wwi/BIKES/CARBON was incorrect.",
      "The pipeline was triggered too early."
    ],
    "site_answers": [
      "From 06:00 to 07:00 on January 10, 2021, there was no data in wwi/BIKES/CARBON."
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics job that uses Scala.\n\nYou need to view the status of the job.\n\nWhat should you do?",
    "question_type": "single",
    "choices": [
      "From Synapse Studio, select the workspace. From Monitor, select SQL requests.",
      "From Azure Monitor, run a Kusto query against the AzureDiagnostics table.",
      "From Synapse Studio, select the workspace. From Monitor, select Apache Sparks applications.",
      "From Azure Monitor, run a Kusto query against the SparkLoggingEvent_CL table."
    ],
    "site_answers": [
      "From Synapse Studio, select the workspace. From Monitor, select Apache Sparks applications."
    ]
  },
  {
    "question_text": "You have an Azure data factory named ADF1.\n\nYou currently publish all pipeline authoring changes directly to ADF1.\n\nYou need to implement version control for the changes made to pipeline artifacts. The solution must ensure that you can apply version control to the resources currently defined in the UX Authoring canvas for ADF1.\n\nWhich two actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "From the UX Authoring canvas, select Set up code repository.",
      "Create a Git repository.",
      "Create a GitHub action.",
      "Create an Azure Data Factory trigger.",
      "From the UX Authoring canvas, select Publish.",
      "From the UX Authoring canvas, run Publish All."
    ],
    "site_answers": [
      "From the UX Authoring canvas, select Set up code repository.",
      "Create a Git repository."
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure subscription that contains an Azure Synapse Analytics dedicated SQL pool named Pool1 and an Azure Data Lake Storage account named storage1. Storage1 requires secure transfers.\n\nYou need to create an external data source in Pool1 that will be used to read .orc files in storage1.\n\nHow should you complete the code? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nCREATE EXTERNAL DATA SOURCE AzureDataLakeStore\nWITH\n( Location1 '$XXX$://data@newyorktaxidataset.dfs.core.windows.net' ,\ncredential = ADLS_credential ,\nTYPE - $YYY$ );",
    "question_type": "multiple",
    "choices": [
      "X: abfs","X: abfss","X: wasb","X: wasbs",
      "Y: BLOB_STORAGE","Y: HADOOP","Y: RDBMS","Y: SHARP MAP MANAGER"
    ],
    "site_answers": [
      "X: abfs","Y: HADOOP"
    ]
  },
  {
    "question_text": "You have an Azure subscription that contains an Azure Synapse Analytics dedicated SQL pool named SQLPool1.\n\nSQLPool1 is currently paused.\n\nYou need to restore the current state of SQLPool1 to a new SQL pool.\n\nWhat should you do first?",
    "question_type": "single",
    "choices": [
      "Create a workspace.",
      "Create a user-defined restore point.",
      "Resume SQLPool1.",
      "Create a new SQL pool."
    ],
    "site_answers": [
      "Resume SQLPool1."
    ]
  },
  {
    "question_text": "You are designing an Azure Synapse Analytics workspace.\n\nYou need to recommend a solution to provide double encryption of all the data at rest.\n\nWhich two components should you include in the recommendation? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "an X.509 certificate",
      "an RSA key",
      "an Azure virtual network that has a network security group (NSG)",
      "an Azure Policy initiative",
      "an Azure key vault that has purge protection enabled"
    ],
    "site_answers": [
      "an RSA key",
      "an Azure key vault that has purge protection enabled"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics serverless SQL pool named Pool1 and an Azure Data Lake Storage Gen2 account named storage1. TheAllowBlobPublicAccess property is disabled for storage1.\n\nYou need to create an external data source that can be used by Azure Active Directory (Azure AD) users to access storage from Pool1.\n\nWhat should you create first?",
    "question_type": "single",
    "choices": [
      "an external resource pool",
      "an external library",
      "database scoped credentials",
      "a remote service binding"
    ],
    "site_answers": [
      "database scoped credentials"
    ]
  },
  {
    "question_text": "You have an Azure Data Factory pipeline named Pipeline1. Pipeline1 contains a copy activity that sends data to an Azure Data Lake Storage Gen2 account.\n\nPipeline1 is executed by a schedule trigger.\n\nYou change the copy activity sink to a new storage account and merge the changes into the collaboration branch.\n\nAfter Pipeline1 executes, you discover that data is NOT copied to the new storage account.\n\nYou need to ensure that the data is copied to the new storage account.\n\nWhat should you do?",
    "question_type": "single",
    "choices": [
      "Publish from the collaboration branch.",
      "Create a pull request.",
      "Modify the schedule trigger.",
      "Configure the change feed of the new storage account."
    ],
    "site_answers": [
      "Publish from the collaboration branch."
    ]
  },
  {
    "question_text": "You have an Azure Data Factory pipeline named pipeline1 that is invoked by a tumbling window trigger named Trigger1. Trigger1 has a recurrence of 60 minutes.\n\nYou need to ensure that pipeline1 will execute only if the previous execution completes successfully.\n\nHow should you configure the self-dependency for Trigger1?",
    "question_type": "single",
    "choices": [
      "offset: \"-00:01:00\" size: \"00:01:00\"",
      "offset: \"01:00:00\" size: \"-01:00:00\"",
      "offset: \"01:00:00\" size: \"01:00:00\"",
      "offset: \"-01:00:00\" size: \"01:00:00\""
    ],
    "site_answers": [
      "offset: \"-01:00:00\" size: \"01:00:00\""
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse Analytics pipeline named Pipeline1 that contains a data flow activity named Dataflow1.\n\nPipeline1 retrieves files from an Azure Data Lake Storage Gen 2 account named storage1.\n\nDataflow1 uses the AutoResolveIntegrationRuntime integration runtime configured with a core count of 128.\n\nYou need to optimize the number of cores used by Dataflow1 to accommodate the size of the files in storage1.\n\nWhat should you configure? To answer, select the appropriate options in the answer area.\n\nHot Area:\nAdd to Pipeline1:X\nFor Dataflow1, set the core count by using: Y",
    "question_type": "multiple",
    "choices": [
      "X: A custom activity","X: A Get Metadata activity","X: An If Condition activity",
      "Y: Dynamic content","Y: Parameters","Y: User properties"
    ],
    "site_answers": [
      "X: A Get Metadata activity","Y: Dynamic content"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:\u2711 A workload for data engineers who will use Python and SQL.\u2711 A workload for jobs that will run notebooks that use Python, Scala, and SQL.\u2711 A workload that data scientists will use to perform ad hoc analysis in Scala and R.\n\nThe enterprise architecture team at your company identifies the following standards for Databricks environments:\u2711 The data engineers must share a cluster.\u2711 The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.\u2711 All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.\n\nYou need to create the Databricks clusters for the workloads.\n\nSolution: You create a Standard cluster for each data scientist, a High Concurrency cluster for the data engineers, and a Standard cluster for the jobs.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "Yes"
    ]
  },
  {
    "question_text": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:\u2711 A workload for data engineers who will use Python and SQL.\u2711 A workload for jobs that will run notebooks that use Python, Scala, and SQL.\u2711 A workload that data scientists will use to perform ad hoc analysis in Scala and R.\n\nThe enterprise architecture team at your company identifies the following standards for Databricks environments:\u2711 The data engineers must share a cluster.\u2711 The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.\u2711 All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.\n\nYou need to create the Databricks clusters for the workloads.\n\nSolution: You create a Standard cluster for each data scientist, a High Concurrency cluster for the data engineers, and a High Concurrency cluster for the jobs.\n\nDoes this meet the goal?",
    "question_type": "single",
    "choices": [
      "Yes",
      "No"
    ],
    "site_answers": [
      "No"
    ]
  },
  {
    "question_text": "You are designing a folder structure for the files in an Azure Data Lake Storage Gen2 account. The account has one container that contains three years of data.\n\nYou need to recommend a folder structure that meets the following requirements:\u2711 Supports partition elimination for queries by Azure Synapse Analytics serverless SQL pools\u2711 Supports fast data retrieval for data from the current month\u2711 Simplifies data security management by departmentWhich folder structure should you recommend?",
    "question_type": "single",
    "choices": [
      "\\Department\\DataSource\\YYYY\\MM\\DataFile_YYYYMMDD.parquet",
      "\\DataSource\\Department\\YYYYMM\\DataFile_YYYYMMDD.parquet",
      "\\DD\\MM\\YYYY\\Department\\DataSource\\DataFile_DDMMYY.parquet",
      "\\YYYY\\MM\\DD\\Department\\DataSource\\DataFile_YYYYMMDD.parquet"
    ],
    "site_answers": [
      "\\Department\\DataSource\\YYYY\\MM\\DataFile_YYYYMMDD.parquet"
    ]
  },
  {
    "question_text": "You have an Azure subscription that contains an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 receives new data once every 24 hours.\n\nYou have the following function.\n\ncreate function dbo.udfFtoC(F decimal)\nreturn decimal\nas\nbegin\nreturn (F - 32) * 5.0 / 9\nend\n\nYou have the following query.\n\nselect avg_date, sensorid, avg_f, dbo.udfFtoC(avg_temperature) as avg_c from SensorTemps\nwhere avg_date = @parameter\n\nThe query is executed once every 15 minutes and the @parameter value is set to the current date.\n\nYou need to minimize the time it takes for the query to return results.\n\nWhich two actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Create an index on the avg_f column.",
      "Convert the avg_c column into a calculated column.",
      "Create an index on the sensorid column.",
      "Enable result set caching.",
      "Change the table distribution to replicate."
    ],
    "site_answers": [
      "Convert the avg_c column into a calculated column.",
      "Enable result set caching."
    ]
  },
  {
    "question_text": "You need to design a solution that will process streaming data from an Azure Event Hub and output the data to Azure Data Lake Storage. The solution must ensure that analysts can interactively query the streaming data.\n\nWhat should you use?",
    "question_type": "single",
    "choices": [
      "Azure Stream Analytics and Azure Synapse notebooks",
      "Structured Streaming in Azure Databricks",
      "event triggers in Azure Data Factory",
      "Azure Queue storage and read-access geo-redundant storage (RA-GRS)"
    ],
    "site_answers": [
      "Structured Streaming in Azure Databricks"
    ]
  },
  {
    "question_text": "You are creating an Apache Spark job in Azure Databricks that will ingest JSON-formatted data.\n\nYou need to convert a nested JSON string into a DataFrame that will contain multiple rows.\n\nWhich Spark SQL function should you use?",
    "question_type": "single",
    "choices": [
      "explode",
      "filter",
      "coalesce",
      "extract"
    ],
    "site_answers": [
      "explode"
    ]
  },
  {
    "question_text": "DRAG DROP -You have an Azure subscription that contains an Azure Databricks workspace. The workspace contains a notebook named Notebook1.\n\nIn Notebook1, you create an Apache Spark DataFrame named df_sales that contains the following columns:\u2022\tCustomer\u2022\tSalesPerson\u2022\tRegion\u2022\tAmountYou need to identify the three top performing salespersons by amount for a region named HQ.\n\nHow should you complete the query? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\ndf_sales.filter(col('Region') == 'HQ').$XXX.agg(sum('Amount').alias('TotalAmount')).$YYY.limit(3)",
    "question_type": "multiple",
    "choices": [
      "agg(col('SalesPerson'))","filter(col('SalesPerson'))","groupBy(col('SalesPerson'))",
      "groupBy(col('TotalAmount'))","orderBy(col('TotalAmount'))","orderBy(desc('TotalAmount'))"
    ],
    "site_answers": [
      "groupBy(col('SalesPerson'))","orderBy(desc('TotalAmount'))"
    ]
  },
  {
    "question_text": "You need to schedule an Azure Data Factory pipeline to execute when a new file arrives in an Azure Data Lake Storage Gen2 container.\n\nWhich type of trigger should you use?",
    "question_type": "single",
    "choices": [
      "on-demand",
      "tumbling window",
      "schedule",
      "storage event"
    ],
    "site_answers": [
      "storage event"
    ]
  },
  {
    "question_text": "You have an Azure subscription that contains an Azure SQL database named DB1 and a storage account named storage1. The storage1 account contains a file named File1.txt. File1.txt contains the names of selected tables in DB1.\n\nYou need to use an Azure Synapse pipeline to copy data from the selected tables in DB1 to the files in storage1. The solution must meet the following requirements:\u2022\tThe Copy activity in the pipeline must be parameterized to use the data in File1.txt to identify the source and destination of the copy.\u2022\tCopy activities must occur in parallel as often as possible.\n\nWhich two pipeline activities should you include in the pipeline? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Get Metadata",
      "Lookup",
      "ForEach",
      "If Condition"
    ],
    "site_answers": [
      "Lookup",
      "ForEach"
    ]
  },
  {
    "question_text": "You have an Azure data factory that connects to a Microsoft Purview account. The data factory is registered in Microsoft Purview.\n\nYou update a Data Factory pipeline.\n\nYou need to ensure that the updated lineage is available in Microsoft Purview.\n\nWhat should you do first?",
    "question_type": "single",
    "choices": [
      "Disconnect the Microsoft Purview account from the data factory.",
      "Execute the pipeline.",
      "Execute an Azure DevOps build pipeline.",
      "Locate the related asset in the Microsoft Purview portal."
    ],
    "site_answers": [
      "Execute the pipeline."
    ]
  },
  {
    "question_text": "You have a Microsoft Purview account.\n\nThe Lineage view of a CSV file is shown in the following exhibit.\nData catalog: Search results: destfile.csv (blob)\npercentage-of-americans-living-alone-by-age.csv (blob) -> COPY_XferFolder (azure data factory copy activity) -> destfile.csv\nHow is the data for the lineage populated?",
    "question_type": "single",
    "choices": [
      "manually",
      "by scanning data stores",
      "by executing a Data Factory pipeline"
    ],
    "site_answers": [
      "by executing a Data Factory pipeline"
    ]
  },
  {
    "question_text": "You have an Azure subscription that contains a Microsoft Purview account named MP1, an Azure data factory named DF1, and a storage account named storage1. MP1 is configured to scan storage1. DF1 is connected to MP1 and contains a dataset named DS1. DS1 references a file in storage1.\n\nIn DF1, you plan to create a pipeline that will process data from DS1.\n\nYou need to review the schema and lineage information in MP1 for the data referenced by DS1.\n\nWhich two features can you use to locate the information? Each correct answer presents a complete solution.\n\nNOTE: Each correct answer is worth one point.",
    "question_type": "multiple",
    "choices": [
      "the search bar in the Microsoft Purview governance portal",
      "the Storage browser of storage1 in the Azure portal",
      "the search bar in the Azure portal",
      "the search bar in Azure Data Factory Studio"
    ],
    "site_answers": [
      "the search bar in the Microsoft Purview governance portal",
      "the search bar in Azure Data Factory Studio"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Blob storage account that contains a folder. The folder contains 120,000 files. Each file contains 62 columns.\n\nEach day, 1,500 new files are added to the folder.\n\nYou plan to incrementally load five data columns from each new file into an Azure Synapse Analytics workspace.\n\nYou need to minimize how long it takes to perform the incremental loads.\n\nWhat should you use to store the files and in which format? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nStorage: X\nFormat: Y",
    "question_type": "multiple",
    "choices": [
      "X: Multiple blob storage accounts","X: Multiple containers in the blob storage account","X: Timeslice partitioning in the folders",
      "Y: Apache Parquet","Y: CSV","Y: JSON"
    ],
    "site_answers": [
      "X: Timeslice partitioning in the folders","Y: Apache Parquet"
    ]
  },
  {
    "question_text": "DRAG DROP -You are batch loading a table in an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to load data from a staging table to the target table. The solution must ensure that if an error occurs while loading the data to the target table, all the inserts in that batch are undone.\n\nHow should you complete the Transact-SQL code? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\n$XXX\nBEGIN TRY\n  INSERT INTO dbo. Table1 (col1, col2, col3)\n  SELECT col1, col2, col3 FROM stage.Table1;\nEND TRY\n  BEGIN CATCH\n    IF @@TRANCOUNT > 0\n    BEGIN\n      $XXX\n    END\n  END CATCH;\nIF @@TRANCOUNT >0\nBEGIN\n  COMMIT TRAN;\nEND",
    "question_type": "multiple",
    "choices": [
      "BEGIN DISTRIBUTED TRANSACTION","BEGIN TRAN","COMMIT TRAN",
      "ROLLBACK TRAN","SET RESULT_SET_CACHING ON"
    ],
    "site_answers": [
      "BEGIN TRAN","ROLLBACK TRAN"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-transactions"
  },
  {
    "question_text": "HOTSPOT -You have two Azure SQL databases named DB1 and DB2.\n\nDB1 contains a table named Table1. Table1 contains a timestamp column named LastModifiedOn. LastModifiedOn contains the timestamp of the most recent update for each individual row.\n\nDB2 contains a table named Watermark. Watermark contains a single timestamp column named WatermarkValue.\n\nYou plan to create an Azure Data Factory pipeline that will incrementally upload into Azure Blob Storage all the rows in Table1 for which the LastModifiedOn column contains a timestamp newer than the most recent value of the WatermarkValue column in Watermark.\n\nYou need to identify which activities to include in the pipeline. The solution must meet the following requirements:\u2022\tMinimize the effort to author the pipeline.\u2022\tEnsure that the number of data integration units allocated to the upload operation can be controlled.\n\nWhat should you identify? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct answer is worth one point.\n\nTo retrieve the watermark value, use: X\nTo perform the upload, use: Y",
    "question_type": "multiple",
    "choices": [
      "X: Filter","X: Get Metadata","X: Lookup",
      "Y: Copy data","Y: Custom","Y: Data flow"
    ],
    "site_answers": [
      "X: Lookup","Y: Copy data"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-portal"
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse serverless SQL pool.\n\nYou need to read JSON documents from a file by using the OPENROWSET function.\n\nHow should you complete the query? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nSELECT *\nFROM OPENROWSET (\n  BULK 'https://sourcedatalake.blob.core.windows.net/public/docs.json',\n  FORMAT = XXX\n  ROWTERMINATOR = '0x0b'\n  FIELDTERMINATOR = '0x0b',\n  FIELDQUOTE = YYY \n) WITH (jsondoc nvarchar(max) AS JsonDocuments",
    "question_type": "multiple",
    "choices": [
      "X: 'CSV'","X: 'DELTA'","X: 'JSON*","X: 'PARQUET'",
      "Y: '0x09'","Y: '0x0a'","Y: '0x0b'","Y: 'Ox0c'"
    ],
    "site_answers": [
      "X: 'CSV'",,"Y: '0x0b'"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files"
  },
  {
    "question_text": "You use Azure Data Factory to create data pipelines.\n\nYou are evaluating whether to integrate Data Factory and GitHub for source and version control.\n\nWhat are two advantages of the integration? Each correct answer presents a complete solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "additional triggers",
      "lower pipeline execution times",
      "the ability to save without publishing",
      "the ability to save pipelines that have validation issues"
    ],
    "site_answers": [
      "the ability to save without publishing",
      "the ability to save pipelines that have validation issues"
    ]
  },
  {
    "question_text": "DRAG DROP -You have an Azure Synapse Analytics workspace named Workspace1.\n\nYou perform the following changes:\u2022\tImplement source control for Workspace1.\u2022\tCreate a branch named Feature based on the collaboration branch.\u2022\tSwitch to the Feature branch.\u2022\tModify Workspace1.\n\nYou need to publish the changes to Azure Synapse.\n\nFrom which branch should you perform each change? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point\nCreate a pull request:\nPublish the changes:",
    "question_type": "multiple",
    "choices": [
      "Collaboration", "Publish", "Feature"
    ],
    "site_answers": [
      "Feature","Collaboration"
    ]
  },
  {
    "question_text": "You have two Azure Blob Storage accounts named account1 and account2.\n\nYou plan to create an Azure Data Factory pipeline that will use scheduled intervals to replicate newly created or modified blobs from account1 to account2.\n\nYou need to recommend a solution to implement the pipeline. The solution must meet the following requirements:\u2022\tEnsure that the pipeline only copies blobs that were created or modified since the most recent replication event.\u2022\tMinimize the effort to create the pipeline.\n\nWhat should you recommend?",
    "question_type": "single",
    "choices": [
      "Run the Copy Data tool and select Metadata-driven copy task.",
      "Create a pipeline that contains a Data Flow activity.",
      "Create a pipeline that contains a flowlet.",
      "Run the Copy Data tool and select Built-in copy task."
    ],
    "site_answers": [
      "Run the Copy Data tool and select Built-in copy task."
    ],
    "links": "https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-lastmodified-copy-data-tool"
  },
  {
    "question_text": "You have an Azure Data Factory pipeline named pipeline1 that contains a data flow activity named activity1.\n\nYou need to run pipeline1.\n\nWhich runtime will be used to run activity1?",
    "question_type": "single",
    "choices": [
      "Azure Integration runtime",
      "Self-hosted integration runtime",
      "SSIS integration runtime"
    ],
    "site_answers": [
      "Azure Integration runtime"
    ],
    "links" "https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime"
  },
  {
    "question_text": "HOTSPOT -You have an Azure subscription that contains an Azure Synapse Analytics workspace named workspace1. Workspace1 contains a dedicated SQL pool named SQLPool1 and an Apache Spark pool named sparkpool1. Sparkpool1 contains a DataFrame named pyspark_df.\n\nYou need to write the contents of pyspark_df to a table in SQLPool1 by using a PySpark notebook.\n\nHow should you complete the code? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "X: %%local","X: %%spark","X: %%sql",
      "Y: jdbc","Y: saveAsTable","Y: synapsesql"
    ],
    "site_answers": [
      "X: %%spark","Y: synapsesql"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-spark-sql-pool-import-export?tabs=python%2Cpython1%2Cpython2%2Cpython3%2Cpython4%2Cpython5"
  },
  {
    "question_text": "You have an Azure data factory named ADF1 and an Azure Synapse Analytics workspace that contains a pipeline named SynPipeLine1. SynPipeLine1 includes a Notebook activity.\n\nYou create a pipeline in ADF1 named ADFPipeline1.\n\nYou need to invoke SynPipeLine1 from ADFPipeline1.\n\nWhich type of activity should you use?",
    "question_type": "single",
    "choices": [
      "Web",
      "Spark",
      "Custom",
      "Notebook"
    ],
    "site_answers": [
      "Web"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure data factory that contains the linked service shown in the following exhibit.\n\nConnect via integration runtime: AutoResolveIntergrationRuntime\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\n\nNOTE: Each correct answer is worth one point.\n\nWhen working in a feature branch, changes to the linked XXX service will be published to the live service\nA Copy activity that uses the linked service as the source YYY will perform the Copy activity",
    "question_type": "multiple",
    "choices": [
      "X: upon publishing the changes","X: upon saving the changes","X: when the changes are merged into the collaboration branch",
      "Y: in the region of the data factory","Y: in the region of the selected external compute","Y: in the region of the source database"
    ],
    "site_answers": ["X: when the changes are merged into the collaboration branch","Y: in the region of the data factory"]
  },
  {
    "question_text": "HOTSPOT -In Azure Data Factory, you have a schedule trigger that is scheduled in Pacific Time.\n\nPacific Time observes daylight saving time.\n\nThe trigger has the following JSON file.\n\n\"name\": \"Trigger 1\",\n\"properties\": {\n  \"annotations\": [],\n  \"runtimeState\": \"Started\",\n  \"pipelines\": [],\n  \"type\": \"ScheduleTrigger\",\n  \"typeProperties\": {\n  \"recurrence\": {\n    \"frequency\": \"Week\",\n    \"interval\": 1,\n    \"startTime\": \"2022-08-05T04:00:00\",\n    \"timeZone\": \"Pacific Standard Time\",\n    \"schedule\": {\n      \"minutes\": [\n        0\n      ]\n      \"hours\": [\n        3,\n        21\n      ]\n      \"weekDays\": [\n        \"Sunday\",\n        \"Saturday\"\n      ]\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented.\n\nNOTE: Each correct selection is worth one point.\n\nThe trigger will execute XXX on Sunday, March 3, 2024.\nThe trigger YYY daylight saving time.",
    "question_type": "multiple",
    "choices": [
      "X: one time","X: two times","X: zero times",
      "Y: is unaffected by","Y: will automatically adjust for","Y: will require an adjustment for"
    ],
    "site_answers": [
      "X: two times","Y: will automatically adjust for"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger?tabs=data-factory#azure-data-factory-and-synapse-portal-experience"
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to create a pipeline that will execute a stored procedure in the dedicated SQL pool and use the returned result set as the input for a downstream activity. The solution must minimize development effort.\n\nWhich type of activity should you use in the pipeline?",
    "question_type": "single",
    "choices": [
      "U-SQL",
      "Stored Procedure",
      "Script",
      "Notebook"
    ],
    "site_answers": [
      "Script"
    ]
  },
  {
    "question_text": "You have an Azure SQL database named DB1 and an Azure Data Factory data pipeline named pipeline1.\n\nFrom Data Factory, you configure a linked service to DB1.\n\nIn DB1, you create a stored procedure named SP1. SP1 returns a single row of data that has four columns.\n\nYou need to add an activity to pipeline1 to execute SP1. The solution must ensure that the values in the columns are stored as pipeline variables.\n\nWhich two types of activities can you use to execute SP1? Each correct answer presents a complete solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Script",
      "Copy",
      "Lookup",
      "Stored Procedure"
    ],
    "site_answers": [
      "Script",
      "Lookup"
    ]
  },
  {
    "question_text": "You have an Azure data factory named ADF1.\n\nYou currently publish all pipeline authoring changes directly to ADF1.\n\nYou need to implement version control for the changes made to pipeline artifacts. The solution must ensure that you can apply version control to the resources currently defined in the Azure Data Factory Studio for ADF1.\n\nWhich two actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "From the Azure Data Factory Studio, run Publish All.",
      "Create an Azure Data Factory trigger.",
      "Create a Git repository.",
      "Create a GitHub action.",
      "From the Azure Data Factory Studio, select Set up code repository.",
      "From the Azure Data Factory Studio, select Publish."
    ],
    "site_answers": [
      "Create a Git repository.",
      "From the Azure Data Factory Studio, select Set up code repository."
    ]
  },
  {
    "question_text": "You have an Azure data factory named ADF1 that contains a pipeline named Pipeline1.\n\nPipeline1 must execute every 30 minutes with a 15-minute offset.\n\nYou need to create a trigger for Pipeline1. The trigger must meet the following requirements:\u2022\tBackfill data from the beginning of the day to the current time.\u2022\tIf Pipeline1 fails, ensure that the pipeline can re-execute within the same 30-minute period.\u2022\tEnsure that only one concurrent pipeline execution can occur.\u2022\tMinimize development and configuration effort.\n\nWhich type of trigger should you create?",
    "question_type": "single",
    "choices": [
      "schedule",
      "event-based",
      "manual",
      "tumbling window"
    ],
    "site_answers": [
      "tumbling window"
    ]
  },
  {
    "question_text": "You have an Azure Data Lake Storage Gen2 account named account1 and an Azure event hub named Hub1. Data is written to account1 by using Event Hubs Capture.\n\nYou plan to query account by using an Apache Spark pool in Azure Synapse Analytics.\n\nYou need to create a notebook and ingest the data from account1. The solution must meet the following requirements:\u2022\tRetrieve multiple rows of records in their entirety.\u2022\tMinimize query execution time.\u2022\tMinimize data processing.\n\nWhich data format should you use?",
    "question_type": "single",
    "choices": [
      "Parquet",
      "Avro",
      "ORC",
      "JSON"
    ],
    "site_answers": [
      "Avro"
    ]
  },
  {
    "question_text": "You have an Azure Blob Storage account named blob1 and an Azure Data Factory pipeline named pipeline1.\n\nYou need to ensure that pipeline1 runs when a file is deleted from a container in blob1. The solution must minimize development effort.\n\nWhich type of trigger should you use?",
    "question_type": "single",
    "choices": [
      "schedule",
      "storage event",
      "tumbling window",
      "custom event"
    ],
    "site_answers": [
      "storage event"
    ]
  },
  {
    "question_text": "HOTSPOT -You have Azure Data Factory configured with Azure Repos Git integration. The collaboration branch and the publish branch are set to the default values.\n\nYou have a pipeline named pipeline1.\n\nYou build a new version of pipeline1 in a branch named feature1.\n\nFrom the Data Factory Studio, you select Publish.\n\nThe source code of which branch will be built, and which branch will contain the output of the Azure Resource Manager (ARM) template? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nSource code: XXX\nARM template output: YYY",
    "question_type": "multiple",
    "choices": [
      "X: adf_publish","X: feature1","X: main",
      "Y: adf_publish","Y: feature1","Y: main",
    ],
    "site_answers": [
      "X: main","Y: adf_publish"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-delivery#cicd-lifecycle"
  },
  {
    "question_text": "DRAG DROP -You have an Azure subscription that contains an Azure data factory.\n\nYou are editing an Azure Data Factory activity JSON.\n\nThe script needs to copy a file from Azure Blob Storage to multiple destinations. The solution must ensure that the source and destination files have consistent folder paths.\n\nHow should you complete the script? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\nactivities: type: XXX\nsink: CopyBeaviour: YYY",
    "question_type": "multiple",
    "choices": [
      "ForEach","PreserveHierarchy","Switch","Until","FlattenHierarchy","MergeFiles"
    ],
    "site_answers": [
      "ForEach","PreserveHierarchy"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"
  },
  {
    "question_text": "You are building a data flow in Azure Data Factory that upserts data into a table in an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to add a transformation to the data flow. The transformation must specify logic indicating when a row from the input data must be upserted into the sink.\n\nWhich type of transformation should you add to the data flow?",
    "question_type": "single",
    "choices": [
      "join",
      "alter row",
      "surrogate key",
      "select"
    ],
    "site_answers": [
      "alter row"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/data-factory/data-flow-alter-row"
  },
  {
    "question_text": "You have an on-premises database named db1 and a set-hosted integration runtime.\n\nYou have an Azure subscription that contains an Azure Data Lake Storage account named dl1.\n\nYou need to develop four data pipeline projects that will use Microsoft Power Query to copy data from db1 to dl1. The solution must meet the following requirements:\u2022\tAll pipelines must use the self-hosted integration runtime.\u2022\tEach project must be stored in a separate Git repository.\u2022\tDevelopment effort must be minimized.\n\nWhat should you use?",
    "question_type": "single",
    "choices": [
      "Azure Synapse Analytics",
      "Azure Logic Apps.",
      "Azure Data Factory",
      "Microsoft Power BI"
    ],
    "site_answers": [
      "Azure Data Factory"
    ]
  },
  {
    "question_text": "You have the Azure Synapse Analytics pipeline shown in the following exhibit.\n\nLookup: Business Activity That Fails -success Lookup-> Set variable Upon Success -failure Lookup-> Set variable Upon failure\nYou need to add a set variable activity to the pipeline to ensure that after the pipeline\u2019s completion, the status of the pipeline is always successful.\n\nWhat should you configure for the set variable activity?",
    "question_type": "single",
    "choices": [
      "a skipped dependency on the Upon Failure activity",
      "a skipped dependency on the Upon Success activity",
      "a success dependency on the Business Activity That Fails activity",
      "a failure dependency on the Upon Failure activity"
    ],
    "site_answers": [
      "a skipped dependency on the Upon Success activity"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#how-pipeline-failure-are-determined"
  },
  {
    "question_text": "You have an on-premises Linux server that contains a database named DB1.\n\nYou have an Azure subscription that contains an Azure data factory named ADF1 and an Azure Data Lake Storage account named ADLS1.\n\nYou need to create a pipeline in ADF1 that will copy data from DB1 to ADLS1.\n\nWhich type of integration runtime should you use to read the data from DB1?",
    "question_type": "single",
    "choices": [
      "self-hosted integration runtime",
      "Azure integration runtime",
      "Azure-SQL Server Integration Services (SSIS)"
    ],
    "site_answers": [
      "self-hosted integration runtime"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse Analytics workspace that contains an Apache Spark pool named Pool1.\n\nYou need to read data from a CSV file and write the data to a Delta table by using Pool1.\n\nHow should you complete the PySpark code? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nfrom delta. tables import *\nfrom pyspark. sql. functions import *\n\n\ndf = spark. read. load\n('abfss://container@mydatalake.dfs.core.windows.net/stage/\nproducts.csv' , format = 'csv', header = True)\ndelta_table path = \"/delta/products-delta\"\n\n\ndf.XXX.save (delta_table_path)\ndeltaTable = YYY(spark, delta_table_path)",
    "question_type": "multiple",
    "choices": [
      "\"X: cache()","X: inputFiles()","X: write.format(\"delta\")","X: write.parquet",
      "Y: deltaTable.alias","Y: deltaTable.convertToDelta","Y: deltaTable.forPath","Y: deltaTable.update"
    ],
    "site_answers": [
      "X: write.format(\"delta\")","Y: deltaTable.forPath"
    ],
    "links":"https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/03-create-delta-tables"
  },
  {
    "question_text": "HOTSPOT -You have an Azure Data Lake Storage account that contains one CSV file per hour for January 1, 2020, through January 31, 2023. The files are partitioned by using the following folder structure.\n\ncsv/system1/{year}/{month}/{filename}.csv\n\nYou need to query the files by using an Azure Synapse Analytics serverless SQL pool. The solution must return the row count of each file created during the last three months of 2022.\n\nHow should you complete the query? To answer, select the appropriate options in the answer area.\n\nSELECT\n  r.filepath() AS filepath,\n  COUNT_BIG (*) AS [rows]\nFROM OPENROWSET (\n\n\n  BULK XXX\n\n\nDATA SOURCE = 'MyDataLake' ,\n  FORMAT = 'CSV' ,\n  PARSER_VERSION = '2.0',\n  FIRSTROW = 2 )\nWITH (vendor_id INT) AS [r]\nWHERE\n\n\n  YYY IN ('10', '11', '12' )\n\n\nGROUP BY",
    "question_type": "multiple",
    "choices": [
      "X: 'csv/system1/2022',","X: 'csv/system1/2022/',","X: 'csv/system1/2022/*/ *. csv',",
      "Y: r.filepath()","Y: r.filepath(1)","Y: r.filepath(2)"
    ],
    "site_answers": [
      "X: 'csv/system1/2022/*/ *. csv',","Y: r.filepath(1)"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-specific-files"
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse Analytics dedicated SQL pool named Pool1 that contains an external table named Sales. Sales contains sales data. Each row in Sales contain data on a single sale, including the name of the salesperson.\n\nYou need to implement row-level security (RLS). The solution must ensure that the salespeople can access only their respective sales.\n\nWhat should you do? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nCreate: X\nRestrict row access by using: Y",
    "question_type": "multiple",
    "choices": [
      "X: A materialized view in Pool1","X: A security policy for Sales","X: Database scoped credentials in Pool1",
      "Y: A masking rule","Y: A table-valued function","Y: The CONTAINS predicate"
    ],
    "site_answers": [
      "X: A security policy for Sales","Y: A table-valued function"
    ],
    "links": "https://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16"
  },
  {
    "question_text": "You have an Azure Data Factory pipeline named P1.\n\nYou need to schedule P1 to run at 10:15 AM, 12:15 PM, 2:15 PM, and 4:15 PM every day.\n\nWhich frequency and interval should you configure for the scheduled trigger?",
    "question_type": "single",
    "choices": [
      "Frequency: Month - Interval: 1",
      "Frequency: Day - Interval: 1",
      "Frequency: Minute - Interval: 60",
      "Frequency: Hour - Interval: 2"
    ],
    "site_answers": [
      "Frequency: Day - Interval: 1"
    ]
  },
  {
    "question_text": "You are creating an Azure Data Factory pipeline.\n\nYou need to add an activity to the pipeline. The activity must execute a Transact-SQL stored procedure that has the following characteristics:\u2022\tReturns the number of sales invoices for a current date\u2022\tDoes NOT require input parametersWhich type on activity should you use?",
    "question_type": "single",
    "choices": [
      "Stored Procedure",
      "Get Metadata",
      "Append Variable",
      "Lookup"
    ],
    "site_answers": [
      "Lookup"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure#:~:text=stored%20procedure%20activity%2C-,use%20lookup%20acitivty,-and%20Script%20activity"
  },
  {
    "question_text": "DRAG DROP -You have an Azure Databricks deployment and a local file named /tmp/file1 that contains the following code.\n\n[\n  {\n    \"string\":\"string1\",\n    \"int\":1,\n    \"dict\": {\"key\": \"'value1\"}\n  },{\n    \"string\": \"string2\",\n    \"int\": 2,\n    \"dict\": {\"key\": \"value2\",\"extra_key\": \"extra_value2\"}\n  }\n]\n\nYou need to read /tmp/file1 into a data frame by using Scala.\n\nHow should you complete the code? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\nval df = spark.read.option(\"XXX\", \"true\").YYY(\"/tmp/file1\")",
    "question_type": "multiple",
    "choices": [
      "Source","inferSchema","ignoreExtension",
      "json","Multiline","schema","text"
    ],
    "site_answers": [
      "Multiline","json"
    ],"
    "links": "https://learn.microsoft.com/en-us/azure/databricks/query/formats/json"
  },
  {
    "question_text": "You have an Azure subscription that contains a Microsoft Purview account.\n\nYou need to search the Microsoft Purview Data Catalog to identify assets that have an assetType property of Table or View.\n\nWhich query should you run?",
    "question_type": "single",
    "choices": [
      "assetType IN ('Table', 'View')",
      "assetType:Table OR assetType:view",
      "assetType = (Table OR View)",
      "assetType:(Table OR View)"
    ],
    "site_answers": [
      "assetType:Table OR assetType:view"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure subscription that contains a logical Microsoft SQL server named Server1. Server1 hosts an Azure Synapse Analytics SQL dedicated pool named Pool1.\n\nYou need to recommend a Transparent Data Encryption (TDE) solution for Server1. The solution must meet the following requirements:\u2711 Track the usage of encryption keys.\n\nMaintain the access of client apps to Pool1 in the event of an Azure datacenter outage that affects the availability of the encryption keys.\n\nWhat should you include in the recommendation? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nTo track encryption key usage: XXX\nTo maintain client app access in the event of a datacenter outage: YYY",
    "question_type": "multiple",
    "choices": [
      "X: Always Encrypted","X: TDE with customer-managed keys","X: TDE with platform-managed keys",
      "Y: Create and configure Azure key vaults in two Azure regions.","Y: Enable Advanced Data Security on Server1.","Y: Implement the client apps by using a Microsoft","Y: NET Framework data provider."
    ],
    "site_answers": [
      "X: TDE with customer-managed keys","Y: Create and configure Azure key vaults in two Azure regions."
    ],
    "links": "https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#recommendations-when-configuring-akv"
  },
  {
    "question_text": "You plan to create an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to minimize the time it takes to identify queries that return confidential information as defined by the company's data privacy regulations and the users who executed the queues.\n\nWhich two components should you include in the solution? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "sensitivity-classification labels applied to columns that contain confidential information",
      "resource tags for databases that contain confidential information",
      "audit logs sent to a Log Analytics workspace",
      "dynamic data masking for columns that contain confidential information"
    ],
    "site_answers": [
      "sensitivity-classification labels applied to columns that contain confidential information",
      "audit logs sent to a Log Analytics workspace"
    ]
  },
  {
    "question_text": "You are designing an enterprise data warehouse in Azure Synapse Analytics that will contain a table named Customers. Customers will contain credit card information.\n\nYou need to recommend a solution to provide salespeople with the ability to view all the entries in Customers. The solution must prevent all the salespeople from viewing or inferring the credit card information.\n\nWhat should you include in the recommendation?",
    "question_type": "single",
    "choices": [
      "data masking",
      "Always Encrypted",
      "column-level security",
      "row-level security"
    ],
    "site_answers": [
      "column-level security"
    ]
  },
  {
    "question_text": "You develop data engineering solutions for a company.\n\nA project requires the deployment of data to Azure Data Lake Storage.\n\nYou need to implement role-based access control (RBAC) so that project members can manage the Azure Data Lake Storage resources.\n\nWhich three actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Create security groups in Azure Active Directory (Azure AD) and add project members.",
      "Configure end-user authentication for the Azure Data Lake Storage account.",
      "Assign Azure AD security groups to Azure Data Lake Storage.",
      "Configure Service-to-service authentication for the Azure Data Lake Storage account.",
      "Configure access control lists (ACL) for the Azure Data Lake Storage account."
    ],
    "site_answers": [
      "Create security groups in Azure Active Directory (Azure AD) and add project members.",
      "Assign Azure AD security groups to Azure Data Lake Storage.",
      "Configure access control lists (ACL) for the Azure Data Lake Storage account."
    ]
  },
  {
    "question_text": "You have an Azure Data Factory version 2 (V2) resource named Df1. Df1 contains a linked service.\n\nYou have an Azure Key vault named vault1 that contains an encryption key named key1.\n\nYou need to encrypt Df1 by using key1.\n\nWhat should you do first?",
    "question_type": "single",
    "choices": [
      "Add a private endpoint connection to vault1.",
      "Enable Azure role-based access control on vault1.",
      "Remove the linked service from Df1.",
      "Create a self-hosted integration runtime."
    ],
    "site_answers": [
      "Remove the linked service from Df1."
    ],
    "links": " https://docs.microsoft.com/en-us/azure/data-factory/enable-customer-managed-key#post-factory-creation-in-data-factory-ui"
  },
  {
    "question_text": "You are designing an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to ensure that you can audit access to Personally Identifiable Information (PII).\n\nWhat should you include in the solution?",
    "question_type": "single",
    "choices": [
      "column-level security",
      "dynamic data masking",
      "row-level security (RLS)",
      "sensitivity classifications"
    ],
    "site_answers": [
      "sensitivity classifications"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/azure-sql/database/data-discovery-and-classification-overview?view=azuresql#audit-sensitive-data"
  },
  {
    "question_text": "HOTSPOT -You have an Azure subscription that contains an Azure Data Lake Storage account. The storage account contains a data lake named DataLake1.\n\nYou plan to use an Azure data factory to ingest data from a folder in DataLake1, transform the data, and land the data in another folder.\n\nYou need to ensure that the data factory can read and write data from any folder in the DataLake1 container. The solution must meet the following requirements:\u2022\tMinimize the risk of unauthorized user access.\u2022\tUse the principle of least privilege.\u2022\tMinimize maintenance effort.\n\nHow should you configure access to the storage account for the data factory? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nUse XXX to authenticate by using YYY.",
    "question_type": "multiple",
    "choices": [
      "X: Microsoft Azure Active Directory (Azure AD), part of Microsoft Entra","X: a shared access signature (SAS)","X: a shared key",
      "Y: a managed identity","Y: a stored access policy","Y: an Authorization header"
    ],
    "site_answers": [
      "X: Microsoft Azure Active Directory (Azure AD), part of Microsoft Entra","Y: a managed identity"
    ]
  },
  {
    "question_text": "You have a data warehouse in Azure Synapse Analytics.\n\nYou need to ensure that the data in the data warehouse is encrypted at rest.\n\nWhat should you enable?",
    "question_type": "single",
    "choices": [
      "Advanced Data Security for this database",
      "Transparent Data Encryption (TDE)",
      "Secure transfer required",
      "Dynamic Data Masking"
    ],
    "site_answers": [
      "Transparent Data Encryption (TDE)"
    ]
  },
  {
    "question_text": "You are designing a streaming data solution that will ingest variable volumes of data.\n\nYou need to ensure that you can change the partition count after creation.\n\nWhich service should you use to ingest the data?",
    "question_type": "single",
    "choices": [
      "Azure Event Hubs Dedicated",
      "Azure Stream Analytics",
      "Azure Data Factory",
      "Azure Synapse Analytics"
    ],
    "site_answers": [
      "Azure Event Hubs Dedicated"
    ],
    "links": "https://docs.microsoft.com/en-us/azure/event-hubs/dynamically-add-partitions"
  },
  {
    "question_text": "You are designing a date dimension table in an Azure Synapse Analytics dedicated SQL pool. The date dimension table will be used by all the fact tables.\n\nWhich distribution type should you recommend to minimize data movement during queries?",
    "question_type": "single",
    "choices": [
      "HASH",
      "REPLICATE",
      "ROUND_ROBIN"
    ],
    "site_answers": [
      "REPLICATE"
    ]
  },
  {
    "question_text": "HOTSPOT -You develop a dataset named DBTBL1 by using Azure Databricks.\n\nDBTBL1 contains the following columns:\u2711 SensorTypeID\u2711 GeographyRegionID\u2711 Year\u2711 Month\u2711 Day\u2711 Hour\u2711 Minute\u2711 Temperature\u2711 WindSpeed\u2711 OtherYou need to store the data to support daily incremental load pipelines that vary for each GeographyRegionID. The solution must minimize storage costs.\n\nHow should you complete the code? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\ndf.write.XXX(\"YYY\").mode(\"append\").ZZZ",
    "question_type": "multiple",
    "choices": [
      "X: bucketBy","X: format","X: partitionBy","X: sortBy",
      "Y: (\"*\")","Y: (\"GeographyRegionID\")","Y: (\"GeographyRegionlD\", \"Year\", \"Month\", \"Day\")","Y: (\"Year\", \"Month\", \"Day\", \"GeographyRegionID\")",
      "Z: .csv(\"/DBTBL1\")","Z: .json(\"/DBTBL1\")","Z: parquet(\"/DBTBL1\")","Z: .saveAsTable(\"/DBTBL1\")"
    ],
    "site_answers": [
      "X: partitionBy","Y: (\"GeographyRegionlD\", \"Year\", \"Month\", \"Day\")","Z: parquet(\"/DBTBL1\")"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices"
  },
  {
    "question_text": "You are designing a security model for an Azure Synapse Analytics dedicated SQL pool that will support multiple companies.\n\nYou need to ensure that users from each company can view only the data of their respective company.\n\nWhich two objects should you include in the solution? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "a security policy",
      "a custom role-based access control (RBAC) role",
      "a predicate function",
      "a column encryption key",
      "asymmetric keys"
    ],
    "site_answers": [
      "a security policy",
      "a predicate function"
    ],
    "links": "https://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16#Description"
  },
  {
    "question_text": "You have a SQL pool in Azure Synapse that contains a table named dbo.\n\nCustomers. The table contains a column name Email.\n\nYou need to prevent nonadministrative users from seeing the full email addresses in the Email column. The users must see values in a format of [email\u00a0protected] instead.\n\nWhat should you do?",
    "question_type": "single",
    "choices": [
      "From Microsoft SQL Server Management Studio, set an email mask on the Email column.",
      "From the Azure portal, set a mask on the Email column.",
      "From Microsoft SQL Server Management Studio, grant the SELECT permission to the users for all the columns in the dbo.Customers table except Email.",
      "From the Azure portal, set a sensitivity classification of Confidential for the Email column."
    ],
    "site_answers": [
      "From Microsoft SQL Server Management Studio, set an email mask on the Email column."
    ]
  },
  {
    "question_text": "You have an Azure Data Lake Storage Gen2 account named adls2 that is protected by a virtual network.\n\nYou are designing a SQL pool in Azure Synapse that will use adls2 as a source.\n\nWhat should you use to authenticate to adls2?",
    "question_type": "single",
    "choices": [
      "an Azure Active Directory (Azure AD) user",
      "a shared key",
      "a shared access signature (SAS)",
      "a managed identity"
    ],
    "site_answers": [
      "a managed identity"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/quickstart-bulk-load-copy-tsql-examples#c-managed-identity"
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse Analytics SQL pool named Pool1. In Azure Active Directory (Azure AD), you have a security group named Group1.\n\nYou need to control the access of Group1 to specific columns and rows in a table in Pool1.\n\nWhich Transact-SQL commands should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nTo control access to the columns: X\nTo control access to the rows: Y",
    "question_type": "multiple",
    "choices": [
      "X: CREATE CRYPTOGRAPHIC PROVIDER","X: CREATE PARTITION FUNCTION","X: CREATE SECURITY POLICY","X: GRANT",
      "Y: CREATE CRYPTOGRAPHIC PROVIDER","Y: CREATE PARTITION FUNCTION","Y: CREATE SECURITY POLICY","Y: GRANT"
    ],
    "site_answers": [
      "X: GRANT","Y: CREATE SECURITY POLICY"
    ],
    "links": ["https://docs.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver15", "https://docs.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver15"]
  },
  {
    "question_text": "HOTSPOT -You need to implement an Azure Databricks cluster that automatically connects to Azure Data Lake Storage Gen2 by using Azure Active Directory (Azure AD) integration.\n\nHow should you configure the new cluster? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nTier: X\nAdvanced option to enable: Y",
    "question_type": "multiple",
    "choices": [
      "X: Premium","X: Standard","Legacy, now use unity catalogue",
      "Y: Azure Data Lake Storage Credential Passthrough","Y: table Access Control"
    ],
    "site_answers": [
      "X: Premium","Y: Azure Data Lake Storage Credential Passthrough"
    ],
    "links": ["https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough", "https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/"]
  },
  {
    "question_text": "You are designing an Azure Synapse solution that will provide a query interface for the data stored in an Azure Storage account. The storage account is only accessible from a virtual network.\n\nYou need to recommend an authentication mechanism to ensure that the solution can access the source data.\n\nWhat should you recommend?",
    "question_type": "single",
    "choices": [
      "a managed identity",
      "anonymous public read access",
      "a shared key"
    ],
    "site_answers": [
      "a managed identity"
    ]
  },
  {
    "question_text": "You are developing an application that uses Azure Data Lake Storage Gen2.\n\nYou need to recommend a solution to grant permissions to a specific application for a limited time period.\n\nWhat should you include in the recommendation?",
    "question_type": "single",
    "choices": [
      "role assignments",
      "shared access signatures (SAS)",
      "Azure Active Directory (Azure AD) identities",
      "account keys"
    ],
    "site_answers": [
      "shared access signatures (SAS)"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Contacts. Contacts contains a column named Phone.\n\nYou need to ensure that users in a specific role only see the last four digits of a phone number when querying the Phone column.\n\nWhat should you include in the solution?",
    "question_type": "single",
    "choices": [
      "table partitions",
      "a default value",
      "row-level security (RLS)",
      "column encryption",
      "dynamic data masking"
    ],
    "site_answers": [
      "dynamic data masking"
    ]
  },
  {
    "question_text": "You are designing database for an Azure Synapse Analytics dedicated SQL pool to support workloads for detecting ecommerce transaction fraud.\n\nData will be combined from multiple ecommerce sites and can include sensitive financial information such as credit card numbers.\n\nYou need to recommend a solution that meets the following requirements:Users must be able to identify potentially fraudulent transactions.\u2711 Users must be able to use credit cards as a potential feature in models.\u2711 Users must NOT be able to access the actual credit card numbers.\n\nWhat should you include in the recommendation?",
    "question_type": "single",
    "choices": [
      "Transparent Data Encryption (TDE)",
      "row-level security (RLS)",
      "column-level encryption",
      "Azure Active Directory (Azure AD) pass-through authentication"
    ],
    "site_answers": [
      "column-level encryption"
    ]
  },
  {
    "question_text": "You have an Azure subscription linked to an Azure Active Directory (Azure AD) tenant that contains a service principal named ServicePrincipal1. The subscription contains an Azure Data Lake Storage account named adls1. Adls1 contains a folder named Folder2 that has a URI of https://adls1.dfs.core.windows.net/ container1/Folder1/Folder2/.\n\nServicePrincipal1 has the access control list (ACL) permissions shown in the following table.\n\nResource    Permission\ncontainer1  Access - Execute\nFolder1     Access - Execute\nFolder2     Access - Read\n\nYou need to ensure that ServicePrincipal1 can perform the following actions:\u2711 Traverse child items that are created in Folder2.\u2711 Read files that are created in Folder2.\n\nThe solution must use the principle of least privilege.\n\nWhich two permissions should you grant to ServicePrincipal1 for Folder2? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Access - Read",
      "Access - Write",
      "Access - Execute",
      "Default - Read",
      "Default - Write",
      "Default - Execute"
    ],
    "site_answers": [
      "Access - Execute",
      "Default - Read"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure subscription that is linked to a hybrid Azure Active Directory (Azure AD) tenant. The subscription contains an Azure Synapse Analytics SQL pool named Pool1.\n\nYou need to recommend an authentication solution for Pool1. The solution must support multi-factor authentication (MFA) and database-level authentication.\n\nWhich authentication solution or solutions should you include in the recommendation? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nMFA: X\nDatabase-level authentification: Y",
    "question_type": "multiple",
    "choices": [
      "X: Azure AD authentication","X: Microsoft SQL Server authenticati","X: Passwordless authentication","X: Windows authentication",
      "Y: Application roles","Y: Contained database users","Y: Database roles","Y: Microsoft SQL Server logins"
    ],
    "site_answers": [
      "X: Azure AD authentication","Y: Contained database users"
    ],
    "links": "https://docs.microsoft.com/en-us/sql/relational-databases/security/contained-database-users-making-your-database-portable?view=sql-server-ver15#contained-database-user-model"
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to ensure that data in the pool is encrypted at rest. The solution must NOT require modifying applications that query the data.\n\nWhat should you do?",
    "question_type": "single",
    "choices": [
      "Enable encryption at rest for the Azure Data Lake Storage Gen2 account.",
      "Enable Transparent Data Encryption (TDE) for the pool.",
      "Use a customer-managed key to enable double encryption for the Azure Synapse workspace.",
      "Create an Azure key vault in the Azure subscription grant access to the pool."
    ],
    "site_answers": [
      "Enable Transparent Data Encryption (TDE) for the pool."
    ],
    "links": "https://docs.microsoft.com/en-us/azure/synapse-analytics/security/workspaces-encryption"
  },
  {
    "question_text": "DRAG DROP -You have an Azure subscription that contains an Azure Data Lake Storage Gen2 account named storage1. Storage1 contains a container named container1.\n\nContainer1 contains a directory named directory1. Directory1 contains a file named file1.\n\nYou have an Azure Active Directory (Azure AD) user named User1 that is assigned the Storage Blob Data Reader role for storage1.\n\nYou need to ensure that User1 can append data to file1. The solution must use the principle of least privilege.\n\nWhich permissions should you grant? To answer, drag the appropriate permissions to the correct resources. Each permission may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nSelect and Place:\n\ncontainer1: \ndirectory1: \nfile1: ",
    "question_type": "multiple",
    "choices": [
      "Read","Write","Execute","Execute"
    ],
    "site_answers": [
      "Execute","Execute","Write"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure subscription that contains an Azure Databricks workspace named databricks1 and an Azure Synapse Analytics workspace named synapse1.\n\nThe synapse1 workspace contains an Apache Spark pool named pool1.\n\nYou need to share an Apache Hive catalog of pool1 with databricks1.\n\nWhat should you do? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:",
    "question_type": "multiple",
    "choices": [
      "X: Azure Cosmos DB","X: Azure Data Lake Storage Gen2","X: Azure SQL Database",
      "Y: An Azure Purview account","Y: A Hive metastore","Y: A managed Hive metastore service"
    ],
    "site_answers": [
      "X: Azure SQL Database","Y: A Hive metastore"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure subscription.\n\nYou need to deploy an Azure Data Lake Storage Gen2 Premium account. The solution must meet the following requirements:* Blobs that are older than 365 days must be deleted.* Administrative effort must be minimized.* Costs must be minimized.\n\nWhat should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nTo minimize costs: X\nTo delete blobs: Y",
    "question_type": "multiple",
    "choices": [
      "X: Locally-redundant storage (LRS)","X: The Archive access tier","X: The Cool access tier","X: Zone-redundant storage (ZRS)",
      "Y: Azure Automation runbooks","Y: Azure Storage lifecycle management","Y: Soft delete"
    ],
    "site_answers": [
      "X: Locally-redundant storage (LRS)","Y: Azure Storage lifecycle management"
    ]
  },
  {
    "question_text": "HOTSPOT -You are designing an application that will use an Azure Data Lake Storage Gen 2 account to store petabytes of license plate photos from toll booths. The account will use zone-redundant storage (ZRS).\n\nYou identify the following usage patterns:* The data will be accessed several times a day during the first 30 days after the data is created. The data must meet an availability SLA of 99.9%.* After 90 days, the data will be accessed infrequently but must be available within 30 seconds.* After 365 days, the data will be accessed infrequently but must be available within five minutes.\n\nYou need to recommend a data retention solution. The solution must minimize costs.\n\nWhich access tier should you recommend for each time frame? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\nFirst 30 days: X\nAfter 90 days: Y\nAfter365 days: Z",
    "question_type": "multiple",
    "choices": [
      "X: Archive","X:Cool","X: Hot",
      "Y: Archive","Y:Cool","Y: Hot",
      "Z: Archive","Z:Cool","Z: Hot",
    ],
    "site_answers": [
      "X: Hot","Y: Cool","Z: Cool"
    ]
  },
  {
    "question_text": "DRAG DROP -You have an Azure Data Lake Storage Gen 2 account named storage1.\n\nYou need to recommend a solution for accessing the content in storage1. The solution must meet the following requirements:\u2022\tList and read permissions must be granted at the storage account level.\u2022\tAdditional permissions can be applied to individual objects in storage1.\u2022\tSecurity principals from Microsoft Azure Active Directory (Azure AD), part of Microsoft Entra, must be used for authentication.\n\nWhat should you use? To answer, drag the appropriate components to the correct requirements. Each component may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.\n\nTo grant permissions at the storage account level: \nTo grant permissions at the object level: ",
    "question_type": "multiple",
    "choices": [
      "Access control lists (ACLs)","Role-based access control (RBAC) roles","Shared access signatures (SAS)","Shared account keys"
    ],
    "site_answers": [
      "Role-based access control (RBAC) roles","Access control lists (ACLs)"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1 that contains a table named Sales.\n\nSales has row-level security (RLS) applied. RLS uses the following predicate filter.\n\nCREATE FUNCTION Security.fn_securitypredicate(@SalesRep AS sysname\n  RETURNS TABLE\nWITH SCHEMABINDING\nAS\n  RETURN SELECT 1 AS fn_securitypredicate_result\nWHERE @SalesRep = USER_NAME() OR USER_NAME() = 'Manager';\n\nA user named SalesUser1 is assigned the db_datareader role for Pool1.\n\nWhich rows in the Sales table are returned when SalesUser1 queries the table?",
    "question_type": "single",
    "choices": [
      "only the rows for which the value in the User_Name column is SalesUser1",
      "all the rows",
      "only the rows for which the value in the SalesRep column is Manager",
      "only the rows for which the value in the SalesRep column is SalesUser1"
    ],
    "site_answers": [
      "only the rows for which the value in the SalesRep column is SalesUser1"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Data Lake Storage Gen2 account named account1 that contains the resources shown in the following table.\n\nYou need to configure access control lists (ACLs) to allow a user named User1 to delete File1. User1 is NOT assigned any role-based access control (RBAC) roles for account1. The solution must use the principle of least privilege.\n\nWhich type of ACL should you configure for each resource? To answer select the appropriate options in the answer area.\n\ncontainer1: X\ndirectory1: Y\nfile1: Z",
    "question_type": "multiple",
    "choices": [
      "X: --- permissions","X: -wx permissions","X: --x permissions",
      "Y: --- permissions","Y: -wx permissions","Y: --x permissions",
      "Z: --- permissions","Z: -wx permissions","Z: --x permissions"
    ],
    "site_answers": [
      "X: --x permissions","Y: -wx permissions","Z: --- permissions"
    ]
  },
  {
    "question_text": "You have an Azure subscription that is linked to a tenant in Microsoft Azure Active Directory (Azure AD), part of Microsoft Entra. The tenant that contains a security group named Group1. The subscription contains an Azure Data Lake Storage account named myaccount1. The myaccount1 account contains two containers named container1 and container2.\n\nYou need to grant Group1 read access to container1. The solution must use the principle of least privilege.\n\nWhich role should you assign to Group1?",
    "question_type": "single",
    "choices": [
      "Storage Table Data Reader for myaccount1",
      "Storage Blob Data Reader for container1",
      "Storage Blob Data Reader for myaccount1",
      "Storage Table Data Reader for container1"
    ],
    "site_answers": [
      "Storage Blob Data Reader for container1"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool that contains a table named dbo.\n\nUsers.\n\nYou need to prevent a group of users from reading user email addresses from dbo.\n\nUsers.\n\nWhat should you use?",
    "question_type": "single",
    "choices": [
      "column-level security",
      "row-level security (RLS)",
      "Transparent Data Encryption (TOE)",
      "dynamic data masking"
    ],
    "site_answers": [
      "column-level security"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse Analytics dedicated SQL pool that hosts a database named DB1.\n\nYou need to ensure that DB1 meets the following security requirements:\u2022\tWhen credit card numbers show in applications, only the last four digits must be visible.\u2022\tTax numbers must be visible only to specific users.\n\nWhat should you use for each requirement? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nCredit card numbers: X\nTax numbers: Y",
    "question_type": "multiple",
    "choices": [
      "X: Column-level security","X: Dynamic Data Masking","X: Row-level security (RLS)",
      "Y: Column-level security","Y: Row-level security (RLS)","Y: Transparent Database Encryption (TDE)"
    ],
    "site_answers": [
      "X: Dynamic Data Masking","Y: Column-level security"
    ]
  },
  {
    "question_text": "You have an Azure subscription that contains a storage account named storage1 and an Azure Synapse Analytics dedicated SQL pool. The storage1 account contains a CSV file that requires an account key for access.\n\nYou plan to read the contents of the CSV file by using an external table.\n\nYou need to create an external data source for the external table.\n\nWhat should you create first?",
    "question_type": "single",
    "choices": [
      "a database role",
      "a database scoped credential",
      "a database view",
      "an external file format"
    ],
    "site_answers": [
      "a database scoped credential"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-external-tables"
  },
  {
    "question_text": "You have a tenant in Microsoft Azure Active Directory (Azure AD), part of Microsoft Entra. The tenant contains a group named Group1.\n\nYou have an Azure subscription that contains the resources shown in the following table.\n\nYou need to ensure that members of Group1 can read CSV files from storage1 by using the OPENROWSET function. The solution must meet the following requirements:\u2022\tThe members of Group1 must use credential1 to access storage1.\u2022\tThe principle of least privilege must be followed.\n\nWhich permission should you grant to Group1?",
    "question_type": "single",
    "choices": [
      "EXECUTE",
      "CONTROL",
      "REFERENCES",
      "SELECT"
    ],
    "site_answers": [
      "REFERENCES"
    ]
  },
  {
    "question_text": "You have an Azure subscription that contains an Azure Data Lake Storage account named dl1 and an Azure Analytics Synapse workspace named workspace1.\n\nYou need to query the data in dl1 by using an Apache Spark pool named Pool1 in workspace1. The solution must ensure that the data is accessible Pool1.\n\nWhich two actions achieve the goal? Each correct answer presents a complete solution.\n\nNOTE: Each correct answer is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Implement Azure Synapse Link.",
      "Load the data to the primary storage account of workspace1.",
      "From workspace1, create a linked service for the dl1.",
      "From Microsoft Purview, register dl1 as a data source."
    ],
    "site_answers": [
      "Load the data to the primary storage account of workspace1.",
      "From workspace1, create a linked service for the dl1."
    ],
    "links": "https://learn.microsoft.com/en-us/troubleshoot/azure/synapse-analytics/spark/spark-jobexec-storage-access#common-issues-and-solutions"
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool named SQL1 and a user named User1.\n\nYou need to ensure that User1 can view requests associated with SQL1 by querying the sys.dm_pdw_exec_requests dynamic management view. The solution must follow the principle of least privilege.\n\nWhich permission should you grant to User1?",
    "question_type": "single",
    "choices": [
      "VIEW DATABASE STATE",
      "SHOWPLAN",
      "CONTROL SERVER",
      "VIEW ANY DATABASE"
    ],
    "site_answers": [
      "VIEW DATABASE STATE"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor"
  },
  {
    "question_text": "You have a Microsoft Entra tenant.\n\nThe tenant contains an Azure Data Lake Storage Gen2 account named storage1 that has two containers named fs1 and fs2.\n\nYou have a Microsoft Entra group named DepartmentA.\n\nYou need to meet the following requirements:\u2022\tDepartmentA must be able to read, write, and list all the files in fs1.\u2022\tDepartmentA must be prevented from accessing any files in fs2.\u2022\tThe solution must use the principle of least privilege.\n\nWhich role should you assign to DepartmentA?",
    "question_type": "single",
    "choices": [
      "Contributor for fs1",
      "Storage Blob Data Owner for fs1",
      "Storage Blob Data Contributor for storage1",
      "Storage Blob Data Contributor for fs1"
    ],
    "site_answers": [
      "Storage Blob Data Contributor for fs1"
    ]
  },
  {
    "question_text": "You implement an enterprise data warehouse in Azure Synapse Analytics.\n\nYou have a large fact table that is 10 terabytes (TB) in size.\n\nIncoming queries use the primary key SaleKey column to retrieve data as displayed in the following table:You need to distribute the large fact table across multiple nodes to optimize performance of the table.\n\nWhich technology should you use?",
    "question_type": "single",
    "choices": [
      "hash distributed table with clustered index",
      "hash distributed table with clustered Columnstore index",
      "round robin distributed table with clustered index",
      "round robin distributed table with clustered Columnstore index",
      "heap table with distribution replicate"
    ],
    "site_answers": [
      "hash distributed table with clustered Columnstore index"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool that contains a large fact table. The table contains 50 columns and 5 billion rows and is a heap.\n\nMost queries against the table aggregate values from approximately 100 million rows and return only two columns.\n\nYou discover that the queries against the fact table are very slow.\n\nWhich type of index should you add to provide the fastest query times?",
    "question_type": "single",
    "choices": [
      "nonclustered columnstore",
      "clustered columnstore",
      "nonclustered",
      "clustered"
    ],
    "site_answers": [
      "clustered columnstore"
    ]
  },
  {
    "question_text": "You create an Azure Databricks cluster and specify an additional library to install.\n\nWhen you attempt to load the library to a notebook, the library in not found.\n\nYou need to identify the cause of the issue.\n\nWhat should you review?",
    "question_type": "single",
    "choices": [
      "notebook logs",
      "cluster event logs",
      "global init scripts logs",
      "workspace logs"
    ],
    "site_answers": [
      "cluster event logs"
    ],
    "links": "https://docs.microsoft.com/en-us/azure/databricks/clusters/clusters-manage#event-log"
  },
  {
    "question_text": "You have an Azure data factory.\n\nYou need to examine the pipeline failures from the last 60 days.\n\nWhat should you use?",
    "question_type": "single",
    "choices": [
      "the Activity log blade for the Data Factory resource",
      "the Monitor & Manage app in Data Factory",
      "the Resource health blade for the Data Factory resource",
      "Azure Monitor"
    ],
    "site_answers": [
      "Azure Monitor"
    ]
  },
  {
    "question_text": "You are monitoring an Azure Stream Analytics job.\n\nThe Backlogged Input Events count has been 20 for the last hour.\n\nYou need to reduce the Backlogged Input Events count.\n\nWhat should you do?",
    "question_type": "single",
    "choices": [
      "Drop late arriving events from the job.",
      "Add an Azure Storage account to the job.",
      "Increase the streaming units for the job.",
      "Stop the job."
    ],
    "site_answers": [
      "Increase the streaming units for the job."
    ]
  },
  {
    "question_text": "You are designing an Azure Databricks interactive cluster. The cluster will be used infrequently and will be configured for auto-termination.\n\nYou need to ensure that the cluster configuration is retained indefinitely after the cluster is terminated. The solution must minimize costs.\n\nWhat should you do?",
    "question_type": "single",
    "choices": [
      "Pin the cluster.",
      "Create an Azure runbook that starts the cluster every 90 days.",
      "Terminate the cluster manually when processing completes.",
      "Clone the cluster after it is terminated."
    ],
    "site_answers": [
      "Pin the cluster."
    ],
    "links": "https://learn.microsoft.com/en-us/azure/databricks/compute/clusters-manage"
  },
  {
    "question_text": "You have an Azure data solution that contains an enterprise data warehouse in Azure Synapse Analytics named DW1.\n\nSeveral users execute ad hoc queries to DW1 concurrently.\n\nYou regularly perform automated data loads to DW1.\n\nYou need to ensure that the automated data loads have enough memory available to complete quickly and successfully when the adhoc queries run.\n\nWhat should you do?",
    "question_type": "single",
    "choices": [
      "Hash distribute the large fact tables in DW1 before performing the automated data loads.",
      "Assign a smaller resource class to the automated data load queries.",
      "Assign a larger resource class to the automated data load queries.",
      "Create sampled statistics for every column in each table of DW1."
    ],
    "site_answers": [
      "Assign a larger resource class to the automated data load queries."
    ],
    "links": "https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/resource-classes-for-workload-management"
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1 and a database named DB1. DB1 contains a fact table named Table1.\n\nYou need to identify the extent of the data skew in Table1.\n\nWhat should you do in Synapse Studio?",
    "question_type": "single",
    "choices": [
      "Connect to the built-in pool and run DBCC PDW_SHOWSPACEUSED.",
      "Connect to the built-in pool and run DBCC CHECKALLOC.",
      "Connect to Pool1 and query sys.dm_pdw_node_status.",
      "Connect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats."
    ],
    "site_answers": [
      "Connect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats."
    ],
    "links": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute"
  },
  {
    "question_text": "HOTSPOT -You need to collect application metrics, streaming query events, and application log messages for an Azure Databrick cluster.\n\nWhich type of library and workspace should you implement? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nLibrary: X\nWorkspace: Y",
    "question_type": "multiple",
    "choices": [
      "X: Azure Databricks Monitoring Library","X: Microsoft Azure Management Monitoring Library","X: PyTorch","X: TensorFlow",
      "Y: Azure Databricks","Y: Azure Log Analytics","Y: Azure Machine Learning"
    ],
    "site_answers": [
      "X: Azure Databricks Monitoring Library","Y: Azure Log Analytics"
    ],
    "links": "https://docs.microsoft.com/en-us/azure/architecture/databricks-monitoring/application-logs"
  },
  {
    "question_text": "You have a SQL pool in Azure Synapse.\n\nYou discover that some queries fail or take a long time to complete.\n\nYou need to monitor for transactions that have rolled back.\n\nWhich dynamic management view should you query?",
    "question_type": "single",
    "choices": [
      "sys.dm_pdw_request_steps",
      "sys.dm_pdw_nodes_tran_database_transactions",
      "sys.dm_pdw_waits",
      "sys.dm_pdw_exec_sessions"
    ],
    "site_answers": [
      "sys.dm_pdw_nodes_tran_database_transactions"
    ]
  },
  {
    "question_text": "You are monitoring an Azure Stream Analytics job.\n\nYou discover that the Backlogged Input Events metric is increasing slowly and is consistently non-zero.\n\nYou need to ensure that the job can handle all the events.\n\nWhat should you do?",
    "question_type": "single",
    "choices": [
      "Change the compatibility level of the Stream Analytics job.",
      "Increase the number of streaming units (SUs).",
      "Remove any named consumer groups from the connection and use $default.",
      "Create an additional output stream for the existing input stream."
    ],
    "site_answers": [
      "Increase the number of streaming units (SUs)."
    ],
    "links": "https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-job-metrics"
  },
  {
    "question_text": "You are designing an inventory updates table in an Azure Synapse Analytics dedicated SQL pool. The table will have a clustered columnstore index and will include the following columns:\n\nTable                  Comment\nEventDate              One million records are added to the table each day\nEventTypeID            The table contains 10 million records for each event type.\nWarehouseID            The table contains 100 million records for each warehouse.\nProductCategoryTypeID  The table contains 25 million records for each product category type.\n\nYou identify the following usage patterns:\u2711 Analysts will most commonly analyze transactions for a warehouse.\u2711 Queries will summarize by product category type, date, and/or inventory event type.\n\nYou need to recommend a partition strategy for the table to minimize query times.\n\nOn which column should you partition the table?",
    "question_type": "single",
    "choices": [
      "EventTypeID",
      "ProductCategoryTypeID",
      "EventDate",
      "WarehouseID"
    ],
    "site_answers": [
      "WarehouseID"
    ]
  },
  {
    "question_text": "You are designing a star schema for a dataset that contains records of online orders. Each record includes an order date, an order due date, and an order ship date.\n\nYou need to ensure that the design provides the fastest query times of the records when querying for arbitrary date ranges and aggregating by fiscal calendar attributes.\n\nWhich two actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Create a date dimension table that has a DateTime key.",
      "Use built-in SQL functions to extract date attributes.",
      "Create a date dimension table that has an integer key in the format of YYYYMMDD.",
      "In the fact table, use integer columns for the date fields.",
      "Use DateTime columns for the date fields."
    ],
    "site_answers": [
      "Create a date dimension table that has an integer key in the format of YYYYMMDD.",
      "In the fact table, use integer columns for the date fields."
    ]
  },
  {
    "question_text": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.\n\nThe company must be able to monitor the devices in real-time.\n\nYou need to design the solution.\n\nWhat should you recommend?",
    "question_type": "single",
    "choices": [
      "Azure Analysis Services using Azure Portal",
      "Azure Analysis Services using Azure PowerShell",
      "Azure Stream Analytics cloud job using Azure Portal",
      "Azure Data Factory instance using Microsoft Visual Studio"
    ],
    "site_answers": [
      "Azure Stream Analytics cloud job using Azure Portal"
    ]
  },
  {
    "question_text": "You have a SQL pool in Azure Synapse.\n\nA user reports that queries against the pool take longer than expected to complete. You determine that the issue relates to queried columnstore segments.\n\nYou need to add monitoring to the underlying storage to help diagnose the issue.\n\nWhich two metrics should you monitor? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Snapshot Storage Size",
      "Cache used percentage",
      "DWU Limit",
      "Cache hit percentage"
    ],
    "site_answers": [
      "Cache used percentage",
      "Cache hit percentage"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-how-to-monitor-cache"
  },
  {
    "question_text": "You manage an enterprise data warehouse in Azure Synapse Analytics.\n\nUsers report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.\n\nYou need to monitor resource utilization to determine the source of the performance issues.\n\nWhich metric should you monitor?",
    "question_type": "single",
    "choices": [
      "DWU percentage",
      "Cache hit percentage",
      "DWU limit",
      "Data IO percentage"
    ],
    "site_answers": [
      "Cache hit percentage"
    ]
  },
  {
    "question_text": "You have an Azure Databricks resource.\n\nYou need to log actions that relate to changes in compute for the Databricks resource.\n\nWhich Databricks services should you log?",
    "question_type": "single",
    "choices": [
      "clusters",
      "workspace",
      "DBFS",
      "SSH",
      "jobs"
    ],
    "site_answers": [
      "clusters"
    ],
    "links": "https://docs.databricks.com/administration-guide/account-settings/audit-logs.html"
  },
  {
    "question_text": "You are designing a highly available Azure Data Lake Storage solution that will include geo-zone-redundant storage (GZRS).\n\nYou need to monitor for replication delays that can affect the recovery point objective (RPO).\n\nWhat should you include in the monitoring solution?",
    "question_type": "single",
    "choices": [
      "5xx: Server Error errors",
      "Average Success E2E Latency",
      "availability",
      "Last Sync Time"
    ],
    "site_answers": [
      "Last Sync Time"
    ],
    "links": "https://docs.microsoft.com/en-us/azure/storage/common/last-sync-time-get?tabs=azure-powershell"
  },
  {
    "question_text": "You configure monitoring for an Azure Synapse Analytics implementation. The implementation uses PolyBase to load data from comma-separated value (CSV) files stored in Azure Data Lake Storage Gen2 using an external table.\n\nFiles with an invalid schema cause errors to occur.\n\nYou need to monitor for an invalid schema error.\n\nFor which error should you monitor?",
    "question_type": "single",
    "choices": [
      "EXTERNAL TABLE access failed due to internal error: 'Java exception raised on call to HdfsBridge_Connect: Error [com.microsoft.polybase.client.KerberosSecureLogin] occurred while accessing external file.'",
      "Cannot execute the query \"Remote Query\" against OLE DB provider \"SQLNCLI11\" for linked server \"(null)\". Query aborted- the maximum reject threshold (0 rows) was reached while reading from an external source: 1 rows rejected out of total 1 rows processed.",
      "EXTERNAL TABLE access failed due to internal error: 'Java exception raised on call to HdfsBridge_Connect: Error [Unable to instantiate LoginClass] occurred while accessing external file.'",
      "EXTERNAL TABLE access failed due to internal error: 'Java exception raised on call to HdfsBridge_Connect: Error [No FileSystem for scheme: wasbs] occurred while accessing external file.'"
    ],
    "site_answers": [
      "Cannot execute the query \"Remote Query\" against OLE DB provider \"SQLNCLI11\" for linked server \"(null)\". Query aborted- the maximum reject threshold (0 rows) was reached while reading from an external source: 1 rows rejected out of total 1 rows processed."
    ],
    "links": "https://techcommunity.microsoft.com/t5/datacat/polybase-setup-errors-and-possible-solutions/ba-p/305297"
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool.\n\nYou run PDW_SHOWSPACEUSED('dbo.\n\nFactInternetSales'); and get the results shown in the following table.\n\nWhich statement accurately describes the dbo.\n\nFactInternetSales table?",
    "question_type": "single",
    "choices": [
      "All distributions contain data.",
      "The table contains less than 10,000 rows.",
      "The table uses round-robin distribution.",
      "The table is skewed."
    ],
    "site_answers": [
      "The table is skewed."
    ]
  },
  {
    "question_text": "You have two fact tables named Flight and Weather. Queries targeting the tables will be based on the join between the following columns.\n\nTable    Column\nFlight   ArrivalAirportID, ArrivalDateTime\nWeather  AirportID, ReportDateTime\n\nYou need to recommend a solution that maximizes query performance.\n\nWhat should you include in the recommendation?",
    "question_type": "single",
    "choices": [
      "In the tables use a hash distribution of ArrivalDateTime and ReportDateTime.",
      "In the tables use a hash distribution of ArrivalAirportID and AirportID.",
      "In each table, create an IDENTITY column.",
      "In each table, create a column as a composite of the other two columns in the table."
    ],
    "site_answers": [
      "In the tables use a hash distribution of ArrivalAirportID and AirportID."
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Data Factory pipeline that has the activities shown in the following exhibit.\n\nUse the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nStored procedure1 will execute Web1 and Set variable1 X.\nIf Web1 fails and Set variable2 succeeds, the pipeline status will be Y.",
    "question_type": "multiple",
    "choices": [
      "X: complete","X: fail","X: succeed",
      "Y: complete","Y: fail","Y: succeed"
    ],
    "site_answers": [
      "X: succeed", "Y: Faild"
    ],
    "links": "https://datasavvy.me/2021/02/18/azure-data-factory-activity-failures-and-pipeline-outcomes/"
  },
  {
    "question_text": "You have several Azure Data Factory pipelines that contain a mix of the following types of activities:\u2711 Wrangling data flow\u2711 Notebook\u2711 Copy\u2711 JarWhich two Azure services should you use to debug the activities? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point",
    "question_type": "multiple",
    "choices": [
      "Azure Synapse Analytics",
      "Azure HDInsight",
      "Azure Machine Learning",
      "Azure Data Factory",
      "Azure Databricks"
    ],
    "site_answers": [
      "Azure Data Factory",
      "Azure Databricks"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1 and a database named DB1. DB1 contains a fact table named Table1.\n\nYou need to identify the extent of the data skew in Table1.\n\nWhat should you do in Synapse Studio?",
    "question_type": "single",
    "choices": [
      "Connect to the built-in pool and run sys.dm_pdw_nodes_db_partition_stats.",
      "Connect to Pool1 and run DBCC CHECKALLOC.",
      "Connect to the built-in pool and run DBCC CHECKALLOC.",
      "Connect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats."
    ],
    "site_answers": [
      "Connect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats."
    ]
  },
  {
    "question_text": "You manage an enterprise data warehouse in Azure Synapse Analytics.\n\nUsers report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.\n\nYou need to monitor resource utilization to determine the source of the performance issues.\n\nWhich metric should you monitor?",
    "question_type": "single",
    "choices": [
      "Local tempdb percentage",
      "Cache used percentage",
      "Data IO percentage",
      "CPU percentage"
    ],
    "site_answers": [
      "Cache used percentage"
    ]
  },
  {
    "question_text": "You have an Azure data factory.\n\nYou need to examine the pipeline failures from the last 180 days.\n\nWhat should you use?",
    "question_type": "single",
    "choices": [
      "the Activity log blade for the Data Factory resource",
      "Pipeline runs in the Azure Data Factory user experience",
      "the Resource health blade for the Data Factory resource",
      "Azure Data Factory activity runs in Azure Monitor"
    ],
    "site_answers": [
      "Azure Data Factory activity runs in Azure Monitor"
    ],
  },
  {
    "question_text": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.\n\nThe company must be able to monitor the devices in real-time.\n\nYou need to design the solution.\n\nWhat should you recommend?",
    "question_type": "single",
    "choices": [
      "Azure Analysis Services using Azure PowerShell",
      "Azure Stream Analytics Edge application using Microsoft Visual Studio",
      "Azure Analysis Services using Microsoft Visual Studio",
      "Azure Data Factory instance using Azure Portal"
    ],
    "site_answers": [
      "Azure Stream Analytics Edge application using Microsoft Visual Studio"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/stream-analytics/quick-create-visual-studio-code"
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool named SA1 that contains a table named Table1.\n\nYou need to identify tables that have a high percentage of deleted rows.\n\nWhat should you run?",
    "question_type": "single",
    "choices": [
      "sys.pdw_nodes_column_store_segments",
      "sys.dm_db_column_store_row_group_operational_stats",
      "sys.pdw_nodes_column_store_row_groups",
      "sys.dm_db_column_store_row_group_physical_stats"
    ],
    "site_answers": [
      "sys.pdw_nodes_column_store_row_groups"
    ],
    "links": "https://learn.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sys-pdw-nodes-column-store-row-groups-transact-sql?view=aps-pdw-2016-au7"
  },
  {
    "question_text": "You have an enterprise data warehouse in Azure Synapse Analytics.\n\nYou need to monitor the data warehouse to identify whether you must scale up to a higher service level to accommodate the current workloads.\n\nWhich is the best metric to monitor?More than one answer choice may achieve the goal. Select the BEST answer.",
    "question_type": "single",
    "choices": [
      "DWU used",
      "CPU percentage",
      "DWU percentage",
      "Data IO percentage"
    ],
    "site_answers": [
      "DWU percentage"
    ]
  },
  {
    "question_text": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.\n\nThe company must be able to monitor the devices in real-time.\n\nYou need to design the solution.\n\nWhat should you recommend?",
    "question_type": "single",
    "choices": [
      "Azure Analysis Services using Azure PowerShell",
      "Azure Data Factory instance using Azure PowerShell",
      "Azure Stream Analytics cloud job using Azure Portal",
      "Azure Data Factory instance using Microsoft Visual Studio"
    ],
    "site_answers": [
      "Azure Stream Analytics cloud job using Azure Portal"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure event hub named retailhub that has 16 partitions. Transactions are posted to retailhub. Each transaction includes the transaction ID, the individual line items, and the payment details. The transaction ID is used as the partition key.\n\nYou are designing an Azure Stream Analytics job to identify potentially fraudulent transactions at a retail store. The job will use retailhub as the input. The job will output the transaction ID, the individual line items, the payment details, a fraud score, and a fraud indicator.\n\nYou plan to send the output to an Azure event hub named fraudhub.\n\nYou need to ensure that the fraud detection solution is highly scalable and processes transactions as quickly as possible.\n\nHow should you structure the output of the Stream Analytics job? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nNumber of partitions: X\nPartition key: Y",
    "question_type": "multiple",
    "choices": [
      "1","8","16","32",
      "Fraud indicator","Fraud score","Individual line items","Payment details","Transaction ID"
    ],
    "site_answers": [
      "16","Transaction ID"
    ]
  },
  {
    "question_text": "You have a partitioned table in an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to design queries to maximize the benefits of partition elimination.\n\nWhat should you include in the Transact-SQL queries?",
    "question_type": "single",
    "choices": [
      "JOIN",
      "WHERE",
      "DISTINCT",
      "GROUP BY"
    ],
    "site_answers": [
      "WHERE"
    ]
  },
  {
    "question_text": "You have an Azure Stream Analytics query. The query returns a result set that contains 10,000 distinct values for a column named clusterID.\n\nYou monitor the Stream Analytics job and discover high latency.\n\nYou need to reduce the latency.\n\nWhich two actions should you perform? Each correct answer presents a complete solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Add a pass-through query.",
      "Increase the number of streaming units.",
      "Add a temporal analytic function.",
      "Scale out the query by using PARTITION BY.",
      "Convert the query to a reference query."
    ],
    "site_answers": [
      "Increase the number of streaming units.",
      "Scale out the query by using PARTITION BY."
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1 and a database named DB1. DB1 contains a fact table named Table1.\n\nYou need to identify the extent of the data skew in Table1.\n\nWhat should you do in Synapse Studio?",
    "question_type": "single",
    "choices": [
      "Connect to the built-in pool and query sys.dm_pdw_nodes_db_partition_stats.",
      "Connect to the built-in pool and run DBCC CHECKALLOC.",
      "Connect to Pool1 and query sys.dm_pdw_node_status.",
      "Connect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats."
    ],
    "site_answers": [
      "Connect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats."
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 contains a fact table named Table1.\n\nYou need to identify the extent of the data skew in Table1.\n\nWhat should you do in Synapse Studio?",
    "question_type": "single",
    "choices": [
      "Connect to Pool1 and DBCC PDW_SHOWSPACEUSED.",
      "Connect to the built-in pool and run DBCC PDW_SHOWSPACEUSED.",
      "Connect to the built-in pool and run DBCC CHECKALLOC.",
      "Connect to the built-in pool and query sys.dm_pdw_sys_info."
    ],
    "site_answers": [
      "Connect to Pool1 and DBCC PDW_SHOWSPACEUSED."
    ],
    "links": "https://github.com/rgl/azure-content/blob/master/articles/sql-data-warehouse/sql-data-warehouse-manage-distributed-data-skew.md"
  },
  {
    "question_text": "You use Azure Data Lake Storage Gen2.\n\nYou need to ensure that workloads can use filter predicates and column projections to filter data at the time the data is read from disk.\n\nWhich two actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Reregister the Azure Storage resource provider.",
      "Create a storage policy that is scoped to a container.",
      "Reregister the Microsoft Data Lake Store resource provider.",
      "Create a storage policy that is scoped to a container prefix filter.",
      "Register the query acceleration feature."
    ],
    "site_answers": [
      "Create a storage policy that is scoped to a container prefix filter.",
      "Register the query acceleration feature."
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 contains a fact table named Table1.\n\nYou need to identify the extent of the data skew in Table1.\n\nWhat should you do in Synapse Studio?",
    "question_type": "single",
    "choices": [
      "Connect to Pool1 and run DBCC PDW_SHOWSPACEUSED.",
      "Connect to the built-in pool and run DBCC PDW_SHOWSPACEUSED.",
      "Connect to Pool1 and run DBCC CHECKALLOC.",
      "Connect to the built-in pool and query sys.dm_pdw_sys_info."
    ],
    "site_answers": [
      "Connect to Pool1 and run DBCC PDW_SHOWSPACEUSED."
    ]
  },
  {
    "question_text": "You have an Azure Data Lake Storage Gen2 account that contains two folders named Folder1 and Folder2.\n\nYou use Azure Data Factory to copy multiple files from Folder1 to Folder2.\n\nYou receive the following error.\n\nOperation on target Copy_sks failed: Failure happened on 'Sink' side.\n\nErrorCode=DelimitedTextMoreColumnsThanDefined,'Type=Microsoft.\n\nDataTransfer.\n\nCommon.\n\nSnared.\n\nHybridDeliveryException,Message=Error found when processing 'Csv/Tsv Format Text' source'0_2020_11_09_11_43_32.avro' with row number 53: found more columns than expected column count 27., Source=Microsoft.\n\nDataTransfer.\n\nComnon,'What should you do to resolve the error?",
    "question_type": "single",
    "choices": [
      "Change the Copy activity setting to Binary Copy.",
      "Lower the degree of copy parallelism.",
      "Add an explicit mapping.",
      "Enable fault tolerance to skip incompatible rows."
    ],
    "site_answers": [
      "Change the Copy activity setting to Binary Copy."
    ],
    "links": "https://learn.microsoft.com/en-us/azure/data-factory/pipeline-trigger-troubleshoot-guide#you-see-a-delimitedtextmorecolumnsthandefined-error-when-copying-a-pipeline"
  },
  {
    "question_text": "A company plans to use Apache Spark analytics to analyze intrusion detection data.\n\nYou need to recommend a solution to analyze network and system activity data for malicious activities and policy violations. The solution must minimize administrative efforts.\n\nWhat should you recommend?",
    "question_type": "single",
    "choices": [
      "Azure HDInsight",
      "Azure Data Factory",
      "Azure Data Lake Storage",
      "Azure Databricks"
    ],
    "site_answers": [
      "Azure Databricks"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/databricks/security/privacy/enhanced-security-monitoring"
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse Analytics dedicated SQL pool.\n\nYou need to monitor the database for long-running queries and identify which queries are waiting on resources.\n\nWhich dynamic management view should you use for each requirement? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct answer is worth one point.\nMonitor the database for long-running queries: X\nIdentify which queries are waiting one resources: Y",
    "question_type": "multiple",
    "choices": [
      "X: sys.dm_pdw_exec_requests","X: sys.dm_pdw_sql_requests","X: sys.dm_pdw_exec_sessions",
      "Y: sys.dm_pdw_waits","Y: sys.dm_pdw_lock_waits","Y: sys.resource_governor_worklood_groups"
    ],
    "site_answers": [
      "X: sys.dm_pdw_exec_requests","Y: sys.dm_pdw_waits"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor"
  },
  {
    "question_text": "You have an Azure Data Factory pipeline named pipeline1 that includes a Copy activity named Copy1. Copy1 has the following configurations:\u2022\tThe source of Copy1 is a table in an on-premises Microsoft SQL Server instance that is accessed by using a linked service connected via a self-hosted integration runtime.\u2022\tThe sink of Copy1 uses a table in an Azure SQL database that is accessed by using a linked service connected via an Azure integration runtime.\n\nYou need to maximize the amount of compute resources available to Copy1. The solution must minimize administrative effort.\n\nWhat should you do?",
    "question_type": "single",
    "choices": [
      "Scale out the self-hosted integration runtime.",
      "Scale up the data flow runtime of the Azure integration runtime and scale out the self-hosted integration runtime.",
      "Scale up the data flow runtime of the Azure integration runtime."
    ],
    "site_answers": [
      "Scale out the self-hosted integration runtime."
    ],
    "linsk": "https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime"
  },
  {
    "question_text": "You are designing a solution that will use tables in Delta Lake on Azure Databricks.\n\nYou need to minimize how long it takes to perform the following:\u2022\tQueries against non-partitioned tables\u2022\tJoins on non-partitioned columnsWhich two options should you include in the solution? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "the clone command",
      "Z-Ordering",
      "Apache Spark caching",
      "dynamic file pruning (DFP)"
    ],
    "site_answers": [
      "Z-Ordering",
      "dynamic file pruning (DFP)"
    ],
    "links": ["https://learn.microsoft.com/en-us/azure/databricks/optimizations/dynamic-file-pruning","https://learn.microsoft.com/en-us/azure/databricks/delta/data-skipping"]
  },
  {
    "question_text": "You have an Azure Data Lake Storage Gen2 account named account1 that contains a container named container1.\n\nYou plan to create lifecycle management policy rules for container1.\n\nYou need to ensure that you can create rules that will move blobs between access tiers based on when each blob was accessed last.\n\nWhat should you do first?",
    "question_type": "single",
    "choices": [
      "Configure object replication",
      "Create an Azure application",
      "Enable access time tracking",
      "Enable the hierarchical namespace"
    ],
    "site_answers": [
      "Enable access time tracking"
    ],
    "links": "https://azure.microsoft.com/en-us/updates/azure-blob-storage-last-access-time-tracking-now-generally-available/"
  },
  {
    "question_text": "You manage an enterprise data warehouse in Azure Synapse Analytics.\n\nUsers report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.\n\nYou need to monitor resource utilization to determine the source of the performance issues.\n\nWhich metric should you monitor?",
    "question_type": "single",
    "choices": [
      "DWU limit",
      "Data IO percentage",
      "Cache hit percentage",
      "CPU percentage"
    ],
    "site_answers": [
      "Cache hit percentage"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure data factory named DF1 that contains 10 pipelines.\n\nThe pipelines are executed hourly by using a schedule trigger. All activities are executed on an Azure integration runtime.\n\nYou need to ensure that you can identify trends in queue times across the pipeline executions and activities The solution must minimize administrative effort.\n\nHow should you configure the Diagnostic settings for DF1? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nCollect: X\nSend to: Y",
    "question_type": "multiple",
    "choices": [
      "X: Pipeline activity runs log","X: Pipeline runs log","X: Trigger runs log",
      "Y: Event hub","Y: Log Analytics workspace","Y: Storage account"
    ],
    "site_answers": [
      "X: Pipeline activity runs log","Y: Log Analytics workspace"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/activity-log?tabs=powershell#send-to-log-analytics-workspace"
  },
  {
    "question_text": "You manage an enterprise data warehouse in Azure Synapse Analytics.\n\nUsers report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.\n\nYou need to monitor resource utilization to determine the source of the performance issues.\n\nWhich metric should you monitor?",
    "question_type": "single",
    "choices": [
      "DWU percentage",
      "Cache hit percentage",
      "Data Warehouse Units (DWU) used",
      "Data IO percentage"
    ],
    "site_answers": [
      "Cache hit percentage"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 contains a fact table named Table1.\n\nYou need to identify the extent of the data skew in Table1.\n\nWhat should you do in Synapse Studio?",
    "question_type": "single",
    "choices": [
      "Connect to the built-in pool and query sys.dm_pdw_nodes_db_partition_stats.",
      "Connect to Pool1 and run DBCC PDW_SHOWSPACEUSED.",
      "Connect to Pool1 and query sys.dm_pdw_node_status.",
      "Connect to the built-in pool and query sys.dm_pdw_sys_info."
    ],
    "site_answers": [
      "Connect to Pool1 and run DBCC PDW_SHOWSPACEUSED."
    ]
  },
  {
    "question_text": "You have several Azure Data Factory pipelines that contain a mix of the following types of activities:\u2022\tPower Query\u2022\tNotebook\u2022\tCopy\u2022\tJarWhich two Azure services should you use to debug the activities? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Azure Machine Learning",
      "Azure Data Factory",
      "Azure Synapse Analytics",
      "Azure HDInsight",
      "Azure Databricks"
    ],
    "site_answers": [
      "Azure Data Factory",
      "Azure Databricks"
    ]
  },
  {
    "question_text": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.\n\nThe company must be able to monitor the devices in real-time.\n\nYou need to design the solution.\n\nWhat should you recommend?",
    "question_type": "single",
    "choices": [
      "Azure Analysis Services using Microsoft Visual Studio",
      "Azure Data Factory instance using Azure PowerShell",
      "Azure Analysis Services using Azure PowerShell",
      "Azure Stream Analytics cloud job using Azure Portal"
    ],
    "site_answers": [
      "Azure Stream Analytics cloud job using Azure Portal"
    ]
  },
  {
    "question_text": "You have an Azure Synapse Analytics dedicated SQL pool named pool1.\n\nYou need to perform a monthly audit of SQL statements that affect sensitive data. The solution must minimize administrative effort.\n\nWhat should you include in the solution?",
    "question_type": "single",
    "choices": [
      "workload management",
      "sensitivity labels",
      "dynamic data masking",
      "Microsoft Defender for SQL"
    ],
    "site_answers": [
      "sensitivity labels"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/azure-sql/database/data-discovery-and-classification-overview?view=azuresql#what-is-dc"
  },
  {
    "question_text": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.\n\nThe company must be able to monitor the devices in real-time.\n\nYou need to design the solution.\n\nWhat should you recommend?",
    "question_type": "single",
    "choices": [
      "Azure Analysis Services using Azure Portal",
      "Azure Stream Analytics Edge application using Microsoft Visual Studio",
      "Azure Analysis Services using Azure PowerShell",
      "Azure Analysis Services using Microsoft Visual Studio"
    ],
    "site_answers": [
      "Azure Stream Analytics Edge application using Microsoft Visual Studio"
    ]
  },
  {
    "question_text": "You manage an enterprise data warehouse in Azure Synapse Analytics.\n\nUsers report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.\n\nYou need to monitor resource utilization to determine the source of the performance issues.\n\nWhich metric should you monitor?",
    "question_type": "single",
    "choices": [
      "DWU percentage",
      "Cache hit percentage",
      "DWU limit",
      "Data Warehouse Units (DWU) used"
    ],
    "site_answers": [
      "Cache hit percentage"
    ]
  },
  {
    "question_text": "You have an Azure subscription that contains an Azure Synapse Analytics workspace and a user named User1.\n\nYou need to ensure that User1 can review the Azure Synapse Analytics database templates from the gallery. The solution must follow the principle of least privilege.\n\nWhich role should you assign to User1?",
    "question_type": "single",
    "choices": [
      "Storage Blob Data Contributor.",
      "Synapse Administrator",
      "Synapse Contributor",
      "Synapse User"
    ],
    "site_answers": [
      "Synapse User"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/synapse-analytics/database-designer/create-lake-database-from-lake-database-templates"
  },
  {
    "question_text": "You have a Log Analytics workspace named la1 and an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 sends logs to la1.\n\nYou need to identify whether a recently executed query on Pool1 used the result set cache.\n\nWhat are two ways to achieve the goal? Each correct answer presents a complete solution.\n\nNOTE: Each correct selection is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Review the sys.dm_pdw_sql_requests dynamic management view in Pool1.",
      "Review the sys.dm_pdw_exec_requests dynamic management view in Pool1.",
      "Use the Monitor hub in Synapse Studio.",
      "Review the AzureDiagnostics table in la1.",
      "Review the sys.dm_pdw_request_steps dynamic management view in Pool1."
    ],
    "site_answers": [
      "Review the sys.dm_pdw_exec_requests dynamic management view in Pool1.",
      "Use the Monitor hub in Synapse Studio."
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Sales.Orders. Sales.Orders contains a column named SalesRep.\n\nYou plan to implement row-level security (RLS) for Sales.\n\nOrders.\n\nYou need to create the security policy that will be used to implement RLS. The solution must ensure that sales representatives only see rows for which the value of the SalesRep column matches their username.\n\nHow should you complete the code? To answer, select the appropriate options in the answer area.",
    "question_type": "multiple",
    "choices": [
      "SCHEMABINDING","ADD FILTER PREDICATE Security.tv_securitypredicate(SalesRep)"
    ],
    "site_answers": [
      "SCHEMABINDING","ADD FILTER PREDICATE Security.tv_securitypredicate(SalesRep)"
    ]
  },
  {
    "question_text": "You have an Azure data factory named DF1. DF1 contains a single pipeline that is executed by using a schedule trigger.\n\nFrom Diagnostics settings, you configure pipeline runs to be sent to a resource-specific destination table in a Log Analytics workspace.\n\nYou need to run KQL queries against the table.\n\nWhich table should you query?",
    "question_type": "single",
    "choices": [
      "ADFPipelineRun",
      "ADFTriggerRun",
      "ADFActivityRun",
      "AzureDiagnostics"
    ],
    "site_answers": [
      "ADFPipelineRun"
    ]
  },
  {
    "question_text": "HOTSPOT -You have an Azure Synapse Analytics dedicated SQL pool named sqlpool1 that contains a table named Sales1.\n\nEach row in the Sales table contains regional sales data and a field that lists the username of a sales analyst.\n\nYou need to configure row-level security (RLS) to ensure that the analysts can view only the rows containing their respective data.\n\nWhat should you do? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nTo configure RLS, create: X\nTo designate which rows each analyst can access, use: Y",
    "question_type": "multiple",
    "choices": [
      "X: A materialized view is sqlpool1","X: A security policy in the Sales table","X: Database scoped credentials in sqlpool1",
      "Y: A masking rule","Y: A table-valued function","Y: The CONTAINS predicate"
    ],
    "site_answers": [
      "X: A security policy in the Sales table","Y: A table-valued function"
    ],
    "links": "https://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16"
  },
  {
    "question_text": "You have an Azure subscription that contains an Azure Synapse workspace named WS1 and an Azure Monitor action group named Group1. WS1 has a dedicated SQL pool.\n\nYou plan to archive monitoring data for integration activity runs.\n\nYou need to ensure that you can configure custom alerts based on the archived data that will execute Group1. The solution must minimize administrative effort.\n\nWhich diagnostic setting should you select?",
    "question_type": "single",
    "choices": [
      "Send to Log Analytics workspace",
      "Archive to a storage account",
      "Stream to an event hub",
      "Send to a partner solution"
    ],
    "site_answers": [
      "Send to Log Analytics workspace"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/sentinel/detect-threats-custom"
  },
  {
    "question_text": "You have an Azure subscription that contains an Azure Synapse Analytics dedicated SQL pool named Pool1.\n\nYou have the queries shown in the following table.\n\nName    Users                              Result set size\nQuery1  Deterministic runtime expressions  25 MB\nQuery2  Deterministic built-in functions   1 GB\nQuery3  User-defined functions (UDFs)      50 MB\nQuery4  Row-level security (RLS)           15 MB\n\nYou are evaluating whether to enable result set caching for Pool1.\n\nWhich query results will be cached if result set caching is enabled?",
    "question_type": "single",
    "choices": [
      "Query1 only",
      "Query2 only",
      "Query1 and Query2 only",
      "Query1 and Query3 only",
      "Query1, Query2, and Query3 only"
    ],
    "site_answers": [
      "Query1 and Query2 only"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-result-set-caching#whats-not-cached"
  },
  {
    "question_text": "You have an Azure subscription that contains an Azure Synapse Analytics workspace name workspace1, workspace1 contains an Azure Synapse Analytics dedicated SQL pool named Pool1.\n\nYou create a mapping data flow in an Azure Synapse pipeline that writes data to Pool1.\n\nYou execute the data flow and capture the execution information.\n\nYou need to identify how long it takes to write the data to Pool1.\n\nWhich metric should you use?",
    "question_type": "single",
    "choices": [
      "the rows written",
      "the sink processing time",
      "the transformation processing time",
      "the post processing time"
    ],
    "site_answers": [
      "the sink processing time"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-monitoring#total-sink-processing-time-vs-transformation-processing-time"
  },
  {
    "question_text": "You have an Azure data factory named DF1. DF1 contains a pipeline that has five activities.\n\nYou need to monitor queue times across the activities by using Log Analytics.\n\nWhat should you do in DF1?",
    "question_type": "single",
    "choices": [
      "Connect DF1 to a Microsoft Purview account.",
      "Add a diagnostic setting that sends activity runs to a Log Analytics workspace.",
      "Enable auto refresh for the Activity Logs Insights workbook.",
      "Add a diagnostic setting that sends pipeline runs to a Log Analytics workspace."
    ],
    "site_answers": [
      "Add a diagnostic setting that sends activity runs to a Log Analytics workspace."
    ]
  },
  {
    "question_text": "You have an Azure subscription that contains an Azure Synapse Analytics dedicated SQL pool named Pool1.\n\nYou need to monitor Pool1. The solution must ensure that you capture the start and end times of each query completed in Pool1.\n\nWhich diagnostic setting should you use?",
    "question_type": "single",
    "choices": [
      "Sql Requests",
      "Request Steps",
      "Dms Workers",
      "Exec Requests"
    ],
    "site_answers": [
      "Sql Requests"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/azure-monitor/reference/supported-logs/microsoft-synapse-workspaces-sqlpools-logs"
  },
  {
    "question_text": "You have an Azure Stream Analytics job named Job1.\n\nThe metrics of Job1 from the last hour are shown in the following table.\n\nMetric                        Time aggregation  Value\nSU (Memory) % Utilization     Average           70\nCPU % Utilization             Average           20\nRuntime Errors                Total             0\nWatermark Delay               Average           20\nInput Deserialization Errors  Total             0\n\nThe late arrival tolerance for Job1 is set to five seconds.\n\nYou need to optimize Job1.\n\nWhich two actions achieve the goal? Each correct answer presents a complete solution.\n\nNOTE: Each correct answer is worth one point.",
    "question_type": "multiple",
    "choices": [
      "Increase the number of SUs.",
      "Parallelize the query.",
      "Resolve errors in output processing.",
      "Resolve errors in input processing."
    ],
    "site_answers": [
      "Increase the number of SUs.",
      "Parallelize the query."
    ]
  },
  {
    "question_text": "Introductory Info\n                                            Case study -This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -Contoso, Ltd. is a clothing retailer based in Seattle. The company has 2,000 retail stores across the United States and an emerging online presence.\n\nThe network contains an Active Directory forest named contoso.com. The forest it integrated with an Azure Active Directory (Azure AD) tenant named contoso.com. Contoso has an Azure subscription associated to the contoso.com Azure AD tenant.\n\nExisting Environment -Transactional Data -Contoso has three years of customer, transactional, operational, sourcing, and supplier data comprised of 10 billion records stored across multiple on-premisesMicrosoft SQL Server servers. The SQL Server instances contain data from various operational systems. The data is loaded into the instances by using SQLServer Integration Services (SSIS) packages.\n\nYou estimate that combining all product sales transactions into a company-wide sales transactions dataset will result in a single table that contains 5 billion rows, with one row per transaction.\n\nMost queries targeting the sales transactions data will be used to identify which products were sold in retail stores and which products were sold online during different time periods. Sales transaction data that is older than three years will be removed monthly.\n\nYou plan to create a retail store table that will contain the address of each retail store. The table will be approximately 2 MB. Queries for retail store sales will include the retail store addresses.\n\nYou plan to create a promotional table that will contain a promotion ID. The promotion ID will be associated to a specific product. The product will be identified by a product ID. The table will be approximately 200 GB.\n\nStreaming Twitter Data -The ecommerce department at Contoso develops an Azure logic app that captures trending Twitter feeds referencing the company's products and pushes the products to Azure Event Hubs.\n\nPlanned Changes and RequirementsPlanned Changes -Contoso plans to implement the following changes: Load the sales transaction dataset to Azure Synapse Analytics. Integrate on-premises data stores with Azure Synapse Analytics by using SSIS packages. Use Azure Synapse Analytics to analyze Twitter feeds to assess customer sentiments about products.\n\nSales Transaction Dataset RequirementsContoso identifies the following requirements for the sales transaction dataset: Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right. Ensure that queries joining and filtering sales transaction records based on product ID complete as quickly as possible. Implement a surrogate key to account for changes to the retail store addresses. Ensure that data storage costs and performance are predictable. Minimize how long it takes to remove old records.\n\nCustomer Sentiment Analytics RequirementsContoso identifies the following requirements for customer sentiment analytics: Allow Contoso users to use PolyBase in an Azure Synapse Analytics dedicated SQL pool to query the content of the data records that host the Twitter feeds.\n\nData must be protected by using row-level security (RLS). The users must be authenticated by using their own Azure AD credentials. Maximize the throughput of ingesting Twitter feeds from Event Hubs to Azure Storage without purchasing additional throughput or capacity units. Store Twitter feeds in Azure Storage by using Event Hubs Capture. The feeds will be converted into Parquet files. Ensure that the data store supports Azure AD-based access control down to the object level.\n\nMinimize administrative effort to maintain the Twitter feed data records. Purge Twitter feed data records that are older than two years.\n\nData Integration Requirements -Contoso identifies the following requirements for data integration: Use an Azure service that leverages the existing SSIS packages to ingest on-premises data into datasets stored in a dedicated SQL pool of Azure SynapseAnalytics and transform the data. Identify a process to ensure that changes to the ingestion and transformation activities can be version-controlled and developed independently by multiple data engineers.\n                                            Question\n                                        \n                                        \n                                            HOTSPOT -You need to design a data storage structure for the product sales transactions. The solution must meet the sales transaction dataset requirements.\n\nWhat should you include in the solution? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:",
    "question_type": "multiple",
    "choices": [
      "X: Hash","X: Round-robin","X: Replicated",
      "Y: Configure a clustered index.","Y: Set the distribution column to product ID.","Y: Set the distribution column to the sales date."
    ],
    "site_answers": [
      "X: Hash","Y: Set the distribution column to product ID."
    ]
  },
  {
    "question_text": "Introductory Info\n                                            Case study -This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -Contoso, Ltd. is a clothing retailer based in Seattle. The company has 2,000 retail stores across the United States and an emerging online presence.\n\nThe network contains an Active Directory forest named contoso.com. The forest it integrated with an Azure Active Directory (Azure AD) tenant named contoso.com. Contoso has an Azure subscription associated to the contoso.com Azure AD tenant.\n\nExisting Environment -Transactional Data -Contoso has three years of customer, transactional, operational, sourcing, and supplier data comprised of 10 billion records stored across multiple on-premisesMicrosoft SQL Server servers. The SQL Server instances contain data from various operational systems. The data is loaded into the instances by using SQLServer Integration Services (SSIS) packages.\n\nYou estimate that combining all product sales transactions into a company-wide sales transactions dataset will result in a single table that contains 5 billion rows, with one row per transaction.\n\nMost queries targeting the sales transactions data will be used to identify which products were sold in retail stores and which products were sold online during different time periods. Sales transaction data that is older than three years will be removed monthly.\n\nYou plan to create a retail store table that will contain the address of each retail store. The table will be approximately 2 MB. Queries for retail store sales will include the retail store addresses.\n\nYou plan to create a promotional table that will contain a promotion ID. The promotion ID will be associated to a specific product. The product will be identified by a product ID. The table will be approximately 200 GB.\n\nStreaming Twitter Data -The ecommerce department at Contoso develops an Azure logic app that captures trending Twitter feeds referencing the company's products and pushes the products to Azure Event Hubs.\n\nPlanned Changes and RequirementsPlanned Changes -Contoso plans to implement the following changes: Load the sales transaction dataset to Azure Synapse Analytics. Integrate on-premises data stores with Azure Synapse Analytics by using SSIS packages. Use Azure Synapse Analytics to analyze Twitter feeds to assess customer sentiments about products.\n\nSales Transaction Dataset RequirementsContoso identifies the following requirements for the sales transaction dataset: Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right. Ensure that queries joining and filtering sales transaction records based on product ID complete as quickly as possible. Implement a surrogate key to account for changes to the retail store addresses. Ensure that data storage costs and performance are predictable. Minimize how long it takes to remove old records.\n\nCustomer Sentiment Analytics RequirementsContoso identifies the following requirements for customer sentiment analytics: Allow Contoso users to use PolyBase in an Azure Synapse Analytics dedicated SQL pool to query the content of the data records that host the Twitter feeds.\n\nData must be protected by using row-level security (RLS). The users must be authenticated by using their own Azure AD credentials. Maximize the throughput of ingesting Twitter feeds from Event Hubs to Azure Storage without purchasing additional throughput or capacity units. Store Twitter feeds in Azure Storage by using Event Hubs Capture. The feeds will be converted into Parquet files. Ensure that the data store supports Azure AD-based access control down to the object level.\n\nMinimize administrative effort to maintain the Twitter feed data records. Purge Twitter feed data records that are older than two years.\n\nData Integration Requirements -Contoso identifies the following requirements for data integration: Use an Azure service that leverages the existing SSIS packages to ingest on-premises data into datasets stored in a dedicated SQL pool of Azure SynapseAnalytics and transform the data. Identify a process to ensure that changes to the ingestion and transformation activities can be version-controlled and developed independently by multiple data engineers.\n                                            Question\n                                        \n                                        \n                                            DRAG DROP -You need to ensure that the Twitter feed data can be analyzed in the dedicated SQL pool. The solution must meet the customer sentiment analytics requirements.\n\nWhich three Transact-SQL DDL commands should you run in sequence? To answer, move the appropriate commands from the list of commands to the answer area and arrange them in the correct order.\n\nNOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.\n\nSelect and Place:",
    "question_type": "multiple",
    "choices": [
      "CREATE EXTERNAL DATA SOURCE",
      "CREATE EXTERNAL FILE FORMAT",
      "CREATE EXTERNAL TABLE",
      "CREATE EXTERNAL TABLE AS SELECT",
      "CREATE DATABASE SCOPED CREDENTIAL"
    ],
    "site_answers": [
      "CREATE EXTERNAL DATA SOURCE",
      "CREATE EXTERNAL FILE FORMAT",
      "CREATE EXTERNAL TABLE AS SELECT",
    ],
    "links": ["https://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/2-transform-data-using-create-external-table-select-statement","https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables","https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql?view=aps-pdw-2016-au7","https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-versioned-feature-summary?view=sql-server-ver16"]
  },
  {
    "question_text": "Introductory Info\n                                            Case study -This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -Contoso, Ltd. is a clothing retailer based in Seattle. The company has 2,000 retail stores across the United States and an emerging online presence.\n\nThe network contains an Active Directory forest named contoso.com. The forest it integrated with an Azure Active Directory (Azure AD) tenant named contoso.com. Contoso has an Azure subscription associated to the contoso.com Azure AD tenant.\n\nExisting Environment -Transactional Data -Contoso has three years of customer, transactional, operational, sourcing, and supplier data comprised of 10 billion records stored across multiple on-premisesMicrosoft SQL Server servers. The SQL Server instances contain data from various operational systems. The data is loaded into the instances by using SQLServer Integration Services (SSIS) packages.\n\nYou estimate that combining all product sales transactions into a company-wide sales transactions dataset will result in a single table that contains 5 billion rows, with one row per transaction.\n\nMost queries targeting the sales transactions data will be used to identify which products were sold in retail stores and which products were sold online during different time periods. Sales transaction data that is older than three years will be removed monthly.\n\nYou plan to create a retail store table that will contain the address of each retail store. The table will be approximately 2 MB. Queries for retail store sales will include the retail store addresses.\n\nYou plan to create a promotional table that will contain a promotion ID. The promotion ID will be associated to a specific product. The product will be identified by a product ID. The table will be approximately 200 GB.\n\nStreaming Twitter Data -The ecommerce department at Contoso develops an Azure logic app that captures trending Twitter feeds referencing the company's products and pushes the products to Azure Event Hubs.\n\nPlanned Changes and RequirementsPlanned Changes -Contoso plans to implement the following changes: Load the sales transaction dataset to Azure Synapse Analytics. Integrate on-premises data stores with Azure Synapse Analytics by using SSIS packages. Use Azure Synapse Analytics to analyze Twitter feeds to assess customer sentiments about products.\n\nSales Transaction Dataset RequirementsContoso identifies the following requirements for the sales transaction dataset: Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right. Ensure that queries joining and filtering sales transaction records based on product ID complete as quickly as possible. Implement a surrogate key to account for changes to the retail store addresses. Ensure that data storage costs and performance are predictable. Minimize how long it takes to remove old records.\n\nCustomer Sentiment Analytics RequirementsContoso identifies the following requirements for customer sentiment analytics: Allow Contoso users to use PolyBase in an Azure Synapse Analytics dedicated SQL pool to query the content of the data records that host the Twitter feeds.\n\nData must be protected by using row-level security (RLS). The users must be authenticated by using their own Azure AD credentials. Maximize the throughput of ingesting Twitter feeds from Event Hubs to Azure Storage without purchasing additional throughput or capacity units. Store Twitter feeds in Azure Storage by using Event Hubs Capture. The feeds will be converted into Parquet files. Ensure that the data store supports Azure AD-based access control down to the object level.\n\nMinimize administrative effort to maintain the Twitter feed data records. Purge Twitter feed data records that are older than two years.\n\nData Integration Requirements -Contoso identifies the following requirements for data integration: Use an Azure service that leverages the existing SSIS packages to ingest on-premises data into datasets stored in a dedicated SQL pool of Azure SynapseAnalytics and transform the data. Identify a process to ensure that changes to the ingestion and transformation activities can be version-controlled and developed independently by multiple data engineers.\n                                            Question\n                                        \n                                        \n                                            HOTSPOT -You need to design the partitions for the product sales transactions. The solution must meet the sales transaction dataset requirements.\n\nWhat should you include in the solution? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nPartition product sales transactions data by: X\nStore product sales transactions data in: Y",
    "question_type": "multiple",
    "choices": [
      "X: Sales date","X: Product ID","X: Promotion ID",
      "Y: An Azure Synapse Analytics dedicated SQL pool","Y: An Azure Synapse Analytics serverless SQL pool","Y: An Azure Data Lake Storage Gen2 account linked","Y: to an Azure Synapse Analytics workspace"
    ],
    "site_answers": [
      "X: Sales date",
    ]
  },
  {
    "question_text": "Introductory Info\n                                            Case study -This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -Contoso, Ltd. is a clothing retailer based in Seattle. The company has 2,000 retail stores across the United States and an emerging online presence.\n\nThe network contains an Active Directory forest named contoso.com. The forest it integrated with an Azure Active Directory (Azure AD) tenant named contoso.com. Contoso has an Azure subscription associated to the contoso.com Azure AD tenant.\n\nExisting Environment -Transactional Data -Contoso has three years of customer, transactional, operational, sourcing, and supplier data comprised of 10 billion records stored across multiple on-premisesMicrosoft SQL Server servers. The SQL Server instances contain data from various operational systems. The data is loaded into the instances by using SQLServer Integration Services (SSIS) packages.\n\nYou estimate that combining all product sales transactions into a company-wide sales transactions dataset will result in a single table that contains 5 billion rows, with one row per transaction.\n\nMost queries targeting the sales transactions data will be used to identify which products were sold in retail stores and which products were sold online during different time periods. Sales transaction data that is older than three years will be removed monthly.\n\nYou plan to create a retail store table that will contain the address of each retail store. The table will be approximately 2 MB. Queries for retail store sales will include the retail store addresses.\n\nYou plan to create a promotional table that will contain a promotion ID. The promotion ID will be associated to a specific product. The product will be identified by a product ID. The table will be approximately 200 GB.\n\nStreaming Twitter Data -The ecommerce department at Contoso develops an Azure logic app that captures trending Twitter feeds referencing the company's products and pushes the products to Azure Event Hubs.\n\nPlanned Changes and RequirementsPlanned Changes -Contoso plans to implement the following changes: Load the sales transaction dataset to Azure Synapse Analytics. Integrate on-premises data stores with Azure Synapse Analytics by using SSIS packages. Use Azure Synapse Analytics to analyze Twitter feeds to assess customer sentiments about products.\n\nSales Transaction Dataset RequirementsContoso identifies the following requirements for the sales transaction dataset: Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right. Ensure that queries joining and filtering sales transaction records based on product ID complete as quickly as possible. Implement a surrogate key to account for changes to the retail store addresses. Ensure that data storage costs and performance are predictable. Minimize how long it takes to remove old records.\n\nCustomer Sentiment Analytics RequirementsContoso identifies the following requirements for customer sentiment analytics: Allow Contoso users to use PolyBase in an Azure Synapse Analytics dedicated SQL pool to query the content of the data records that host the Twitter feeds.\n\nData must be protected by using row-level security (RLS). The users must be authenticated by using their own Azure AD credentials. Maximize the throughput of ingesting Twitter feeds from Event Hubs to Azure Storage without purchasing additional throughput or capacity units. Store Twitter feeds in Azure Storage by using Event Hubs Capture. The feeds will be converted into Parquet files. Ensure that the data store supports Azure AD-based access control down to the object level.\n\nMinimize administrative effort to maintain the Twitter feed data records. Purge Twitter feed data records that are older than two years.\n\nData Integration Requirements -Contoso identifies the following requirements for data integration: Use an Azure service that leverages the existing SSIS packages to ingest on-premises data into datasets stored in a dedicated SQL pool of Azure SynapseAnalytics and transform the data. Identify a process to ensure that changes to the ingestion and transformation activities can be version-controlled and developed independently by multiple data engineers.\n                                            Question\n                                        \n                                        \n                                            You need to implement the surrogate key for the retail store table. The solution must meet the sales transaction dataset requirements.\n\nWhat should you create?",
    "question_type": "single",
    "choices": [
      "a table that has an IDENTITY property",
      "a system-versioned temporal table",
      "a user-defined SEQUENCE object",
      "a table that has a FOREIGN KEY constraint"
    ],
    "site_answers": [
      "a table that has an IDENTITY property"
    ]
  },
  {
    "question_text": "Introductory Info\n                                            Case study -This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -Contoso, Ltd. is a clothing retailer based in Seattle. The company has 2,000 retail stores across the United States and an emerging online presence.\n\nThe network contains an Active Directory forest named contoso.com. The forest it integrated with an Azure Active Directory (Azure AD) tenant named contoso.com. Contoso has an Azure subscription associated to the contoso.com Azure AD tenant.\n\nExisting Environment -Transactional Data -Contoso has three years of customer, transactional, operational, sourcing, and supplier data comprised of 10 billion records stored across multiple on-premisesMicrosoft SQL Server servers. The SQL Server instances contain data from various operational systems. The data is loaded into the instances by using SQLServer Integration Services (SSIS) packages.\n\nYou estimate that combining all product sales transactions into a company-wide sales transactions dataset will result in a single table that contains 5 billion rows, with one row per transaction.\n\nMost queries targeting the sales transactions data will be used to identify which products were sold in retail stores and which products were sold online during different time periods. Sales transaction data that is older than three years will be removed monthly.\n\nYou plan to create a retail store table that will contain the address of each retail store. The table will be approximately 2 MB. Queries for retail store sales will include the retail store addresses.\n\nYou plan to create a promotional table that will contain a promotion ID. The promotion ID will be associated to a specific product. The product will be identified by a product ID. The table will be approximately 200 GB.\n\nStreaming Twitter Data -The ecommerce department at Contoso develops an Azure logic app that captures trending Twitter feeds referencing the company's products and pushes the products to Azure Event Hubs.\n\nPlanned Changes and RequirementsPlanned Changes -Contoso plans to implement the following changes: Load the sales transaction dataset to Azure Synapse Analytics. Integrate on-premises data stores with Azure Synapse Analytics by using SSIS packages. Use Azure Synapse Analytics to analyze Twitter feeds to assess customer sentiments about products.\n\nSales Transaction Dataset RequirementsContoso identifies the following requirements for the sales transaction dataset: Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right. Ensure that queries joining and filtering sales transaction records based on product ID complete as quickly as possible. Implement a surrogate key to account for changes to the retail store addresses. Ensure that data storage costs and performance are predictable. Minimize how long it takes to remove old records.\n\nCustomer Sentiment Analytics RequirementsContoso identifies the following requirements for customer sentiment analytics: Allow Contoso users to use PolyBase in an Azure Synapse Analytics dedicated SQL pool to query the content of the data records that host the Twitter feeds.\n\nData must be protected by using row-level security (RLS). The users must be authenticated by using their own Azure AD credentials. Maximize the throughput of ingesting Twitter feeds from Event Hubs to Azure Storage without purchasing additional throughput or capacity units. Store Twitter feeds in Azure Storage by using Event Hubs Capture. The feeds will be converted into Parquet files. Ensure that the data store supports Azure AD-based access control down to the object level.\n\nMinimize administrative effort to maintain the Twitter feed data records. Purge Twitter feed data records that are older than two years.\n\nData Integration Requirements -Contoso identifies the following requirements for data integration: Use an Azure service that leverages the existing SSIS packages to ingest on-premises data into datasets stored in a dedicated SQL pool of Azure SynapseAnalytics and transform the data. Identify a process to ensure that changes to the ingestion and transformation activities can be version-controlled and developed independently by multiple data engineers.\n                                            Question\n                                        \n                                        \n                                            HOTSPOT -You need to design an analytical storage solution for the transactional data. The solution must meet the sales transaction dataset requirements.\n\nWhat should you include in the solution? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nTable type to store retail store data: X\nTable type to store promotional data: Y",
    "question_type": "multiple",
    "choices": [
      "X: Hash","X: Replicated","X: Round-robin",
      "Y: Hash","Y: Replicated","Y: Round-robin"
    ],
    "site_answers": [
      "X: Replicated","Y: Hash"
    ]
  },
  {
    "question_text": "Introductory Info\n                                            Case study -This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -Contoso, Ltd. is a clothing retailer based in Seattle. The company has 2,000 retail stores across the United States and an emerging online presence.\n\nThe network contains an Active Directory forest named contoso.com. The forest it integrated with an Azure Active Directory (Azure AD) tenant named contoso.com. Contoso has an Azure subscription associated to the contoso.com Azure AD tenant.\n\nExisting Environment -Transactional Data -Contoso has three years of customer, transactional, operational, sourcing, and supplier data comprised of 10 billion records stored across multiple on-premisesMicrosoft SQL Server servers. The SQL Server instances contain data from various operational systems. The data is loaded into the instances by using SQLServer Integration Services (SSIS) packages.\n\nYou estimate that combining all product sales transactions into a company-wide sales transactions dataset will result in a single table that contains 5 billion rows, with one row per transaction.\n\nMost queries targeting the sales transactions data will be used to identify which products were sold in retail stores and which products were sold online during different time periods. Sales transaction data that is older than three years will be removed monthly.\n\nYou plan to create a retail store table that will contain the address of each retail store. The table will be approximately 2 MB. Queries for retail store sales will include the retail store addresses.\n\nYou plan to create a promotional table that will contain a promotion ID. The promotion ID will be associated to a specific product. The product will be identified by a product ID. The table will be approximately 200 GB.\n\nStreaming Twitter Data -The ecommerce department at Contoso develops an Azure logic app that captures trending Twitter feeds referencing the company's products and pushes the products to Azure Event Hubs.\n\nPlanned Changes and RequirementsPlanned Changes -Contoso plans to implement the following changes: Load the sales transaction dataset to Azure Synapse Analytics. Integrate on-premises data stores with Azure Synapse Analytics by using SSIS packages. Use Azure Synapse Analytics to analyze Twitter feeds to assess customer sentiments about products.\n\nSales Transaction Dataset RequirementsContoso identifies the following requirements for the sales transaction dataset: Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right. Ensure that queries joining and filtering sales transaction records based on product ID complete as quickly as possible. Implement a surrogate key to account for changes to the retail store addresses. Ensure that data storage costs and performance are predictable. Minimize how long it takes to remove old records.\n\nCustomer Sentiment Analytics RequirementsContoso identifies the following requirements for customer sentiment analytics: Allow Contoso users to use PolyBase in an Azure Synapse Analytics dedicated SQL pool to query the content of the data records that host the Twitter feeds.\n\nData must be protected by using row-level security (RLS). The users must be authenticated by using their own Azure AD credentials. Maximize the throughput of ingesting Twitter feeds from Event Hubs to Azure Storage without purchasing additional throughput or capacity units. Store Twitter feeds in Azure Storage by using Event Hubs Capture. The feeds will be converted into Parquet files. Ensure that the data store supports Azure AD-based access control down to the object level.\n\nMinimize administrative effort to maintain the Twitter feed data records. Purge Twitter feed data records that are older than two years.\n\nData Integration Requirements -Contoso identifies the following requirements for data integration: Use an Azure service that leverages the existing SSIS packages to ingest on-premises data into datasets stored in a dedicated SQL pool of Azure SynapseAnalytics and transform the data. Identify a process to ensure that changes to the ingestion and transformation activities can be version-controlled and developed independently by multiple data engineers.\n                                            Question\n                                        \n                                        \n                                            HOTSPOT -You need to implement an Azure Synapse Analytics database object for storing the sales transactions data. The solution must meet the sales transaction dataset requirements.\n\nWhat should you do? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nTransact-SQL DDL command to use: X\nPartitioning option to use in the WITH clause of the DDL statement: Y",
    "question_type": "multiple",
    "choices": [
      "X: CREATE EXTERNAL TABLE","X: CREATE TABLE","X: CREATE VIEW",
      "Y: FORMAT_OPTIONS","Y: FORMAT_TYPE","Y: RANGE LEFT FOR VALUES","Y: RANGE RIGHT FOR VALUES"
    ],
    "site_answers": [
      "X: CREATE TABLE","Y: RANGE RIGHT FOR VALUES"
    ]
  },
  {
    "question_text": "Introductory Info\n                                            Case study -This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -Contoso, Ltd. is a clothing retailer based in Seattle. The company has 2,000 retail stores across the United States and an emerging online presence.\n\nThe network contains an Active Directory forest named contoso.com. The forest it integrated with an Azure Active Directory (Azure AD) tenant named contoso.com. Contoso has an Azure subscription associated to the contoso.com Azure AD tenant.\n\nExisting Environment -Transactional Data -Contoso has three years of customer, transactional, operational, sourcing, and supplier data comprised of 10 billion records stored across multiple on-premisesMicrosoft SQL Server servers. The SQL Server instances contain data from various operational systems. The data is loaded into the instances by using SQLServer Integration Services (SSIS) packages.\n\nYou estimate that combining all product sales transactions into a company-wide sales transactions dataset will result in a single table that contains 5 billion rows, with one row per transaction.\n\nMost queries targeting the sales transactions data will be used to identify which products were sold in retail stores and which products were sold online during different time periods. Sales transaction data that is older than three years will be removed monthly.\n\nYou plan to create a retail store table that will contain the address of each retail store. The table will be approximately 2 MB. Queries for retail store sales will include the retail store addresses.\n\nYou plan to create a promotional table that will contain a promotion ID. The promotion ID will be associated to a specific product. The product will be identified by a product ID. The table will be approximately 200 GB.\n\nStreaming Twitter Data -The ecommerce department at Contoso develops an Azure logic app that captures trending Twitter feeds referencing the company's products and pushes the products to Azure Event Hubs.\n\nPlanned Changes and RequirementsPlanned Changes -Contoso plans to implement the following changes: Load the sales transaction dataset to Azure Synapse Analytics. Integrate on-premises data stores with Azure Synapse Analytics by using SSIS packages. Use Azure Synapse Analytics to analyze Twitter feeds to assess customer sentiments about products.\n\nSales Transaction Dataset RequirementsContoso identifies the following requirements for the sales transaction dataset: Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right. Ensure that queries joining and filtering sales transaction records based on product ID complete as quickly as possible. Implement a surrogate key to account for changes to the retail store addresses. Ensure that data storage costs and performance are predictable. Minimize how long it takes to remove old records.\n\nCustomer Sentiment Analytics RequirementsContoso identifies the following requirements for customer sentiment analytics: Allow Contoso users to use PolyBase in an Azure Synapse Analytics dedicated SQL pool to query the content of the data records that host the Twitter feeds.\n\nData must be protected by using row-level security (RLS). The users must be authenticated by using their own Azure AD credentials. Maximize the throughput of ingesting Twitter feeds from Event Hubs to Azure Storage without purchasing additional throughput or capacity units. Store Twitter feeds in Azure Storage by using Event Hubs Capture. The feeds will be converted into Parquet files. Ensure that the data store supports Azure AD-based access control down to the object level.\n\nMinimize administrative effort to maintain the Twitter feed data records. Purge Twitter feed data records that are older than two years.\n\nData Integration Requirements -Contoso identifies the following requirements for data integration: Use an Azure service that leverages the existing SSIS packages to ingest on-premises data into datasets stored in a dedicated SQL pool of Azure SynapseAnalytics and transform the data. Identify a process to ensure that changes to the ingestion and transformation activities can be version-controlled and developed independently by multiple data engineers.\n                                            Question\n                                        \n                                        \n                                            You need to design a data retention solution for the Twitter feed data records. The solution must meet the customer sentiment analytics requirements.\n\nWhich Azure Storage functionality should you include in the solution?",
    "question_type": "single",
    "choices": [
      "change feed",
      "soft delete",
      "time-based retention",
      "lifecycle management"
    ],
    "site_answers": [
      "lifecycle management"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/storage/blobs/immutable-time-based-retention-policy-overview"
  },
  {
    "question_text": "Introductory Info\n                                            Case study -This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -Contoso, Ltd. is a clothing retailer based in Seattle. The company has 2,000 retail stores across the United States and an emerging online presence.\n\nThe network contains an Active Directory forest named contoso.com. The forest it integrated with an Azure Active Directory (Azure AD) tenant named contoso.com. Contoso has an Azure subscription associated to the contoso.com Azure AD tenant.\n\nExisting Environment -Transactional Data -Contoso has three years of customer, transactional, operational, sourcing, and supplier data comprised of 10 billion records stored across multiple on-premisesMicrosoft SQL Server servers. The SQL Server instances contain data from various operational systems. The data is loaded into the instances by using SQLServer Integration Services (SSIS) packages.\n\nYou estimate that combining all product sales transactions into a company-wide sales transactions dataset will result in a single table that contains 5 billion rows, with one row per transaction.\n\nMost queries targeting the sales transactions data will be used to identify which products were sold in retail stores and which products were sold online during different time periods. Sales transaction data that is older than three years will be removed monthly.\n\nYou plan to create a retail store table that will contain the address of each retail store. The table will be approximately 2 MB. Queries for retail store sales will include the retail store addresses.\n\nYou plan to create a promotional table that will contain a promotion ID. The promotion ID will be associated to a specific product. The product will be identified by a product ID. The table will be approximately 200 GB.\n\nStreaming Twitter Data -The ecommerce department at Contoso develops an Azure logic app that captures trending Twitter feeds referencing the company's products and pushes the products to Azure Event Hubs.\n\nPlanned Changes and RequirementsPlanned Changes -Contoso plans to implement the following changes: Load the sales transaction dataset to Azure Synapse Analytics. Integrate on-premises data stores with Azure Synapse Analytics by using SSIS packages. Use Azure Synapse Analytics to analyze Twitter feeds to assess customer sentiments about products.\n\nSales Transaction Dataset RequirementsContoso identifies the following requirements for the sales transaction dataset: Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right. Ensure that queries joining and filtering sales transaction records based on product ID complete as quickly as possible. Implement a surrogate key to account for changes to the retail store addresses. Ensure that data storage costs and performance are predictable. Minimize how long it takes to remove old records.\n\nCustomer Sentiment Analytics RequirementsContoso identifies the following requirements for customer sentiment analytics: Allow Contoso users to use PolyBase in an Azure Synapse Analytics dedicated SQL pool to query the content of the data records that host the Twitter feeds.\n\nData must be protected by using row-level security (RLS). The users must be authenticated by using their own Azure AD credentials. Maximize the throughput of ingesting Twitter feeds from Event Hubs to Azure Storage without purchasing additional throughput or capacity units. Store Twitter feeds in Azure Storage by using Event Hubs Capture. The feeds will be converted into Parquet files. Ensure that the data store supports Azure AD-based access control down to the object level.\n\nMinimize administrative effort to maintain the Twitter feed data records. Purge Twitter feed data records that are older than two years.\n\nData Integration Requirements -Contoso identifies the following requirements for data integration: Use an Azure service that leverages the existing SSIS packages to ingest on-premises data into datasets stored in a dedicated SQL pool of Azure SynapseAnalytics and transform the data. Identify a process to ensure that changes to the ingestion and transformation activities can be version-controlled and developed independently by multiple data engineers.\n                                            Question\n                                        \n                                        \n                                            DRAG DROP -You need to implement versioned changes to the integration pipelines. The solution must meet the data integration requirements.\n\nIn which order should you perform the actions? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\n\nSelect and Place:",
    "question_type": "multiple",
    "choices": [
      "Merge changes","Create a pull request","Create a feature branch",
      "Publish changes","Create a repository and a main branch"
    ],
    "site_answers": [
      "Create a repository and a main branch","Create a feature branch",
      "Create a pull request","Merge changes","Publish changes"
    ]
  },
  {
    "question_text": "Introductory Info\n                                            Case study -This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -Contoso, Ltd. is a clothing retailer based in Seattle. The company has 2,000 retail stores across the United States and an emerging online presence.\n\nThe network contains an Active Directory forest named contoso.com. The forest it integrated with an Azure Active Directory (Azure AD) tenant named contoso.com. Contoso has an Azure subscription associated to the contoso.com Azure AD tenant.\n\nExisting Environment -Transactional Data -Contoso has three years of customer, transactional, operational, sourcing, and supplier data comprised of 10 billion records stored across multiple on-premisesMicrosoft SQL Server servers. The SQL Server instances contain data from various operational systems. The data is loaded into the instances by using SQLServer Integration Services (SSIS) packages.\n\nYou estimate that combining all product sales transactions into a company-wide sales transactions dataset will result in a single table that contains 5 billion rows, with one row per transaction.\n\nMost queries targeting the sales transactions data will be used to identify which products were sold in retail stores and which products were sold online during different time periods. Sales transaction data that is older than three years will be removed monthly.\n\nYou plan to create a retail store table that will contain the address of each retail store. The table will be approximately 2 MB. Queries for retail store sales will include the retail store addresses.\n\nYou plan to create a promotional table that will contain a promotion ID. The promotion ID will be associated to a specific product. The product will be identified by a product ID. The table will be approximately 200 GB.\n\nStreaming Twitter Data -The ecommerce department at Contoso develops an Azure logic app that captures trending Twitter feeds referencing the company's products and pushes the products to Azure Event Hubs.\n\nPlanned Changes and RequirementsPlanned Changes -Contoso plans to implement the following changes: Load the sales transaction dataset to Azure Synapse Analytics. Integrate on-premises data stores with Azure Synapse Analytics by using SSIS packages. Use Azure Synapse Analytics to analyze Twitter feeds to assess customer sentiments about products.\n\nSales Transaction Dataset RequirementsContoso identifies the following requirements for the sales transaction dataset: Partition data that contains sales transaction records. Partitions must be designed to provide efficient loads by month. Boundary values must belong to the partition on the right. Ensure that queries joining and filtering sales transaction records based on product ID complete as quickly as possible. Implement a surrogate key to account for changes to the retail store addresses. Ensure that data storage costs and performance are predictable. Minimize how long it takes to remove old records.\n\nCustomer Sentiment Analytics RequirementsContoso identifies the following requirements for customer sentiment analytics: Allow Contoso users to use PolyBase in an Azure Synapse Analytics dedicated SQL pool to query the content of the data records that host the Twitter feeds.\n\nData must be protected by using row-level security (RLS). The users must be authenticated by using their own Azure AD credentials. Maximize the throughput of ingesting Twitter feeds from Event Hubs to Azure Storage without purchasing additional throughput or capacity units. Store Twitter feeds in Azure Storage by using Event Hubs Capture. The feeds will be converted into Parquet files. Ensure that the data store supports Azure AD-based access control down to the object level.\n\nMinimize administrative effort to maintain the Twitter feed data records. Purge Twitter feed data records that are older than two years.\n\nData Integration Requirements -Contoso identifies the following requirements for data integration: Use an Azure service that leverages the existing SSIS packages to ingest on-premises data into datasets stored in a dedicated SQL pool of Azure SynapseAnalytics and transform the data. Identify a process to ensure that changes to the ingestion and transformation activities can be version-controlled and developed independently by multiple data engineers.\n                                            Question\n                                        \n                                        \n                                            HOTSPOT -You need to design a data ingestion and storage solution for the Twitter feeds. The solution must meet the customer sentiment analytics requirements.\n\nWhat should you include in the solution? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:\n\nTo increase the throughput of ingesting the Twitter feeds: X\nTo store the Twitter feed data, use: Y",
    "question_type": "multiple",
    "choices": [
      "X: Configure Event Hubs partitions.","X: Enable Auto-Inflate in Event Hubs.","X: Use Event Hubs Dedicated.",
      "Y: An Azure Data Lake Storage Gen2 account","Y: An Azure Databricks high concurrency cluster","Y: An Azure General-purpose v2 storage account in the Premium tier"
    ],
    "site_answers": [
      "X: Configure Event Hubs partitions.","Y: An Azure Data Lake Storage Gen2 account"
    ],
    "links": "https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-dedicated-overview#supports-streaming-large-messages"
  },
  {
    "question_text": "Introductory Info\n                                            Case study -This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -Litware, Inc. owns and operates 300 convenience stores across the US. The company sells a variety of packaged foods and drinks, as well as a variety of prepared foods, such as sandwiches and pizzas.\n\nLitware has a loyalty club whereby members can get daily discounts on specific items by providing their membership number at checkout.\n\nLitware employs business analysts who prefer to analyze data by using Microsoft Power BI, and data scientists who prefer analyzing data in Azure Databricks notebooks.\n\nRequirements -Business Goals -Litware wants to create a new analytics environment in Azure to meet the following requirements: See inventory levels across the stores. Data must be updated as close to real time as possible. Execute ad hoc analytical queries on historical data to identify whether the loyalty club discounts increase sales of the discounted products. Every four hours, notify store employees about how many prepared food items to produce based on historical demand from the sales data.\n\nTechnical Requirements -Litware identifies the following technical requirements: Minimize the number of different Azure services needed to achieve the business goals. Use platform as a service (PaaS) offerings whenever possible and avoid having to provision virtual machines that must be managed by Litware. Ensure that the analytical data store is accessible only to the company's on-premises network and Azure services. Use Azure Active Directory (Azure AD) authentication whenever possible. Use the principle of least privilege when designing security. Stage Inventory data in Azure Data Lake Storage Gen2 before loading the data into the analytical data store. Litware wants to remove transient data from DataLake Storage once the data is no longer in use. Files that have a modified date that is older than 14 days must be removed. Limit the business analysts' access to customer contact information, such as phone numbers, because this type of data is not analytically relevant. Ensure that you can quickly restore a copy of the analytical data store within one hour in the event of corruption or accidental deletion.\n\nPlanned Environment -Litware plans to implement the following environment: The application development team will create an Azure event hub to receive real-time sales data, including store number, date, time, product ID, customer loyalty number, price, and discount amount, from the point of sale (POS) system and output the data to data storage in Azure. Customer data, including name, contact information, and loyalty number, comes from Salesforce, a SaaS application, and can be imported into Azure once every eight hours. Row modified dates are not trusted in the source table. Product data, including product ID, name, and category, comes from Salesforce and can be imported into Azure once every eight hours. Row modified dates are not trusted in the source table. Daily inventory data comes from a Microsoft SQL server located on a private network. Litware currently has 5 TB of historical sales data and 100 GB of customer data. The company expects approximately 100 GB of new data per month for the next year. Litware will build a custom application named FoodPrep to provide store employees with the calculation results of how many prepared food items to produce every four hours. Litware does not plan to implement Azure ExpressRoute or a VPN between the on-premises network and Azure.\n                                            Question\n                                        \n                                        \n                                            What should you recommend to prevent users outside the Litware on-premises network from accessing the analytical data store?",
    "question_type": "single",
    "choices": [
      "a server-level virtual network rule",
      "a database-level virtual network rule",
      "a server-level firewall IP rule",
      "a database-level firewall IP rule"
    ],
    "site_answers": [
      "a server-level firewall IP rule"
    ],
    "links": "https://docs.microsoft.com/en-us/azure/azure-sql/database/network-access-controls-overview"
  },
  {
    "question_text": "Introductory Info\n                                            Case study -This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -Litware, Inc. owns and operates 300 convenience stores across the US. The company sells a variety of packaged foods and drinks, as well as a variety of prepared foods, such as sandwiches and pizzas.\n\nLitware has a loyalty club whereby members can get daily discounts on specific items by providing their membership number at checkout.\n\nLitware employs business analysts who prefer to analyze data by using Microsoft Power BI, and data scientists who prefer analyzing data in Azure Databricks notebooks.\n\nRequirements -Business Goals -Litware wants to create a new analytics environment in Azure to meet the following requirements: See inventory levels across the stores. Data must be updated as close to real time as possible. Execute ad hoc analytical queries on historical data to identify whether the loyalty club discounts increase sales of the discounted products. Every four hours, notify store employees about how many prepared food items to produce based on historical demand from the sales data.\n\nTechnical Requirements -Litware identifies the following technical requirements: Minimize the number of different Azure services needed to achieve the business goals. Use platform as a service (PaaS) offerings whenever possible and avoid having to provision virtual machines that must be managed by Litware. Ensure that the analytical data store is accessible only to the company's on-premises network and Azure services. Use Azure Active Directory (Azure AD) authentication whenever possible. Use the principle of least privilege when designing security. Stage Inventory data in Azure Data Lake Storage Gen2 before loading the data into the analytical data store. Litware wants to remove transient data from DataLake Storage once the data is no longer in use. Files that have a modified date that is older than 14 days must be removed. Limit the business analysts' access to customer contact information, such as phone numbers, because this type of data is not analytically relevant. Ensure that you can quickly restore a copy of the analytical data store within one hour in the event of corruption or accidental deletion.\n\nPlanned Environment -Litware plans to implement the following environment: The application development team will create an Azure event hub to receive real-time sales data, including store number, date, time, product ID, customer loyalty number, price, and discount amount, from the point of sale (POS) system and output the data to data storage in Azure. Customer data, including name, contact information, and loyalty number, comes from Salesforce, a SaaS application, and can be imported into Azure once every eight hours. Row modified dates are not trusted in the source table. Product data, including product ID, name, and category, comes from Salesforce and can be imported into Azure once every eight hours. Row modified dates are not trusted in the source table. Daily inventory data comes from a Microsoft SQL server located on a private network. Litware currently has 5 TB of historical sales data and 100 GB of customer data. The company expects approximately 100 GB of new data per month for the next year. Litware will build a custom application named FoodPrep to provide store employees with the calculation results of how many prepared food items to produce every four hours. Litware does not plan to implement Azure ExpressRoute or a VPN between the on-premises network and Azure.\n                                            Question\n                                        \n                                        \n                                            What should you recommend using to secure sensitive customer contact information?",
    "question_type": "single",
    "choices": [
      "Transparent Data Encryption (TDE)",
      "row-level security",
      "column-level security",
      "data sensitivity labels"
    ],
    "site_answers": [
      "column-level security"
    ],
    "links": "https://azure.microsoft.com/en-ca/updates/column-level-security-is-now-supported-in-azure-sql-data-warehouse/"
  },
  {
    "question_text": "Introductory Info\n                                            Case study -This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -Litware, Inc. owns and operates 300 convenience stores across the US. The company sells a variety of packaged foods and drinks, as well as a variety of prepared foods, such as sandwiches and pizzas.\n\nLitware has a loyalty club whereby members can get daily discounts on specific items by providing their membership number at checkout.\n\nLitware employs business analysts who prefer to analyze data by using Microsoft Power BI, and data scientists who prefer analyzing data in Azure Databricks notebooks.\n\nRequirements -Business Goals -Litware wants to create a new analytics environment in Azure to meet the following requirements: See inventory levels across the stores. Data must be updated as close to real time as possible. Execute ad hoc analytical queries on historical data to identify whether the loyalty club discounts increase sales of the discounted products. Every four hours, notify store employees about how many prepared food items to produce based on historical demand from the sales data.\n\nTechnical Requirements -Litware identifies the following technical requirements: Minimize the number of different Azure services needed to achieve the business goals. Use platform as a service (PaaS) offerings whenever possible and avoid having to provision virtual machines that must be managed by Litware. Ensure that the analytical data store is accessible only to the company's on-premises network and Azure services. Use Azure Active Directory (Azure AD) authentication whenever possible. Use the principle of least privilege when designing security. Stage Inventory data in Azure Data Lake Storage Gen2 before loading the data into the analytical data store. Litware wants to remove transient data from DataLake Storage once the data is no longer in use. Files that have a modified date that is older than 14 days must be removed. Limit the business analysts' access to customer contact information, such as phone numbers, because this type of data is not analytically relevant. Ensure that you can quickly restore a copy of the analytical data store within one hour in the event of corruption or accidental deletion.\n\nPlanned Environment -Litware plans to implement the following environment: The application development team will create an Azure event hub to receive real-time sales data, including store number, date, time, product ID, customer loyalty number, price, and discount amount, from the point of sale (POS) system and output the data to data storage in Azure. Customer data, including name, contact information, and loyalty number, comes from Salesforce, a SaaS application, and can be imported into Azure once every eight hours. Row modified dates are not trusted in the source table. Product data, including product ID, name, and category, comes from Salesforce and can be imported into Azure once every eight hours. Row modified dates are not trusted in the source table. Daily inventory data comes from a Microsoft SQL server located on a private network. Litware currently has 5 TB of historical sales data and 100 GB of customer data. The company expects approximately 100 GB of new data per month for the next year. Litware will build a custom application named FoodPrep to provide store employees with the calculation results of how many prepared food items to produce every four hours. Litware does not plan to implement Azure ExpressRoute or a VPN between the on-premises network and Azure.\n                                            Question\n                                        \n                                        \n                                            What should you do to improve high availability of the real-time data processing solution?",
    "question_type": "single",
    "choices": [
      "Deploy a High Concurrency Databricks cluster.",
      "Deploy an Azure Stream Analytics job and use an Azure Automation runbook to check the status of the job and to start the job if it stops.",
      "Set Data Lake Storage to use geo-redundant storage (GRS).",
      "Deploy identical Azure Stream Analytics jobs to paired regions in Azure."
    ],
    "site_answers": [
      "Deploy identical Azure Stream Analytics jobs to paired regions in Azure."
    ]
  },
  {
    "question_text": "Introductory Info\n                                            Case study -This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nOverview -Litware, Inc. owns and operates 300 convenience stores across the US. The company sells a variety of packaged foods and drinks, as well as a variety of prepared foods, such as sandwiches and pizzas.\n\nLitware has a loyalty club whereby members can get daily discounts on specific items by providing their membership number at checkout.\n\nLitware employs business analysts who prefer to analyze data by using Microsoft Power BI, and data scientists who prefer analyzing data in Azure Databricks notebooks.\n\nRequirements -Business Goals -Litware wants to create a new analytics environment in Azure to meet the following requirements: See inventory levels across the stores. Data must be updated as close to real time as possible. Execute ad hoc analytical queries on historical data to identify whether the loyalty club discounts increase sales of the discounted products. Every four hours, notify store employees about how many prepared food items to produce based on historical demand from the sales data.\n\nTechnical Requirements -Litware identifies the following technical requirements: Minimize the number of different Azure services needed to achieve the business goals. Use platform as a service (PaaS) offerings whenever possible and avoid having to provision virtual machines that must be managed by Litware. Ensure that the analytical data store is accessible only to the company's on-premises network and Azure services. Use Azure Active Directory (Azure AD) authentication whenever possible. Use the principle of least privilege when designing security. Stage Inventory data in Azure Data Lake Storage Gen2 before loading the data into the analytical data store. Litware wants to remove transient data from DataLake Storage once the data is no longer in use. Files that have a modified date that is older than 14 days must be removed. Limit the business analysts' access to customer contact information, such as phone numbers, because this type of data is not analytically relevant. Ensure that you can quickly restore a copy of the analytical data store within one hour in the event of corruption or accidental deletion.\n\nPlanned Environment -Litware plans to implement the following environment: The application development team will create an Azure event hub to receive real-time sales data, including store number, date, time, product ID, customer loyalty number, price, and discount amount, from the point of sale (POS) system and output the data to data storage in Azure. Customer data, including name, contact information, and loyalty number, comes from Salesforce, a SaaS application, and can be imported into Azure once every eight hours. Row modified dates are not trusted in the source table. Product data, including product ID, name, and category, comes from Salesforce and can be imported into Azure once every eight hours. Row modified dates are not trusted in the source table. Daily inventory data comes from a Microsoft SQL server located on a private network. Litware currently has 5 TB of historical sales data and 100 GB of customer data. The company expects approximately 100 GB of new data per month for the next year. Litware will build a custom application named FoodPrep to provide store employees with the calculation results of how many prepared food items to produce every four hours. Litware does not plan to implement Azure ExpressRoute or a VPN between the on-premises network and Azure.\n                                            Question\n                                        \n                                        \n                                            HOTSPOT -Which Azure Data Factory components should you recommend using together to import the daily inventory data from the SQL server to Azure Data Lake Storage?To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.\n\nHot Area:",
    "question_type": "multiple",
    "choices": [
      "X: Azure integration runtime","X: Azure-SSIS integration runtime","X: Self-hosted integration runtime",
      "Y: Event-based trigger","Y: Schedule trigger","Y: Tumbling window trigger",
      "Z: Copy activity","Z: Lookup activity","Z: Stored procedure activity"
    ],
    "site_answers": [
      "X: Self-hosted integration runtime","Y: Schedule trigger","Z: Copy activity"
    ]
  }
]
